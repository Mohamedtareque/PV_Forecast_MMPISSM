{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b3925a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Data\n",
    "from src.data_preparation import load_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f69e9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 11,560 records\n",
      "Date range: 2014-01-03 14:00:00+00:00 to 2016-12-30 23:00:00+00:00\n",
      "Timezone: UTC\n",
      "Loaded 10,510 records\n",
      "Date range: 2014-01-03 14:00:00+00:00 to 2016-12-30 23:00:00+00:00\n",
      "Timezone: UTC\n"
     ]
    }
   ],
   "source": [
    "PROCESSED_DATA_PATH = \"data/processed/df_1h_lag_BLV_spatial_images.csv\"\n",
    "df = load_data(PROCESSED_DATA_PATH, date_col=\"measurement_time\")\n",
    "\n",
    "PROCESSED_DATA_PATH_2 = \"data/processed/df_3h_lag_BLV_spatial_images.csv\"\n",
    "df2 = load_data(PROCESSED_DATA_PATH_2, date_col=\"measurement_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a7d74f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"Unnamed: 0\", \"timestamp\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "575ce992",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.drop(columns=[\"Unnamed: 0\", \"timestamp\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92729be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "measurement_time",
         "rawType": "datetime64[ns, UTC]",
         "type": "unknown"
        },
        {
         "name": "ghi",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "dni",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "PST_Time_meas",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "solar_zenith",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "time_gap_hours",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "time_gap_norm",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "day_boundary_flag",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "hour_progression",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "absolute_hour",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "GHI_cs",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "DNI_cs",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "CSI_ghi",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "CSI_dni",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "season_flag",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "hour_sin",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "hour_cos",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "month_sin",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "month_cos",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "nam_ghi",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "nam_dni",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "nam_cc",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "PST_Time_nam",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "nam_target_time",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "B_CSI_ghi_8h",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "V_CSI_ghi_8h",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "L_CSI_ghi_8h",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "B_CSI_ghi_9h",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "V_CSI_ghi_9h",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "L_CSI_ghi_9h",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "B_CSI_ghi_10h",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "V_CSI_ghi_10h",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "L_CSI_ghi_10h",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "B_CSI_ghi_11h",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "V_CSI_ghi_11h",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "L_CSI_ghi_11h",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "B_CSI_ghi_12h",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "V_CSI_ghi_12h",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "L_CSI_ghi_12h",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "B_CSI_ghi_13h",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "V_CSI_ghi_13h",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "L_CSI_ghi_13h",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "B_CSI_ghi_14h",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "V_CSI_ghi_14h",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "L_CSI_ghi_14h",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "B_CSI_ghi_15h",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "V_CSI_ghi_15h",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "L_CSI_ghi_15h",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "B_CSI_ghi_16h",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "V_CSI_ghi_16h",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "L_CSI_ghi_16h",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "B_CSI_ghi_17h",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "V_CSI_ghi_17h",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "L_CSI_ghi_17h",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "B_CSI_ghi_18h",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "V_CSI_ghi_18h",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "L_CSI_ghi_18h",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "B_CSI_ghi_19h",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "V_CSI_ghi_19h",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "L_CSI_ghi_19h",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "80_dwsw",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "valtime",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "80_cloud_cover",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "56_dwsw",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "56_cloud_cover",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "20_dwsw",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "20_cloud_cover",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "88_dwsw",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "88_cloud_cover",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "AVG(R)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "STD(R)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "ENT(R)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "AVG(G)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "STD(G)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "ENT(G)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "AVG(B)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "STD(B)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "ENT(B)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "AVG(RB)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "STD(RB)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "ENT(RB)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "AVG(NRB)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "STD(NRB)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "ENT(NRB)",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "999bd4dd-a824-4288-af4c-c5076725f841",
       "rows": [
        [
         "2014-01-03 14:00:00+00:00",
         "0.0",
         "0.0",
         "2014-01-03 06:29:30-08:00",
         "100.2760463499351",
         "0.1666666666666666",
         "0.0069444444444444",
         "0.0166666666666666",
         "6.491666666666666",
         "24.491666666666667",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "2",
         "-0.4999999999999997",
         "-0.8660254037844388",
         "0.4999999999999999",
         "0.8660254037844387",
         "0.0",
         "0.0",
         "2.0",
         "2014-01-03 07:00:00-08:00",
         "2014-01-03 15:00:00+00:00",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "2014-01-03 15:00:00+00:00",
         "2.0",
         "0.0",
         "4.0",
         "0.0",
         "0.0",
         "0.0",
         "8.0",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "2014-01-03 15:00:00+00:00",
         "12.9145",
         "89.24583333333334",
         "2014-01-03 07:29:30-08:00",
         "89.72416161681372",
         "0.0166666666666666",
         "0.0006944444444444",
         "0.0",
         "7.491666666666666",
         "25.491666666666667",
         "0.0",
         "0.0",
         "1.2",
         "1.2",
         "2",
         "-0.7071067811865471",
         "-0.7071067811865479",
         "0.4999999999999999",
         "0.8660254037844387",
         "99.625",
         "497.069277",
         "2.0",
         "2014-01-03 08:00:00-08:00",
         "2014-01-03 16:00:00+00:00",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "99.625",
         "2014-01-03 16:00:00+00:00",
         "2.0",
         "97.375",
         "2.0",
         "95.0",
         "0.0",
         "102.0",
         "6.0",
         "124.241865",
         "42.07147666666667",
         "5.547265",
         "139.99577",
         "35.670941666666664",
         "5.564803333333334",
         "151.24645833333332",
         "28.52096833333333",
         "5.360366666666667",
         "0.8083733333333334",
         "0.1443316666666666",
         "4.688985",
         "-0.11327",
         "0.0861316666666666",
         "4.00898"
        ],
        [
         "2014-01-03 16:00:00+00:00",
         "143.77550000000002",
         "636.475",
         "2014-01-03 08:29:30-08:00",
         "80.14672391327649",
         "0.0166666666666666",
         "0.0006944444444444",
         "0.0",
         "8.491666666666667",
         "26.491666666666667",
         "35.1594999933757",
         "182.2978396691529",
         "1.2",
         "1.2",
         "2",
         "-0.8660254037844384",
         "-0.5000000000000004",
         "0.4999999999999999",
         "0.8660254037844387",
         "274.75",
         "760.162689",
         "0.0",
         "2014-01-03 09:00:00-08:00",
         "2014-01-03 17:00:00+00:00",
         "0.6",
         "0.848528137423857",
         "0.0",
         "0.6",
         "0.848528137423857",
         "0.0",
         "0.6",
         "0.848528137423857",
         "0.0",
         "0.6",
         "0.848528137423857",
         "0.0",
         "0.6",
         "0.848528137423857",
         "0.0",
         "0.6",
         "0.848528137423857",
         "0.0",
         "0.6",
         "0.848528137423857",
         "0.0",
         "0.6",
         "0.848528137423857",
         "0.0",
         "0.6",
         "0.848528137423857",
         "0.0",
         "0.6",
         "0.848528137423857",
         "0.0",
         "0.6",
         "0.848528137423857",
         "0.0",
         "0.6",
         "0.848528137423857",
         "0.0",
         "274.75",
         "2014-01-03 17:00:00+00:00",
         "0.0",
         "273.375",
         "0.0",
         "274.125",
         "0.0",
         "279.0",
         "2.0",
         "130.75375",
         "46.386163333333336",
         "5.403886666666668",
         "137.97395833333334",
         "41.028275",
         "5.510188333333333",
         "141.71321166666667",
         "33.48125833333334",
         "5.414776666666668",
         "0.9099883333333332",
         "0.1486833333333333",
         "4.742913333333333",
         "-0.0537866666666666",
         "0.0793033333333333",
         "3.913008333333333"
        ],
        [
         "2014-01-03 17:00:00+00:00",
         "289.955",
         "783.1933333333333",
         "2014-01-03 09:29:30-08:00",
         "71.98881355725932",
         "0.0166666666666666",
         "0.0006944444444444",
         "0.0",
         "9.491666666666667",
         "27.491666666666667",
         "184.55471121765652",
         "540.1203017770864",
         "1.2",
         "1.2",
         "2",
         "-0.9659258262890684",
         "-0.2588190451025206",
         "0.4999999999999999",
         "0.8660254037844387",
         "416.375",
         "871.248807",
         "2.0",
         "2014-01-03 10:00:00-08:00",
         "2014-01-03 18:00:00+00:00",
         "0.7999999999999999",
         "0.6928203230275509",
         "0.0",
         "0.7999999999999999",
         "0.6928203230275509",
         "0.0",
         "0.7999999999999999",
         "0.6928203230275509",
         "0.0",
         "0.7999999999999999",
         "0.6928203230275509",
         "0.0",
         "0.7999999999999999",
         "0.6928203230275509",
         "0.0",
         "0.7999999999999999",
         "0.6928203230275509",
         "0.0",
         "0.7999999999999999",
         "0.6928203230275509",
         "0.0",
         "0.7999999999999999",
         "0.6928203230275509",
         "0.0",
         "0.7999999999999999",
         "0.6928203230275509",
         "0.0",
         "0.7999999999999999",
         "0.6928203230275509",
         "0.0",
         "0.7999999999999999",
         "0.6928203230275509",
         "0.0",
         "0.7999999999999999",
         "0.6928203230275509",
         "0.0",
         "416.375",
         "2014-01-03 18:00:00+00:00",
         "2.0",
         "411.5",
         "0.0",
         "417.5",
         "4.0",
         "410.375",
         "0.0",
         "134.42082666666667",
         "48.50835666666666",
         "5.400825",
         "136.76727833333334",
         "44.092711666666666",
         "5.478535",
         "137.21341666666666",
         "35.40382666666667",
         "5.377718333333334",
         "0.9653666666666668",
         "0.1426683333333333",
         "4.686905",
         "-0.0226466666666666",
         "0.0712749999999999",
         "3.7773866666666662"
        ],
        [
         "2014-01-03 18:00:00+00:00",
         "402.6433333333333",
         "843.05",
         "2014-01-03 10:29:30-08:00",
         "65.81615818728889",
         "0.0166666666666666",
         "0.0006944444444444",
         "0.0",
         "10.491666666666667",
         "28.491666666666667",
         "321.99246197475424",
         "692.5140645275006",
         "1.2",
         "1.2",
         "2",
         "-1.0",
         "-1.8369701987210294e-16",
         "0.4999999999999999",
         "0.8660254037844387",
         "510.875",
         "919.980698",
         "8.0",
         "2014-01-03 11:00:00-08:00",
         "2014-01-03 19:00:00+00:00",
         "0.8999999999999999",
         "0.6000000000000001",
         "0.0",
         "0.8999999999999999",
         "0.6000000000000001",
         "0.0",
         "0.8999999999999999",
         "0.6000000000000001",
         "0.0",
         "0.8999999999999999",
         "0.6000000000000001",
         "0.0",
         "0.8999999999999999",
         "0.6000000000000001",
         "0.0",
         "0.8999999999999999",
         "0.6000000000000001",
         "0.0",
         "0.8999999999999999",
         "0.6000000000000001",
         "0.0",
         "0.8999999999999999",
         "0.6000000000000001",
         "0.0",
         "0.8999999999999999",
         "0.6000000000000001",
         "0.0",
         "0.8999999999999999",
         "0.6000000000000001",
         "0.0",
         "0.8999999999999999",
         "0.6000000000000001",
         "0.0",
         "0.8999999999999999",
         "0.6000000000000001",
         "0.0",
         "510.875",
         "2014-01-03 19:00:00+00:00",
         "8.0",
         "508.125",
         "4.0",
         "509.875",
         "12.0",
         "508.75",
         "2.0",
         "133.57898166666666",
         "49.570825000000006",
         "5.419333333333333",
         "137.04061",
         "45.139165",
         "5.456043333333334",
         "138.61621",
         "36.16260833333333",
         "5.35696",
         "0.94705",
         "0.14095",
         "4.682246666666667",
         "-0.03225",
         "0.0724816666666666",
         "3.788301666666667"
        ]
       ],
       "shape": {
        "columns": 83,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ghi</th>\n",
       "      <th>dni</th>\n",
       "      <th>PST_Time_meas</th>\n",
       "      <th>solar_zenith</th>\n",
       "      <th>time_gap_hours</th>\n",
       "      <th>time_gap_norm</th>\n",
       "      <th>day_boundary_flag</th>\n",
       "      <th>hour_progression</th>\n",
       "      <th>absolute_hour</th>\n",
       "      <th>GHI_cs</th>\n",
       "      <th>...</th>\n",
       "      <th>ENT(G)</th>\n",
       "      <th>AVG(B)</th>\n",
       "      <th>STD(B)</th>\n",
       "      <th>ENT(B)</th>\n",
       "      <th>AVG(RB)</th>\n",
       "      <th>STD(RB)</th>\n",
       "      <th>ENT(RB)</th>\n",
       "      <th>AVG(NRB)</th>\n",
       "      <th>STD(NRB)</th>\n",
       "      <th>ENT(NRB)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>measurement_time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-01-03 14:00:00+00:00</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2014-01-03 06:29:30-08:00</td>\n",
       "      <td>100.276046</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.006944</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>6.491667</td>\n",
       "      <td>24.491667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-03 15:00:00+00:00</th>\n",
       "      <td>12.914500</td>\n",
       "      <td>89.245833</td>\n",
       "      <td>2014-01-03 07:29:30-08:00</td>\n",
       "      <td>89.724162</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.491667</td>\n",
       "      <td>25.491667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.564803</td>\n",
       "      <td>151.246458</td>\n",
       "      <td>28.520968</td>\n",
       "      <td>5.360367</td>\n",
       "      <td>0.808373</td>\n",
       "      <td>0.144332</td>\n",
       "      <td>4.688985</td>\n",
       "      <td>-0.113270</td>\n",
       "      <td>0.086132</td>\n",
       "      <td>4.008980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-03 16:00:00+00:00</th>\n",
       "      <td>143.775500</td>\n",
       "      <td>636.475000</td>\n",
       "      <td>2014-01-03 08:29:30-08:00</td>\n",
       "      <td>80.146724</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.491667</td>\n",
       "      <td>26.491667</td>\n",
       "      <td>35.159500</td>\n",
       "      <td>...</td>\n",
       "      <td>5.510188</td>\n",
       "      <td>141.713212</td>\n",
       "      <td>33.481258</td>\n",
       "      <td>5.414777</td>\n",
       "      <td>0.909988</td>\n",
       "      <td>0.148683</td>\n",
       "      <td>4.742913</td>\n",
       "      <td>-0.053787</td>\n",
       "      <td>0.079303</td>\n",
       "      <td>3.913008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-03 17:00:00+00:00</th>\n",
       "      <td>289.955000</td>\n",
       "      <td>783.193333</td>\n",
       "      <td>2014-01-03 09:29:30-08:00</td>\n",
       "      <td>71.988814</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.491667</td>\n",
       "      <td>27.491667</td>\n",
       "      <td>184.554711</td>\n",
       "      <td>...</td>\n",
       "      <td>5.478535</td>\n",
       "      <td>137.213417</td>\n",
       "      <td>35.403827</td>\n",
       "      <td>5.377718</td>\n",
       "      <td>0.965367</td>\n",
       "      <td>0.142668</td>\n",
       "      <td>4.686905</td>\n",
       "      <td>-0.022647</td>\n",
       "      <td>0.071275</td>\n",
       "      <td>3.777387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-03 18:00:00+00:00</th>\n",
       "      <td>402.643333</td>\n",
       "      <td>843.050000</td>\n",
       "      <td>2014-01-03 10:29:30-08:00</td>\n",
       "      <td>65.816158</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.491667</td>\n",
       "      <td>28.491667</td>\n",
       "      <td>321.992462</td>\n",
       "      <td>...</td>\n",
       "      <td>5.456043</td>\n",
       "      <td>138.616210</td>\n",
       "      <td>36.162608</td>\n",
       "      <td>5.356960</td>\n",
       "      <td>0.947050</td>\n",
       "      <td>0.140950</td>\n",
       "      <td>4.682247</td>\n",
       "      <td>-0.032250</td>\n",
       "      <td>0.072482</td>\n",
       "      <td>3.788302</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 83 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  ghi         dni              PST_Time_meas  \\\n",
       "measurement_time                                                               \n",
       "2014-01-03 14:00:00+00:00    0.000000    0.000000  2014-01-03 06:29:30-08:00   \n",
       "2014-01-03 15:00:00+00:00   12.914500   89.245833  2014-01-03 07:29:30-08:00   \n",
       "2014-01-03 16:00:00+00:00  143.775500  636.475000  2014-01-03 08:29:30-08:00   \n",
       "2014-01-03 17:00:00+00:00  289.955000  783.193333  2014-01-03 09:29:30-08:00   \n",
       "2014-01-03 18:00:00+00:00  402.643333  843.050000  2014-01-03 10:29:30-08:00   \n",
       "\n",
       "                           solar_zenith  time_gap_hours  time_gap_norm  \\\n",
       "measurement_time                                                         \n",
       "2014-01-03 14:00:00+00:00    100.276046        0.166667       0.006944   \n",
       "2014-01-03 15:00:00+00:00     89.724162        0.016667       0.000694   \n",
       "2014-01-03 16:00:00+00:00     80.146724        0.016667       0.000694   \n",
       "2014-01-03 17:00:00+00:00     71.988814        0.016667       0.000694   \n",
       "2014-01-03 18:00:00+00:00     65.816158        0.016667       0.000694   \n",
       "\n",
       "                           day_boundary_flag  hour_progression  absolute_hour  \\\n",
       "measurement_time                                                                \n",
       "2014-01-03 14:00:00+00:00           0.016667          6.491667      24.491667   \n",
       "2014-01-03 15:00:00+00:00           0.000000          7.491667      25.491667   \n",
       "2014-01-03 16:00:00+00:00           0.000000          8.491667      26.491667   \n",
       "2014-01-03 17:00:00+00:00           0.000000          9.491667      27.491667   \n",
       "2014-01-03 18:00:00+00:00           0.000000         10.491667      28.491667   \n",
       "\n",
       "                               GHI_cs  ...    ENT(G)      AVG(B)     STD(B)  \\\n",
       "measurement_time                       ...                                    \n",
       "2014-01-03 14:00:00+00:00    0.000000  ...       NaN         NaN        NaN   \n",
       "2014-01-03 15:00:00+00:00    0.000000  ...  5.564803  151.246458  28.520968   \n",
       "2014-01-03 16:00:00+00:00   35.159500  ...  5.510188  141.713212  33.481258   \n",
       "2014-01-03 17:00:00+00:00  184.554711  ...  5.478535  137.213417  35.403827   \n",
       "2014-01-03 18:00:00+00:00  321.992462  ...  5.456043  138.616210  36.162608   \n",
       "\n",
       "                             ENT(B)   AVG(RB)   STD(RB)   ENT(RB)  AVG(NRB)  \\\n",
       "measurement_time                                                              \n",
       "2014-01-03 14:00:00+00:00       NaN       NaN       NaN       NaN       NaN   \n",
       "2014-01-03 15:00:00+00:00  5.360367  0.808373  0.144332  4.688985 -0.113270   \n",
       "2014-01-03 16:00:00+00:00  5.414777  0.909988  0.148683  4.742913 -0.053787   \n",
       "2014-01-03 17:00:00+00:00  5.377718  0.965367  0.142668  4.686905 -0.022647   \n",
       "2014-01-03 18:00:00+00:00  5.356960  0.947050  0.140950  4.682247 -0.032250   \n",
       "\n",
       "                           STD(NRB)  ENT(NRB)  \n",
       "measurement_time                               \n",
       "2014-01-03 14:00:00+00:00       NaN       NaN  \n",
       "2014-01-03 15:00:00+00:00  0.086132  4.008980  \n",
       "2014-01-03 16:00:00+00:00  0.079303  3.913008  \n",
       "2014-01-03 17:00:00+00:00  0.071275  3.777387  \n",
       "2014-01-03 18:00:00+00:00  0.072482  3.788302  \n",
       "\n",
       "[5 rows x 83 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10ba68f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ghi', 'dni', 'PST_Time_meas', 'solar_zenith', 'time_gap_hours',\n",
       "       'time_gap_norm', 'day_boundary_flag', 'hour_progression',\n",
       "       'absolute_hour', 'GHI_cs', 'DNI_cs', 'CSI_ghi', 'CSI_dni',\n",
       "       'season_flag', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos',\n",
       "       'nam_ghi', 'nam_dni', 'nam_cc', 'PST_Time_nam', 'nam_target_time',\n",
       "       'B_CSI_ghi_8h', 'V_CSI_ghi_8h', 'L_CSI_ghi_8h', 'B_CSI_ghi_9h',\n",
       "       'V_CSI_ghi_9h', 'L_CSI_ghi_9h', 'B_CSI_ghi_10h', 'V_CSI_ghi_10h',\n",
       "       'L_CSI_ghi_10h', 'B_CSI_ghi_11h', 'V_CSI_ghi_11h', 'L_CSI_ghi_11h',\n",
       "       'B_CSI_ghi_12h', 'V_CSI_ghi_12h', 'L_CSI_ghi_12h', 'B_CSI_ghi_13h',\n",
       "       'V_CSI_ghi_13h', 'L_CSI_ghi_13h', 'B_CSI_ghi_14h', 'V_CSI_ghi_14h',\n",
       "       'L_CSI_ghi_14h', 'B_CSI_ghi_15h', 'V_CSI_ghi_15h', 'L_CSI_ghi_15h',\n",
       "       'B_CSI_ghi_16h', 'V_CSI_ghi_16h', 'L_CSI_ghi_16h', 'B_CSI_ghi_17h',\n",
       "       'V_CSI_ghi_17h', 'L_CSI_ghi_17h', 'B_CSI_ghi_18h', 'V_CSI_ghi_18h',\n",
       "       'L_CSI_ghi_18h', 'B_CSI_ghi_19h', 'V_CSI_ghi_19h', 'L_CSI_ghi_19h',\n",
       "       '80_dwsw', 'valtime', '80_cloud_cover', '56_dwsw', '56_cloud_cover',\n",
       "       '20_dwsw', '20_cloud_cover', '88_dwsw', '88_cloud_cover', 'AVG(R)',\n",
       "       'STD(R)', 'ENT(R)', 'AVG(G)', 'STD(G)', 'ENT(G)', 'AVG(B)', 'STD(B)',\n",
       "       'ENT(B)', 'AVG(RB)', 'STD(RB)', 'ENT(RB)', 'AVG(NRB)', 'STD(NRB)',\n",
       "       'ENT(NRB)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1f58e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ROLLING ORIGIN EVALUATION\n",
      "============================================================\n",
      "Total folds: 35\n",
      "Frequency: MS\n",
      "Data range: 2014-01-03 to 2016-12-30\n",
      "\n",
      "Fold Summary:\n",
      "  Fold 1: Train [2014-01-03 to 2014-02-01] (297 records) → Val [2014-02-02 to 2014-02-28] (186 records)\n",
      "  Fold 2: Train [2014-01-03 to 2014-03-01] (494 records) → Val [2014-03-02 to 2014-03-31] (319 records)\n",
      "  Fold 3: Train [2014-01-03 to 2014-04-01] (835 records) → Val [2014-04-02 to 2014-04-30] (308 records)\n",
      "  Fold 4: Train [2014-01-03 to 2014-05-01] (1,165 records) → Val [2014-05-02 to 2014-05-31] (319 records)\n",
      "  Fold 5: Train [2014-01-03 to 2014-06-01] (1,506 records) → Val [2014-06-02 to 2014-06-30] (308 records)\n",
      "  Fold 6: Train [2014-01-03 to 2014-07-01] (1,836 records) → Val [2014-07-02 to 2014-07-31] (319 records)\n",
      "  Fold 7: Train [2014-01-03 to 2014-08-01] (2,177 records) → Val [2014-08-02 to 2014-08-31] (319 records)\n",
      "  Fold 8: Train [2014-01-03 to 2014-09-01] (2,518 records) → Val [2014-09-02 to 2014-09-30] (308 records)\n",
      "  Fold 9: Train [2014-01-03 to 2014-10-01] (2,848 records) → Val [2014-10-02 to 2014-10-31] (319 records)\n",
      "  Fold 10: Train [2014-01-03 to 2014-11-01] (3,189 records) → Val [2014-11-02 to 2014-11-30] (308 records)\n",
      "  Fold 11: Train [2014-01-03 to 2014-12-01] (3,519 records) → Val [2014-12-02 to 2014-12-31] (319 records)\n",
      "  Fold 12: Train [2014-01-03 to 2015-01-01] (3,860 records) → Val [2015-01-02 to 2015-01-31] (319 records)\n",
      "  Fold 13: Train [2014-01-03 to 2015-02-01] (4,201 records) → Val [2015-02-02 to 2015-02-28] (286 records)\n",
      "  Fold 14: Train [2014-01-03 to 2015-03-01] (4,509 records) → Val [2015-03-02 to 2015-03-31] (319 records)\n",
      "  Fold 15: Train [2014-01-03 to 2015-04-01] (4,850 records) → Val [2015-04-02 to 2015-04-30] (308 records)\n",
      "  Fold 16: Train [2014-01-03 to 2015-05-01] (5,180 records) → Val [2015-05-02 to 2015-05-31] (319 records)\n",
      "  Fold 17: Train [2014-01-03 to 2015-06-01] (5,521 records) → Val [2015-06-02 to 2015-06-30] (308 records)\n",
      "  Fold 18: Train [2014-01-03 to 2015-07-01] (5,851 records) → Val [2015-07-02 to 2015-07-31] (319 records)\n",
      "  Fold 19: Train [2014-01-03 to 2015-08-01] (6,192 records) → Val [2015-08-02 to 2015-08-31] (308 records)\n",
      "  Fold 20: Train [2014-01-03 to 2015-09-01] (6,522 records) → Val [2015-09-02 to 2015-09-30] (308 records)\n",
      "  Fold 21: Train [2014-01-03 to 2015-10-01] (6,852 records) → Val [2015-10-02 to 2015-10-31] (319 records)\n",
      "  Fold 22: Train [2014-01-03 to 2015-11-01] (7,193 records) → Val [2015-11-02 to 2015-11-30] (308 records)\n",
      "  Fold 23: Train [2014-01-03 to 2015-12-01] (7,523 records) → Val [2015-12-02 to 2015-12-31] (319 records)\n",
      "  Fold 24: Train [2014-01-03 to 2016-01-01] (7,854 records) → Val [2016-01-02 to 2016-01-31] (220 records)\n",
      "  Fold 25: Train [2014-01-03 to 2016-02-01] (8,095 records) → Val [2016-02-02 to 2016-02-29] (231 records)\n",
      "  Fold 26: Train [2014-01-03 to 2016-03-01] (8,348 records) → Val [2016-03-02 to 2016-03-31] (319 records)\n",
      "  Fold 27: Train [2014-01-03 to 2016-04-01] (8,689 records) → Val [2016-04-02 to 2016-04-30] (308 records)\n",
      "  Fold 28: Train [2014-01-03 to 2016-05-01] (9,019 records) → Val [2016-05-02 to 2016-05-31] (319 records)\n",
      "  Fold 29: Train [2014-01-03 to 2016-06-01] (9,360 records) → Val [2016-06-02 to 2016-06-30] (308 records)\n",
      "  Fold 30: Train [2014-01-03 to 2016-07-01] (9,690 records) → Val [2016-07-02 to 2016-07-31] (297 records)\n",
      "  Fold 31: Train [2014-01-03 to 2016-08-01] (10,009 records) → Val [2016-08-02 to 2016-08-31] (319 records)\n",
      "  Fold 32: Train [2014-01-03 to 2016-09-01] (10,350 records) → Val [2016-09-02 to 2016-09-30] (308 records)\n",
      "  Fold 33: Train [2014-01-03 to 2016-10-01] (10,680 records) → Val [2016-10-02 to 2016-10-31] (319 records)\n",
      "  Fold 34: Train [2014-01-03 to 2016-11-01] (11,021 records) → Val [2016-11-02 to 2016-11-30] (308 records)\n",
      "  Fold 35: Train [2014-01-03 to 2016-12-01] (11,351 records) → Val [2016-12-02 to 2016-12-31] (208 records)\n",
      "============================================================\n",
      "\n",
      "Split metadata saved to 'splits/exp-004/' directory\n"
     ]
    }
   ],
   "source": [
    "# 2. Rolling Origin Split (More Suitable in TimeSeries) like K-Folds\n",
    "from src.data_preparation import rolling_origin_evaluation,save_splits_info\n",
    "\n",
    "rollingSplits_df = rolling_origin_evaluation(df=df, start_train = '2014-01-2',\n",
    "    end_train = '2016-12-31')\n",
    "save_splits_info({}, rollingSplits_df, experiment_name=\"exp-004\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "028b5b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ROLLING ORIGIN EVALUATION\n",
      "============================================================\n",
      "Total folds: 35\n",
      "Frequency: MS\n",
      "Data range: 2014-01-03 to 2016-12-30\n",
      "\n",
      "Fold Summary:\n",
      "  Fold 1: Train [2014-01-03 to 2014-02-01] (270 records) → Val [2014-02-02 to 2014-02-28] (170 records)\n",
      "  Fold 2: Train [2014-01-03 to 2014-03-01] (450 records) → Val [2014-03-02 to 2014-03-31] (290 records)\n",
      "  Fold 3: Train [2014-01-03 to 2014-04-01] (760 records) → Val [2014-04-02 to 2014-04-30] (280 records)\n",
      "  Fold 4: Train [2014-01-03 to 2014-05-01] (1,060 records) → Val [2014-05-02 to 2014-05-31] (290 records)\n",
      "  Fold 5: Train [2014-01-03 to 2014-06-01] (1,370 records) → Val [2014-06-02 to 2014-06-30] (280 records)\n",
      "  Fold 6: Train [2014-01-03 to 2014-07-01] (1,670 records) → Val [2014-07-02 to 2014-07-31] (290 records)\n",
      "  Fold 7: Train [2014-01-03 to 2014-08-01] (1,980 records) → Val [2014-08-02 to 2014-08-31] (290 records)\n",
      "  Fold 8: Train [2014-01-03 to 2014-09-01] (2,290 records) → Val [2014-09-02 to 2014-09-30] (280 records)\n",
      "  Fold 9: Train [2014-01-03 to 2014-10-01] (2,590 records) → Val [2014-10-02 to 2014-10-31] (290 records)\n",
      "  Fold 10: Train [2014-01-03 to 2014-11-01] (2,900 records) → Val [2014-11-02 to 2014-11-30] (280 records)\n",
      "  Fold 11: Train [2014-01-03 to 2014-12-01] (3,200 records) → Val [2014-12-02 to 2014-12-31] (290 records)\n",
      "  Fold 12: Train [2014-01-03 to 2015-01-01] (3,510 records) → Val [2015-01-02 to 2015-01-31] (290 records)\n",
      "  Fold 13: Train [2014-01-03 to 2015-02-01] (3,820 records) → Val [2015-02-02 to 2015-02-28] (260 records)\n",
      "  Fold 14: Train [2014-01-03 to 2015-03-01] (4,100 records) → Val [2015-03-02 to 2015-03-31] (290 records)\n",
      "  Fold 15: Train [2014-01-03 to 2015-04-01] (4,410 records) → Val [2015-04-02 to 2015-04-30] (280 records)\n",
      "  Fold 16: Train [2014-01-03 to 2015-05-01] (4,710 records) → Val [2015-05-02 to 2015-05-31] (290 records)\n",
      "  Fold 17: Train [2014-01-03 to 2015-06-01] (5,020 records) → Val [2015-06-02 to 2015-06-30] (280 records)\n",
      "  Fold 18: Train [2014-01-03 to 2015-07-01] (5,320 records) → Val [2015-07-02 to 2015-07-31] (290 records)\n",
      "  Fold 19: Train [2014-01-03 to 2015-08-01] (5,630 records) → Val [2015-08-02 to 2015-08-31] (280 records)\n",
      "  Fold 20: Train [2014-01-03 to 2015-09-01] (5,930 records) → Val [2015-09-02 to 2015-09-30] (280 records)\n",
      "  Fold 21: Train [2014-01-03 to 2015-10-01] (6,230 records) → Val [2015-10-02 to 2015-10-31] (290 records)\n",
      "  Fold 22: Train [2014-01-03 to 2015-11-01] (6,540 records) → Val [2015-11-02 to 2015-11-30] (280 records)\n",
      "  Fold 23: Train [2014-01-03 to 2015-12-01] (6,840 records) → Val [2015-12-02 to 2015-12-31] (290 records)\n",
      "  Fold 24: Train [2014-01-03 to 2016-01-01] (7,140 records) → Val [2016-01-02 to 2016-01-31] (200 records)\n",
      "  Fold 25: Train [2014-01-03 to 2016-02-01] (7,360 records) → Val [2016-02-02 to 2016-02-29] (210 records)\n",
      "  Fold 26: Train [2014-01-03 to 2016-03-01] (7,590 records) → Val [2016-03-02 to 2016-03-31] (290 records)\n",
      "  Fold 27: Train [2014-01-03 to 2016-04-01] (7,900 records) → Val [2016-04-02 to 2016-04-30] (280 records)\n",
      "  Fold 28: Train [2014-01-03 to 2016-05-01] (8,200 records) → Val [2016-05-02 to 2016-05-31] (290 records)\n",
      "  Fold 29: Train [2014-01-03 to 2016-06-01] (8,510 records) → Val [2016-06-02 to 2016-06-30] (280 records)\n",
      "  Fold 30: Train [2014-01-03 to 2016-07-01] (8,810 records) → Val [2016-07-02 to 2016-07-31] (270 records)\n",
      "  Fold 31: Train [2014-01-03 to 2016-08-01] (9,100 records) → Val [2016-08-02 to 2016-08-31] (290 records)\n",
      "  Fold 32: Train [2014-01-03 to 2016-09-01] (9,410 records) → Val [2016-09-02 to 2016-09-30] (280 records)\n",
      "  Fold 33: Train [2014-01-03 to 2016-10-01] (9,710 records) → Val [2016-10-02 to 2016-10-31] (290 records)\n",
      "  Fold 34: Train [2014-01-03 to 2016-11-01] (10,020 records) → Val [2016-11-02 to 2016-11-30] (280 records)\n",
      "  Fold 35: Train [2014-01-03 to 2016-12-01] (10,320 records) → Val [2016-12-02 to 2016-12-31] (190 records)\n",
      "============================================================\n",
      "\n",
      "Split metadata saved to 'splits/exp-005/' directory\n"
     ]
    }
   ],
   "source": [
    "# 2. Rolling Origin Split (More Suitable in TimeSeries) like K-Folds\n",
    "from src.data_preparation import rolling_origin_evaluation,save_splits_info\n",
    "\n",
    "rollingSplits_df2 = rolling_origin_evaluation(df=df2, start_train = '2014-01-2',\n",
    "    end_train = '2016-12-31')\n",
    "save_splits_info({}, rollingSplits_df2, experiment_name=\"exp-005\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9cb756f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does NAM records have a 1-hour lag ahead from measurements? Yes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_noIndex = df.reset_index()\n",
    "df_noIndex['nam_target_time'] = pd.to_datetime(df_noIndex['nam_target_time'])\n",
    "df_noIndex['measurement_time'] = pd.to_datetime(df_noIndex['measurement_time'])\n",
    "df_noIndex['time_diff'] = (df_noIndex['nam_target_time'] - df_noIndex['measurement_time']).dt.total_seconds() / 3600  # Calculate the time difference in hours\n",
    "    \n",
    "# Check if the time difference is exactly 1 hour\n",
    "is_nam_lag_correct = np.allclose(df_noIndex['time_diff'], 1)  # All time differences should be 1 hour\n",
    "print(f\"Does NAM records have a 1-hour lag ahead from measurements? {'Yes' if is_nam_lag_correct else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14df23d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2. Check if sequences have the same length across each day\n",
    "# # Extract date from the 'measurement_time'\n",
    "# df_noIndex['date'] = df_noIndex['measurement_time'].dt.date\n",
    "    \n",
    "# # Group by date and count records per day\n",
    "# daily_counts = df_noIndex.groupby('date').size()\n",
    "    \n",
    "# # Find dates where the record count is not equal to 24 (assuming 24 records per day)\n",
    "# inconsistent_dates = daily_counts[daily_counts != 11] \n",
    "    \n",
    "# # Print dates where records differ from 24\n",
    "# if not inconsistent_dates.empty:\n",
    "#     print(\"Dates with record counts different from 14 hours:\")\n",
    "#     print(inconsistent_dates)\n",
    "#     print(len(inconsistent_dates))\n",
    "# else:\n",
    "#     print(\"All dates have the expected 14-hour record count.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c93f72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a list of dates to drop\n",
    "# dates_to_drop = [\n",
    "#     '2014-01-03', '2014-01-30', '2014-02-05', '2014-02-13', '2014-02-19',\n",
    "#     '2015-08-23', '2015-08-24', '2015-12-31', '2016-01-01', '2016-07-13',\n",
    "#     '2016-07-15', '2016-12-31'\n",
    "# ]\n",
    "\n",
    "# # Print the first few rows of the index to see its format\n",
    "# print(\"Index sample before processing:\")\n",
    "# print(df.index[:5])\n",
    "# print(\"\\nIndex timezone:\", df.index.tz)\n",
    "\n",
    "# # Convert dates to datetime with UTC timezone\n",
    "# dates_to_drop = pd.to_datetime(dates_to_drop).tz_localize('UTC')\n",
    "\n",
    "# # Get the original size of the DataFrame\n",
    "# original_size = len(df)\n",
    "\n",
    "# df = df[~df.index.normalize().isin(dates_to_drop)]\n",
    "\n",
    "# # Print the number of rows dropped\n",
    "# print(f\"\\nOriginal number of records: {original_size}\")\n",
    "# print(f\"Number of records after dropping dates: {len(df)}\")\n",
    "# print(f\"Number of records dropped: {original_size - len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a003357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ghi', 'dni', 'PST_Time_meas', 'solar_zenith', 'time_gap_hours',\n",
       "       'time_gap_norm', 'day_boundary_flag', 'hour_progression',\n",
       "       'absolute_hour', 'GHI_cs', 'DNI_cs', 'CSI_ghi', 'CSI_dni',\n",
       "       'season_flag', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos',\n",
       "       'nam_ghi', 'nam_dni', 'nam_cc', 'PST_Time_nam', 'nam_target_time',\n",
       "       'B_CSI_ghi_8h', 'V_CSI_ghi_8h', 'L_CSI_ghi_8h', 'B_CSI_ghi_9h',\n",
       "       'V_CSI_ghi_9h', 'L_CSI_ghi_9h', 'B_CSI_ghi_10h', 'V_CSI_ghi_10h',\n",
       "       'L_CSI_ghi_10h', 'B_CSI_ghi_11h', 'V_CSI_ghi_11h', 'L_CSI_ghi_11h',\n",
       "       'B_CSI_ghi_12h', 'V_CSI_ghi_12h', 'L_CSI_ghi_12h', 'B_CSI_ghi_13h',\n",
       "       'V_CSI_ghi_13h', 'L_CSI_ghi_13h', 'B_CSI_ghi_14h', 'V_CSI_ghi_14h',\n",
       "       'L_CSI_ghi_14h', 'B_CSI_ghi_15h', 'V_CSI_ghi_15h', 'L_CSI_ghi_15h',\n",
       "       'B_CSI_ghi_16h', 'V_CSI_ghi_16h', 'L_CSI_ghi_16h', 'B_CSI_ghi_17h',\n",
       "       'V_CSI_ghi_17h', 'L_CSI_ghi_17h', 'B_CSI_ghi_18h', 'V_CSI_ghi_18h',\n",
       "       'L_CSI_ghi_18h', 'B_CSI_ghi_19h', 'V_CSI_ghi_19h', 'L_CSI_ghi_19h',\n",
       "       '80_dwsw', 'valtime', '80_cloud_cover', '56_dwsw', '56_cloud_cover',\n",
       "       '20_dwsw', '20_cloud_cover', '88_dwsw', '88_cloud_cover', 'AVG(R)',\n",
       "       'STD(R)', 'ENT(R)', 'AVG(G)', 'STD(G)', 'ENT(G)', 'AVG(B)', 'STD(B)',\n",
       "       'ENT(B)', 'AVG(RB)', 'STD(RB)', 'ENT(RB)', 'AVG(NRB)', 'STD(NRB)',\n",
       "       'ENT(NRB)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd1cb126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Features With NaN Values ---\n",
      "The following features have NaN values, with the total count for each:\n",
      "AVG(R)      1259\n",
      "STD(R)      1259\n",
      "ENT(R)      1259\n",
      "AVG(G)      1259\n",
      "STD(G)      1259\n",
      "ENT(G)      1259\n",
      "AVG(B)      1259\n",
      "STD(B)      1259\n",
      "ENT(B)      1259\n",
      "AVG(RB)     1259\n",
      "STD(RB)     1259\n",
      "ENT(RB)     1259\n",
      "AVG(NRB)    1259\n",
      "STD(NRB)    1259\n",
      "ENT(NRB)    1259\n",
      "dtype: int64\n",
      "\n",
      "========================================\n",
      "\n",
      "--- Distribution of NaN-Value Records by Hour ---\n",
      "Distribution of records (rows) containing at least one NaN, by hour:\n",
      "measurement_time\n",
      "2     1050\n",
      "14      84\n",
      "15      14\n",
      "16      14\n",
      "17      14\n",
      "18      14\n",
      "19      14\n",
      "20      13\n",
      "21      14\n",
      "22      14\n",
      "23      14\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total number of rows with at least one NaN: 1259\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "features_to_check = ['ghi', 'dni', 'solar_zenith', 'GHI_cs', 'DNI_cs', 'CSI_ghi', \n",
    "                                   'CSI_dni',\n",
    "                                   'nam_ghi', 'nam_dni', 'nam_cc', 'B_CSI_ghi_8h', 'V_CSI_ghi_8h', 'L_CSI_ghi_8h', 'B_CSI_ghi_9h',\n",
    "       'V_CSI_ghi_9h', 'L_CSI_ghi_9h', 'B_CSI_ghi_10h', 'V_CSI_ghi_10h',\n",
    "       'L_CSI_ghi_10h', 'B_CSI_ghi_11h', 'V_CSI_ghi_11h', 'L_CSI_ghi_11h',\n",
    "       'B_CSI_ghi_12h', 'V_CSI_ghi_12h', 'L_CSI_ghi_12h', 'B_CSI_ghi_13h',\n",
    "       'V_CSI_ghi_13h', 'L_CSI_ghi_13h', 'B_CSI_ghi_14h', 'V_CSI_ghi_14h',\n",
    "       'L_CSI_ghi_14h', 'B_CSI_ghi_15h', 'V_CSI_ghi_15h', 'L_CSI_ghi_15h',\n",
    "       'B_CSI_ghi_16h', 'V_CSI_ghi_16h', 'L_CSI_ghi_16h', 'B_CSI_ghi_17h',\n",
    "       'V_CSI_ghi_17h', 'L_CSI_ghi_17h', 'B_CSI_ghi_18h', 'V_CSI_ghi_18h',\n",
    "       'L_CSI_ghi_18h', 'B_CSI_ghi_19h', 'V_CSI_ghi_19h', 'L_CSI_ghi_19h',\n",
    "       '80_dwsw', '80_cloud_cover', '56_dwsw', '56_cloud_cover',\n",
    "       '20_dwsw', '20_cloud_cover', '88_dwsw', '88_cloud_cover', 'AVG(R)',\n",
    "       'STD(R)', 'ENT(R)', 'AVG(G)', 'STD(G)', 'ENT(G)', 'AVG(B)', 'STD(B)',\n",
    "       'ENT(B)', 'AVG(RB)', 'STD(RB)', 'ENT(RB)', 'AVG(NRB)', 'STD(NRB)',\n",
    "       'ENT(NRB)']\n",
    "\n",
    "# 2. Calculate the number of NaN values for each feature\n",
    "# We use .isna() instead of == 0\n",
    "nans_per_feature = df[features_to_check].isna().sum()\n",
    "\n",
    "# 3. Filter to get only features that actually have NaN values\n",
    "features_with_nans = nans_per_feature[nans_per_feature > 0]\n",
    "\n",
    "# 4. Report the findings for which features have NaNs\n",
    "if features_with_nans.empty:\n",
    "    print(\"No NaN (missing) values found in any of the specified features.\")\n",
    "else:\n",
    "    print(\"--- Features With NaN Values ---\")\n",
    "    print(\"The following features have NaN values, with the total count for each:\")\n",
    "    # Sort for clearer output\n",
    "    print(features_with_nans.sort_values(ascending=False))\n",
    "    print(\"\\n\" + \"=\"*40 + \"\\n\")\n",
    "\n",
    "    # 5. Analyze the distribution of hours for rows containing NaNs\n",
    "    print(\"--- Distribution of NaN-Value Records by Hour ---\")\n",
    "    \n",
    "    # Create a boolean mask for rows that contain *at least one* NaN\n",
    "    # in the specified columns\n",
    "    rows_with_any_nan = df[features_to_check].isna().any(axis=1)\n",
    "    \n",
    "    if rows_with_any_nan.sum() > 0:\n",
    "        # Get the index for these rows\n",
    "        nan_rows_index = df.index[rows_with_any_nan]\n",
    "        \n",
    "        # Extract the hour from the DatetimeIndex and get the value counts\n",
    "        hour_distribution = nan_rows_index.hour.value_counts().sort_index()\n",
    "        \n",
    "        print(\"Distribution of records (rows) containing at least one NaN, by hour:\")\n",
    "        print(hour_distribution)\n",
    "        \n",
    "        # Optional: Print total number of affected rows\n",
    "        print(f\"\\nTotal number of rows with at least one NaN: {rows_with_any_nan.sum()}\")\n",
    "    else:\n",
    "        # This case shouldn't be hit if features_with_nans was not empty,\n",
    "        # but it's good practice to include.\n",
    "        print(\"No rows found with NaN values (this is unexpected, check logic).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2971adc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4035714/931910069.py:1: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df_interpolated = df.interpolate(method='linear')\n"
     ]
    }
   ],
   "source": [
    "df_interpolated = df.interpolate(method='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74827a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4035714/1599088726.py:1: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df_interpolated2 = df2.interpolate(method='linear')\n"
     ]
    }
   ],
   "source": [
    "df_interpolated2 = df2.interpolate(method='linear')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33763587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Features With NaN Values ---\n",
      "The following features have NaN values, with the total count for each:\n",
      "AVG(R)      1\n",
      "STD(R)      1\n",
      "ENT(R)      1\n",
      "AVG(G)      1\n",
      "STD(G)      1\n",
      "ENT(G)      1\n",
      "AVG(B)      1\n",
      "STD(B)      1\n",
      "ENT(B)      1\n",
      "AVG(RB)     1\n",
      "STD(RB)     1\n",
      "ENT(RB)     1\n",
      "AVG(NRB)    1\n",
      "STD(NRB)    1\n",
      "ENT(NRB)    1\n",
      "dtype: int64\n",
      "\n",
      "========================================\n",
      "\n",
      "--- Distribution of NaN-Value Records by Hour ---\n",
      "Distribution of records (rows) containing at least one NaN, by hour:\n",
      "measurement_time\n",
      "14    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total number of rows with at least one NaN: 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "features_to_check = ['ghi', 'dni', 'solar_zenith', 'GHI_cs', 'DNI_cs', 'CSI_ghi', \n",
    "                                   'CSI_dni',\n",
    "                                   'nam_ghi', 'nam_dni', 'nam_cc', 'B_CSI_ghi_8h', 'V_CSI_ghi_8h', 'L_CSI_ghi_8h', 'B_CSI_ghi_9h',\n",
    "       'V_CSI_ghi_9h', 'L_CSI_ghi_9h', 'B_CSI_ghi_10h', 'V_CSI_ghi_10h',\n",
    "       'L_CSI_ghi_10h', 'B_CSI_ghi_11h', 'V_CSI_ghi_11h', 'L_CSI_ghi_11h',\n",
    "       'B_CSI_ghi_12h', 'V_CSI_ghi_12h', 'L_CSI_ghi_12h', 'B_CSI_ghi_13h',\n",
    "       'V_CSI_ghi_13h', 'L_CSI_ghi_13h', 'B_CSI_ghi_14h', 'V_CSI_ghi_14h',\n",
    "       'L_CSI_ghi_14h', 'B_CSI_ghi_15h', 'V_CSI_ghi_15h', 'L_CSI_ghi_15h',\n",
    "       'B_CSI_ghi_16h', 'V_CSI_ghi_16h', 'L_CSI_ghi_16h', 'B_CSI_ghi_17h',\n",
    "       'V_CSI_ghi_17h', 'L_CSI_ghi_17h', 'B_CSI_ghi_18h', 'V_CSI_ghi_18h',\n",
    "       'L_CSI_ghi_18h', 'B_CSI_ghi_19h', 'V_CSI_ghi_19h', 'L_CSI_ghi_19h',\n",
    "       '80_dwsw', '80_cloud_cover', '56_dwsw', '56_cloud_cover',\n",
    "       '20_dwsw', '20_cloud_cover', '88_dwsw', '88_cloud_cover', 'AVG(R)',\n",
    "       'STD(R)', 'ENT(R)', 'AVG(G)', 'STD(G)', 'ENT(G)', 'AVG(B)', 'STD(B)',\n",
    "       'ENT(B)', 'AVG(RB)', 'STD(RB)', 'ENT(RB)', 'AVG(NRB)', 'STD(NRB)',\n",
    "       'ENT(NRB)']\n",
    "\n",
    "# 2. Calculate the number of NaN values for each feature\n",
    "# We use .isna() instead of == 0\n",
    "nans_per_feature = df_interpolated[features_to_check].isna().sum()\n",
    "\n",
    "# 3. Filter to get only features that actually have NaN values\n",
    "features_with_nans = nans_per_feature[nans_per_feature > 0]\n",
    "\n",
    "# 4. Report the findings for which features have NaNs\n",
    "if features_with_nans.empty:\n",
    "    print(\"No NaN (missing) values found in any of the specified features.\")\n",
    "else:\n",
    "    print(\"--- Features With NaN Values ---\")\n",
    "    print(\"The following features have NaN values, with the total count for each:\")\n",
    "    # Sort for clearer output\n",
    "    print(features_with_nans.sort_values(ascending=False))\n",
    "    print(\"\\n\" + \"=\"*40 + \"\\n\")\n",
    "\n",
    "    # 5. Analyze the distribution of hours for rows containing NaNs\n",
    "    print(\"--- Distribution of NaN-Value Records by Hour ---\")\n",
    "    \n",
    "    # Create a boolean mask for rows that contain *at least one* NaN\n",
    "    # in the specified columns\n",
    "    rows_with_any_nan = df_interpolated[features_to_check].isna().any(axis=1)\n",
    "    \n",
    "    if rows_with_any_nan.sum() > 0:\n",
    "        # Get the index for these rows\n",
    "        nan_rows_index = df_interpolated.index[rows_with_any_nan]\n",
    "        \n",
    "        # Extract the hour from the DatetimeIndex and get the value counts\n",
    "        hour_distribution = nan_rows_index.hour.value_counts().sort_index()\n",
    "        \n",
    "        print(\"Distribution of records (rows) containing at least one NaN, by hour:\")\n",
    "        print(hour_distribution)\n",
    "        \n",
    "        # Optional: Print total number of affected rows\n",
    "        print(f\"\\nTotal number of rows with at least one NaN: {rows_with_any_nan.sum()}\")\n",
    "    else:\n",
    "        # This case shouldn't be hit if features_with_nans was not empty,\n",
    "        # but it's good practice to include.\n",
    "        print(\"No rows found with NaN values (this is unexpected, check logic).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a903c55a",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e150b7f",
   "metadata": {},
   "source": [
    "### Data Preparation for Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a075dbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8fe3b436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ghi', 'dni', 'PST_Time_meas', 'solar_zenith', 'time_gap_hours',\n",
       "       'time_gap_norm', 'day_boundary_flag', 'hour_progression',\n",
       "       'absolute_hour', 'GHI_cs', 'DNI_cs', 'CSI_ghi', 'CSI_dni',\n",
       "       'season_flag', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos',\n",
       "       'nam_ghi', 'nam_dni', 'nam_cc', 'PST_Time_nam', 'nam_target_time',\n",
       "       'B_CSI_ghi_8h', 'V_CSI_ghi_8h', 'L_CSI_ghi_8h', 'B_CSI_ghi_9h',\n",
       "       'V_CSI_ghi_9h', 'L_CSI_ghi_9h', 'B_CSI_ghi_10h', 'V_CSI_ghi_10h',\n",
       "       'L_CSI_ghi_10h', 'B_CSI_ghi_11h', 'V_CSI_ghi_11h', 'L_CSI_ghi_11h',\n",
       "       'B_CSI_ghi_12h', 'V_CSI_ghi_12h', 'L_CSI_ghi_12h', 'B_CSI_ghi_13h',\n",
       "       'V_CSI_ghi_13h', 'L_CSI_ghi_13h', 'B_CSI_ghi_14h', 'V_CSI_ghi_14h',\n",
       "       'L_CSI_ghi_14h', 'B_CSI_ghi_15h', 'V_CSI_ghi_15h', 'L_CSI_ghi_15h',\n",
       "       'B_CSI_ghi_16h', 'V_CSI_ghi_16h', 'L_CSI_ghi_16h', 'B_CSI_ghi_17h',\n",
       "       'V_CSI_ghi_17h', 'L_CSI_ghi_17h', 'B_CSI_ghi_18h', 'V_CSI_ghi_18h',\n",
       "       'L_CSI_ghi_18h', 'B_CSI_ghi_19h', 'V_CSI_ghi_19h', 'L_CSI_ghi_19h',\n",
       "       '80_dwsw', 'valtime', '80_cloud_cover', '56_dwsw', '56_cloud_cover',\n",
       "       '20_dwsw', '20_cloud_cover', '88_dwsw', '88_cloud_cover', 'AVG(R)',\n",
       "       'STD(R)', 'ENT(R)', 'AVG(G)', 'STD(G)', 'ENT(G)', 'AVG(B)', 'STD(B)',\n",
       "       'ENT(B)', 'AVG(RB)', 'STD(RB)', 'ENT(RB)', 'AVG(NRB)', 'STD(NRB)',\n",
       "       'ENT(NRB)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_interpolated.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b271582",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_phase1 = df_interpolated[['solar_zenith', 'CSI_ghi','time_gap_hours',\n",
    "       'time_gap_norm', 'day_boundary_flag', 'hour_progression',\n",
    "       'absolute_hour', 'PST_Time_meas',\n",
    "       'season_flag', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos']]\n",
    "\n",
    "\n",
    "df_phase2 = df_interpolated[['solar_zenith', 'CSI_ghi','time_gap_hours',\n",
    "       'time_gap_norm', 'day_boundary_flag', 'hour_progression',\n",
    "       'absolute_hour',\n",
    "       'season_flag', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos', 'B_CSI_ghi_8h', 'V_CSI_ghi_8h', 'L_CSI_ghi_8h', 'B_CSI_ghi_9h',\n",
    "       'V_CSI_ghi_9h', 'L_CSI_ghi_9h', 'B_CSI_ghi_10h', 'V_CSI_ghi_10h',\n",
    "       'L_CSI_ghi_10h', 'B_CSI_ghi_11h', 'V_CSI_ghi_11h', 'L_CSI_ghi_11h',\n",
    "       'B_CSI_ghi_12h', 'V_CSI_ghi_12h', 'L_CSI_ghi_12h', 'B_CSI_ghi_13h',\n",
    "       'V_CSI_ghi_13h', 'L_CSI_ghi_13h', 'B_CSI_ghi_14h', 'V_CSI_ghi_14h',\n",
    "       'L_CSI_ghi_14h', 'B_CSI_ghi_15h', 'V_CSI_ghi_15h', 'L_CSI_ghi_15h',\n",
    "       'B_CSI_ghi_16h', 'V_CSI_ghi_16h', 'L_CSI_ghi_16h', 'B_CSI_ghi_17h',\n",
    "       'V_CSI_ghi_17h', 'L_CSI_ghi_17h', 'B_CSI_ghi_18h', 'V_CSI_ghi_18h',\n",
    "       'L_CSI_ghi_18h', 'B_CSI_ghi_19h', 'V_CSI_ghi_19h', 'L_CSI_ghi_19h']]\n",
    "\n",
    "df_phase3 = df_interpolated[['solar_zenith', 'CSI_ghi','time_gap_hours',\n",
    "       'time_gap_norm', 'day_boundary_flag', 'hour_progression',\n",
    "       'absolute_hour',\n",
    "       'season_flag', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos', 'B_CSI_ghi_8h', 'V_CSI_ghi_8h', 'L_CSI_ghi_8h', 'B_CSI_ghi_9h',\n",
    "       'V_CSI_ghi_9h', 'L_CSI_ghi_9h', 'B_CSI_ghi_10h', 'V_CSI_ghi_10h',\n",
    "       'L_CSI_ghi_10h', 'B_CSI_ghi_11h', 'V_CSI_ghi_11h', 'L_CSI_ghi_11h',\n",
    "       'B_CSI_ghi_12h', 'V_CSI_ghi_12h', 'L_CSI_ghi_12h', 'B_CSI_ghi_13h',\n",
    "       'V_CSI_ghi_13h', 'L_CSI_ghi_13h', 'B_CSI_ghi_14h', 'V_CSI_ghi_14h',\n",
    "       'L_CSI_ghi_14h', 'B_CSI_ghi_15h', 'V_CSI_ghi_15h', 'L_CSI_ghi_15h',\n",
    "       'B_CSI_ghi_16h', 'V_CSI_ghi_16h', 'L_CSI_ghi_16h', 'B_CSI_ghi_17h',\n",
    "       'V_CSI_ghi_17h', 'L_CSI_ghi_17h', 'B_CSI_ghi_18h', 'V_CSI_ghi_18h',\n",
    "       'L_CSI_ghi_18h', 'B_CSI_ghi_19h', 'V_CSI_ghi_19h', 'L_CSI_ghi_19h',\n",
    "       'nam_ghi', 'nam_cc']]\n",
    "\n",
    "df_phase4 = df_interpolated[['solar_zenith', 'CSI_ghi','time_gap_hours',\n",
    "       'time_gap_norm', 'day_boundary_flag', 'hour_progression',\n",
    "       'absolute_hour',\n",
    "       'season_flag', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos', 'B_CSI_ghi_8h', 'V_CSI_ghi_8h', 'L_CSI_ghi_8h', 'B_CSI_ghi_9h',\n",
    "       'V_CSI_ghi_9h', 'L_CSI_ghi_9h', 'B_CSI_ghi_10h', 'V_CSI_ghi_10h',\n",
    "       'L_CSI_ghi_10h', 'B_CSI_ghi_11h', 'V_CSI_ghi_11h', 'L_CSI_ghi_11h',\n",
    "       'B_CSI_ghi_12h', 'V_CSI_ghi_12h', 'L_CSI_ghi_12h', 'B_CSI_ghi_13h',\n",
    "       'V_CSI_ghi_13h', 'L_CSI_ghi_13h', 'B_CSI_ghi_14h', 'V_CSI_ghi_14h',\n",
    "       'L_CSI_ghi_14h', 'B_CSI_ghi_15h', 'V_CSI_ghi_15h', 'L_CSI_ghi_15h',\n",
    "       'B_CSI_ghi_16h', 'V_CSI_ghi_16h', 'L_CSI_ghi_16h', 'B_CSI_ghi_17h',\n",
    "       'V_CSI_ghi_17h', 'L_CSI_ghi_17h', 'B_CSI_ghi_18h', 'V_CSI_ghi_18h',\n",
    "       'L_CSI_ghi_18h', 'B_CSI_ghi_19h', 'V_CSI_ghi_19h', 'L_CSI_ghi_19h',\n",
    "       'nam_ghi', 'nam_cc', '80_dwsw', 'valtime', '80_cloud_cover', '56_dwsw', '56_cloud_cover',\n",
    "       '20_dwsw', '20_cloud_cover', '88_dwsw', '88_cloud_cover']]\n",
    "\n",
    "df_phase5 = df_interpolated[['solar_zenith', 'CSI_ghi','time_gap_hours',\n",
    "       'time_gap_norm', 'day_boundary_flag', 'hour_progression',\n",
    "       'absolute_hour',\n",
    "       'season_flag', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos', 'B_CSI_ghi_8h', 'V_CSI_ghi_8h', 'L_CSI_ghi_8h', 'B_CSI_ghi_9h',\n",
    "       'V_CSI_ghi_9h', 'L_CSI_ghi_9h', 'B_CSI_ghi_10h', 'V_CSI_ghi_10h',\n",
    "       'L_CSI_ghi_10h', 'B_CSI_ghi_11h', 'V_CSI_ghi_11h', 'L_CSI_ghi_11h',\n",
    "       'B_CSI_ghi_12h', 'V_CSI_ghi_12h', 'L_CSI_ghi_12h', 'B_CSI_ghi_13h',\n",
    "       'V_CSI_ghi_13h', 'L_CSI_ghi_13h', 'B_CSI_ghi_14h', 'V_CSI_ghi_14h',\n",
    "       'L_CSI_ghi_14h', 'B_CSI_ghi_15h', 'V_CSI_ghi_15h', 'L_CSI_ghi_15h',\n",
    "       'B_CSI_ghi_16h', 'V_CSI_ghi_16h', 'L_CSI_ghi_16h', 'B_CSI_ghi_17h',\n",
    "       'V_CSI_ghi_17h', 'L_CSI_ghi_17h', 'B_CSI_ghi_18h', 'V_CSI_ghi_18h',\n",
    "       'L_CSI_ghi_18h', 'B_CSI_ghi_19h', 'V_CSI_ghi_19h', 'L_CSI_ghi_19h',\n",
    "       'nam_ghi', 'nam_cc','80_cloud_cover',  '56_cloud_cover',\n",
    "        '20_cloud_cover', '88_cloud_cover']]\n",
    "\n",
    "df_phase6 = df_interpolated[['solar_zenith', 'CSI_ghi','time_gap_hours',\n",
    "       'time_gap_norm', 'day_boundary_flag', 'hour_progression',\n",
    "       'absolute_hour',\n",
    "       'season_flag', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos', 'B_CSI_ghi_8h', 'V_CSI_ghi_8h', 'L_CSI_ghi_8h', 'B_CSI_ghi_9h',\n",
    "       'V_CSI_ghi_9h', 'L_CSI_ghi_9h', 'B_CSI_ghi_10h', 'V_CSI_ghi_10h',\n",
    "       'L_CSI_ghi_10h', 'B_CSI_ghi_11h', 'V_CSI_ghi_11h', 'L_CSI_ghi_11h',\n",
    "       'B_CSI_ghi_12h', 'V_CSI_ghi_12h', 'L_CSI_ghi_12h', 'B_CSI_ghi_13h',\n",
    "       'V_CSI_ghi_13h', 'L_CSI_ghi_13h', 'B_CSI_ghi_14h', 'V_CSI_ghi_14h',\n",
    "       'L_CSI_ghi_14h', 'B_CSI_ghi_15h', 'V_CSI_ghi_15h', 'L_CSI_ghi_15h',\n",
    "       'B_CSI_ghi_16h', 'V_CSI_ghi_16h', 'L_CSI_ghi_16h', 'B_CSI_ghi_17h',\n",
    "       'V_CSI_ghi_17h', 'L_CSI_ghi_17h', 'B_CSI_ghi_18h', 'V_CSI_ghi_18h',\n",
    "       'L_CSI_ghi_18h', 'B_CSI_ghi_19h', 'V_CSI_ghi_19h', 'L_CSI_ghi_19h',\n",
    "       'nam_ghi', 'nam_cc', '80_dwsw', 'valtime', '80_cloud_cover', '56_dwsw', '56_cloud_cover',\n",
    "       '20_dwsw', '20_cloud_cover', '88_dwsw', '88_cloud_cover'\n",
    "      ]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abe0f33",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe8b05e",
   "metadata": {},
   "source": [
    "## Testing and Validating the Data Preparation for traning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a0abf9",
   "metadata": {},
   "source": [
    "### Step 01: Defining Paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f46c81c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = df_interpolated[['solar_zenith', 'absolute_hour', 'season_flag', 'CSI_ghi']]\n",
    "TARGET_COL = \"CSI_ghi\"  \n",
    "history_days = 7\n",
    "horizon_days = 1\n",
    "timestamp_col=\"measurement_time\"\n",
    "target_time_col= \"nam_target_time\"\n",
    "\n",
    "feature_cols = [c for c in test_df.columns.tolist() if c != TARGET_COL]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9497e6dc",
   "metadata": {},
   "source": [
    "### Step 02: Building model arrays (X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d3452245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11560, 4)\n"
     ]
    }
   ],
   "source": [
    "from src.preprocessing import build_model_arrays, to_fixedgrid_multiindex\n",
    "\n",
    "fixed_df = to_fixedgrid_multiindex(test_df, timestamp_col=\"measurement_time\", expected_T=None)\n",
    "print(fixed_df.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62e1490c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>solar_zenith</th>\n",
       "      <th>absolute_hour</th>\n",
       "      <th>season_flag</th>\n",
       "      <th>CSI_ghi</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th>bin_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2014-01-03</th>\n",
       "      <th>0</th>\n",
       "      <td>100.276046</td>\n",
       "      <td>24.491667</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>89.724162</td>\n",
       "      <td>25.491667</td>\n",
       "      <td>2</td>\n",
       "      <td>1.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>80.146724</td>\n",
       "      <td>26.491667</td>\n",
       "      <td>2</td>\n",
       "      <td>1.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>71.988814</td>\n",
       "      <td>27.491667</td>\n",
       "      <td>2</td>\n",
       "      <td>1.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>65.816158</td>\n",
       "      <td>28.491667</td>\n",
       "      <td>2</td>\n",
       "      <td>1.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2016-12-30</th>\n",
       "      <th>6</th>\n",
       "      <td>62.502377</td>\n",
       "      <td>26237.491667</td>\n",
       "      <td>2</td>\n",
       "      <td>1.051822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>62.079248</td>\n",
       "      <td>26238.491667</td>\n",
       "      <td>2</td>\n",
       "      <td>1.001250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>64.765899</td>\n",
       "      <td>26239.491667</td>\n",
       "      <td>2</td>\n",
       "      <td>0.961864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>70.203098</td>\n",
       "      <td>26240.491667</td>\n",
       "      <td>2</td>\n",
       "      <td>0.896756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>77.805433</td>\n",
       "      <td>26241.491667</td>\n",
       "      <td>2</td>\n",
       "      <td>0.804137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11560 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   solar_zenith  absolute_hour  season_flag   CSI_ghi\n",
       "date       bin_id                                                    \n",
       "2014-01-03 0         100.276046      24.491667            2  0.000000\n",
       "           1          89.724162      25.491667            2  1.200000\n",
       "           2          80.146724      26.491667            2  1.200000\n",
       "           3          71.988814      27.491667            2  1.200000\n",
       "           4          65.816158      28.491667            2  1.200000\n",
       "...                         ...            ...          ...       ...\n",
       "2016-12-30 6          62.502377   26237.491667            2  1.051822\n",
       "           7          62.079248   26238.491667            2  1.001250\n",
       "           8          64.765899   26239.491667            2  0.961864\n",
       "           9          70.203098   26240.491667            2  0.896756\n",
       "           10         77.805433   26241.491667            2  0.804137\n",
       "\n",
       "[11560 rows x 4 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b2e74e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1055, 7, 11, 3) (1055, 1, 11) 1055\n"
     ]
    }
   ],
   "source": [
    "X, Y, labels_list = build_model_arrays(\n",
    "        fixed_df,\n",
    "        feature_cols=feature_cols, \n",
    "        target_col=TARGET_COL,\n",
    "        history_days=history_days,\n",
    "        horizon_days=horizon_days,\n",
    "    )\n",
    "print(X.shape, Y.shape, len(labels_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42ca02d",
   "metadata": {},
   "source": [
    "### Step 03: Save Arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "135297ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.utils:Saved arrays to data/test_data_*.npy\n",
      "INFO:src.utils:X shape: (1055, 7, 11, 3), Y shape: (1055, 1, 11)\n"
     ]
    }
   ],
   "source": [
    "from src.utils import DataManager\n",
    "\n",
    "data_manager = DataManager()\n",
    "data_manager.save_arrays(\n",
    "    X, Y,\n",
    "    pd.DataFrame(index=pd.to_datetime(labels_list, utc=True)),\n",
    "    filename_prefix='test_data', \n",
    "    feature_cols=feature_cols,\n",
    "    target_col=TARGET_COL,\n",
    "    metadata={\n",
    "        \"input_csv\": \"data/processed/df_1h_lag_BLV_spatial_images.csv\",\n",
    "        \"timestamp_col\": \"measurement_time\",\n",
    "        \"feature_set\": feature_cols,\n",
    "        \"history_days\": 7,\n",
    "        \"horizon_days\": 1\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dccebc3",
   "metadata": {},
   "source": [
    "### Step 03: Configure Model and Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bac48785",
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_CONFIG = {\n",
    "    \"experiment_name\": \"TEST_UniLSTM_test_data\",\n",
    "    \"model_type\": \"LSTM\",\n",
    "    \"model_config\": {\n",
    "        \"hidden_size\": 64,\n",
    "        \"num_layers\": 2,\n",
    "        \"steps_per_day\": 11,\n",
    "        \"dropout\": 0.35,\n",
    "        \"bidirectional\": True,\n",
    "        \"use_attention\": False\n",
    "    },\n",
    "    \"data_prefix\": \"test_data\",  # <-- FIX 2: Match the filename_prefix\n",
    "    \"splits_file\": \"exp-004/exp-004rolling_origin_splits.json\",\n",
    "    \"feature_cols\": feature_cols,\n",
    "    \"feature_selection\": feature_cols, # <-- This is correct!\n",
    "    \"target_col\": TARGET_COL,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_epochs\": 50,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"loss_function\": \"Huber\",  # <-- This setting needs the fix below\n",
    "    \"early_stopping_patience\": 20,\n",
    "    \"max_folds\": 3,\n",
    "    \"scale_target\": False,\n",
    "    \n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7dc4ce",
   "metadata": {},
   "source": [
    "### Step 04: Investigation of SolarForecastingPipeline \n",
    "\n",
    "`pipeline = SolarForecastingPipeline(LSTM_CONFIG)`\n",
    "\n",
    "`_, summary = pipeline.run()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c53b3459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiement Directory: experiments/TEST_UniLSTM_test_data_20251107_155345 , Experiemnt Name: TEST_UniLSTM_test_data\n",
      "Data Directory: data, Splits Directory: splits, Processing Device cuda\n"
     ]
    }
   ],
   "source": [
    "from src.pipeline import SolarForecastingPipeline\n",
    "\n",
    "# 1. Initializing the Pipeline with Model Configuration\n",
    "#   1.1 Once pipeline has initialized, the following modules instance will be initialized\n",
    "#       - DataManager \n",
    "#       - ExperimentTracker\n",
    "#       - Available device\n",
    "#   1.2 Model Configuration will be passed into the self.config\n",
    "pipeline = SolarForecastingPipeline(LSTM_CONFIG)\n",
    "\n",
    "print(f\"Experiement Directory: {pipeline.tracker.exp_dir} , Experiemnt Name: {pipeline.tracker.experiment_name}\")\n",
    "print(f\"Data Directory: {pipeline.data_manager.data_dir}, Splits Directory: {pipeline.data_manager.splits_dir}, Processing Device {pipeline.device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "997f17d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n{\\n  \"experiment_name\": \"TEST_UniLSTM_test_data\",\\n  \"model_type\": \"LSTM\",\\n  \"model_config\": {\\n    \"hidden_size\": 64,\\n    \"num_layers\": 2,\\n    \"dropout\": 0.35,\\n    \"bidirectional\": true\\n  },\\n  \"data_prefix\": \"test_data\",\\n  \"splits_file\": \"exp-004/exp-004rolling_origin_splits.json\",\\n  \"feature_cols\": [\\n    \"solar_zenith\",\\n    \"absolute_hour\",\\n    \"season_flag\"\\n  ],\\n  \"feature_selection\": [\\n    \"solar_zenith\",\\n    \"absolute_hour\",\\n    \"season_flag\"\\n  ],\\n  \"target_col\": \"CSI_ghi\",\\n  \"batch_size\": 32,\\n  \"num_epochs\": 50,\\n  \"learning_rate\": 0.001,\\n  \"loss_function\": \"Huber\",\\n  \"early_stopping_patience\": 20,\\n  \"max_folds\": 3\\n}\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Run....\n",
    "# Save configuration to json file. \n",
    "pipeline.tracker.save_config(pipeline.config)\n",
    "\n",
    "# After this step TEST_UniLSTM_test_data_{dat_time} dire for the exp will be created\n",
    "# config.json \n",
    "\n",
    "\"\"\"\n",
    "{\n",
    "  \"experiment_name\": \"TEST_UniLSTM_test_data\",\n",
    "  \"model_type\": \"LSTM\",\n",
    "  \"model_config\": {\n",
    "    \"hidden_size\": 64,\n",
    "    \"num_layers\": 2,\n",
    "    \"dropout\": 0.35,\n",
    "    \"bidirectional\": true\n",
    "  },\n",
    "  \"data_prefix\": \"test_data\",\n",
    "  \"splits_file\": \"exp-004/exp-004rolling_origin_splits.json\",\n",
    "  \"feature_cols\": [\n",
    "    \"solar_zenith\",\n",
    "    \"absolute_hour\",\n",
    "    \"season_flag\"\n",
    "  ],\n",
    "  \"feature_selection\": [\n",
    "    \"solar_zenith\",\n",
    "    \"absolute_hour\",\n",
    "    \"season_flag\"\n",
    "  ],\n",
    "  \"target_col\": \"CSI_ghi\",\n",
    "  \"batch_size\": 32,\n",
    "  \"num_epochs\": 50,\n",
    "  \"learning_rate\": 0.001,\n",
    "  \"loss_function\": \"Huber\",\n",
    "  \"early_stopping_patience\": 20,\n",
    "  \"max_folds\": 3\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# metrics, models, and plots directory to track the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "00a85d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.utils:Loaded arrays from data/test_data_*.npy\n",
      "INFO:src.utils:X shape: (1055, 7, 11, 3), Y shape: (1055, 1, 11)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "# Data already stored in data directory, with prefix test_data.\n",
    "# So we have test_data_X.npy, test_data_Y.npy, test_data_labels.pkl\n",
    "# test_data_meta_data.json\n",
    "\n",
    "\"\"\"\n",
    "{\n",
    "  \"X_shape\": [\n",
    "    1055,\n",
    "    7,\n",
    "    11,\n",
    "    3\n",
    "  ],\n",
    "  \"Y_shape\": [\n",
    "    1055,\n",
    "    1,\n",
    "    11\n",
    "  ],\n",
    "  \"X_dtype\": \"float64\",\n",
    "  \"Y_dtype\": \"float64\",\n",
    "  \"saved_at\": \"2025-11-07T13:18:40.072858\",\n",
    "  \"has_labels\": true,\n",
    "  \"feature_cols\": [\n",
    "    \"solar_zenith\",\n",
    "    \"absolute_hour\",\n",
    "    \"season_flag\"\n",
    "  ],\n",
    "  \"target_col\": \"CSI_ghi\",\n",
    "  \"input_csv\": \"data/processed/df_1h_lag_BLV_spatial_images.csv\",\n",
    "  \"timestamp_col\": \"measurement_time\",\n",
    "  \"feature_set\": [\n",
    "    \"solar_zenith\",\n",
    "    \"absolute_hour\",\n",
    "    \"season_flag\"\n",
    "  ],\n",
    "  \"history_days\": 7,\n",
    "  \"horizon_days\": 1\n",
    "}\n",
    "\"\"\"\n",
    "X, Y, labels, metadata = pipeline.data_manager.load_arrays(\n",
    "            filename_prefix=pipeline.config.get('data_prefix', 'kbins_data')\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ade7751c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-01-10 00:00:00+00:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-11 00:00:00+00:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-12 00:00:00+00:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-13 00:00:00+00:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-14 00:00:00+00:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-26 00:00:00+00:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-27 00:00:00+00:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-28 00:00:00+00:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-29 00:00:00+00:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-30 00:00:00+00:00</th>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1055 rows × 0 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: [2014-01-10 00:00:00+00:00, 2014-01-11 00:00:00+00:00, 2014-01-12 00:00:00+00:00, 2014-01-13 00:00:00+00:00, 2014-01-14 00:00:00+00:00, 2014-01-15 00:00:00+00:00, 2014-01-16 00:00:00+00:00, 2014-01-17 00:00:00+00:00, 2014-01-18 00:00:00+00:00, 2014-01-19 00:00:00+00:00, 2014-01-20 00:00:00+00:00, 2014-01-21 00:00:00+00:00, 2014-01-22 00:00:00+00:00, 2014-01-23 00:00:00+00:00, 2014-01-24 00:00:00+00:00, 2014-01-25 00:00:00+00:00, 2014-01-26 00:00:00+00:00, 2014-01-27 00:00:00+00:00, 2014-01-28 00:00:00+00:00, 2014-01-29 00:00:00+00:00, 2014-01-30 00:00:00+00:00, 2014-02-05 00:00:00+00:00, 2014-02-06 00:00:00+00:00, 2014-02-07 00:00:00+00:00, 2014-02-08 00:00:00+00:00, 2014-02-09 00:00:00+00:00, 2014-02-10 00:00:00+00:00, 2014-02-11 00:00:00+00:00, 2014-02-12 00:00:00+00:00, 2014-02-13 00:00:00+00:00, 2014-02-19 00:00:00+00:00, 2014-02-20 00:00:00+00:00, 2014-02-21 00:00:00+00:00, 2014-02-22 00:00:00+00:00, 2014-02-23 00:00:00+00:00, 2014-02-24 00:00:00+00:00, 2014-02-25 00:00:00+00:00, 2014-02-26 00:00:00+00:00, 2014-02-27 00:00:00+00:00, 2014-02-28 00:00:00+00:00, 2014-03-01 00:00:00+00:00, 2014-03-02 00:00:00+00:00, 2014-03-03 00:00:00+00:00, 2014-03-04 00:00:00+00:00, 2014-03-05 00:00:00+00:00, 2014-03-06 00:00:00+00:00, 2014-03-07 00:00:00+00:00, 2014-03-08 00:00:00+00:00, 2014-03-09 00:00:00+00:00, 2014-03-10 00:00:00+00:00, 2014-03-11 00:00:00+00:00, 2014-03-12 00:00:00+00:00, 2014-03-13 00:00:00+00:00, 2014-03-14 00:00:00+00:00, 2014-03-15 00:00:00+00:00, 2014-03-16 00:00:00+00:00, 2014-03-17 00:00:00+00:00, 2014-03-18 00:00:00+00:00, 2014-03-19 00:00:00+00:00, 2014-03-20 00:00:00+00:00, 2014-03-21 00:00:00+00:00, 2014-03-22 00:00:00+00:00, 2014-03-23 00:00:00+00:00, 2014-03-24 00:00:00+00:00, 2014-03-25 00:00:00+00:00, 2014-03-26 00:00:00+00:00, 2014-03-27 00:00:00+00:00, 2014-03-28 00:00:00+00:00, 2014-03-29 00:00:00+00:00, 2014-03-30 00:00:00+00:00, 2014-03-31 00:00:00+00:00, 2014-04-01 00:00:00+00:00, 2014-04-02 00:00:00+00:00, 2014-04-03 00:00:00+00:00, 2014-04-04 00:00:00+00:00, 2014-04-05 00:00:00+00:00, 2014-04-06 00:00:00+00:00, 2014-04-07 00:00:00+00:00, 2014-04-08 00:00:00+00:00, 2014-04-09 00:00:00+00:00, 2014-04-10 00:00:00+00:00, 2014-04-11 00:00:00+00:00, 2014-04-12 00:00:00+00:00, 2014-04-13 00:00:00+00:00, 2014-04-14 00:00:00+00:00, 2014-04-15 00:00:00+00:00, 2014-04-16 00:00:00+00:00, 2014-04-17 00:00:00+00:00, 2014-04-18 00:00:00+00:00, 2014-04-19 00:00:00+00:00, 2014-04-20 00:00:00+00:00, 2014-04-21 00:00:00+00:00, 2014-04-22 00:00:00+00:00, 2014-04-23 00:00:00+00:00, 2014-04-24 00:00:00+00:00, 2014-04-25 00:00:00+00:00, 2014-04-26 00:00:00+00:00, 2014-04-27 00:00:00+00:00, 2014-04-28 00:00:00+00:00, 2014-04-29 00:00:00+00:00, ...]\n",
       "\n",
       "[1055 rows x 0 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "30256215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['solar_zenith', 'absolute_hour', 'season_flag']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_features = metadata.get(\"feature_cols\")\n",
    "saved_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b391d043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['solar_zenith', 'absolute_hour', 'season_flag']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if the metadata matches the proposed configuration.\n",
    "expected_features = pipeline.config.get(\"feature_cols\")\n",
    "expected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bd3114c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign feature_cols\n",
    "pipeline.config[\"feature_cols\"] = saved_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "648de274",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation_utils import _load_processed_dataframe\n",
    "from src.config import PROCESSED_DATA_PATH\n",
    "# Source of data\n",
    "PROCESSED_DATA_PATH = \"data/processed/df_1h_lag_BLV_spatial_images.csv\"\n",
    "csv_path = metadata.get(\"input_csv\", PROCESSED_DATA_PATH) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8d54c1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df = _load_processed_dataframe(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "54b2da1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ghi</th>\n",
       "      <th>dni</th>\n",
       "      <th>PST_Time_meas</th>\n",
       "      <th>solar_zenith</th>\n",
       "      <th>time_gap_hours</th>\n",
       "      <th>time_gap_norm</th>\n",
       "      <th>day_boundary_flag</th>\n",
       "      <th>hour_progression</th>\n",
       "      <th>absolute_hour</th>\n",
       "      <th>...</th>\n",
       "      <th>ENT(G)</th>\n",
       "      <th>AVG(B)</th>\n",
       "      <th>STD(B)</th>\n",
       "      <th>ENT(B)</th>\n",
       "      <th>AVG(RB)</th>\n",
       "      <th>STD(RB)</th>\n",
       "      <th>ENT(RB)</th>\n",
       "      <th>AVG(NRB)</th>\n",
       "      <th>STD(NRB)</th>\n",
       "      <th>ENT(NRB)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>measurement_time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-01-03 14:00:00+00:00</th>\n",
       "      <td>2014-01-03 14:00:00+00:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2014-01-03 06:29:30-08:00</td>\n",
       "      <td>100.276046</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.006944</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>6.491667</td>\n",
       "      <td>24.491667</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-03 15:00:00+00:00</th>\n",
       "      <td>2014-01-03 15:00:00+00:00</td>\n",
       "      <td>12.914500</td>\n",
       "      <td>89.245833</td>\n",
       "      <td>2014-01-03 07:29:30-08:00</td>\n",
       "      <td>89.724162</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.491667</td>\n",
       "      <td>25.491667</td>\n",
       "      <td>...</td>\n",
       "      <td>5.564803</td>\n",
       "      <td>151.246458</td>\n",
       "      <td>28.520968</td>\n",
       "      <td>5.360367</td>\n",
       "      <td>0.808373</td>\n",
       "      <td>0.144332</td>\n",
       "      <td>4.688985</td>\n",
       "      <td>-0.113270</td>\n",
       "      <td>0.086132</td>\n",
       "      <td>4.008980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-03 16:00:00+00:00</th>\n",
       "      <td>2014-01-03 16:00:00+00:00</td>\n",
       "      <td>143.775500</td>\n",
       "      <td>636.475000</td>\n",
       "      <td>2014-01-03 08:29:30-08:00</td>\n",
       "      <td>80.146724</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.491667</td>\n",
       "      <td>26.491667</td>\n",
       "      <td>...</td>\n",
       "      <td>5.510188</td>\n",
       "      <td>141.713212</td>\n",
       "      <td>33.481258</td>\n",
       "      <td>5.414777</td>\n",
       "      <td>0.909988</td>\n",
       "      <td>0.148683</td>\n",
       "      <td>4.742913</td>\n",
       "      <td>-0.053787</td>\n",
       "      <td>0.079303</td>\n",
       "      <td>3.913008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-03 17:00:00+00:00</th>\n",
       "      <td>2014-01-03 17:00:00+00:00</td>\n",
       "      <td>289.955000</td>\n",
       "      <td>783.193333</td>\n",
       "      <td>2014-01-03 09:29:30-08:00</td>\n",
       "      <td>71.988814</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.491667</td>\n",
       "      <td>27.491667</td>\n",
       "      <td>...</td>\n",
       "      <td>5.478535</td>\n",
       "      <td>137.213417</td>\n",
       "      <td>35.403827</td>\n",
       "      <td>5.377718</td>\n",
       "      <td>0.965367</td>\n",
       "      <td>0.142668</td>\n",
       "      <td>4.686905</td>\n",
       "      <td>-0.022647</td>\n",
       "      <td>0.071275</td>\n",
       "      <td>3.777387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-03 18:00:00+00:00</th>\n",
       "      <td>2014-01-03 18:00:00+00:00</td>\n",
       "      <td>402.643333</td>\n",
       "      <td>843.050000</td>\n",
       "      <td>2014-01-03 10:29:30-08:00</td>\n",
       "      <td>65.816158</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.491667</td>\n",
       "      <td>28.491667</td>\n",
       "      <td>...</td>\n",
       "      <td>5.456043</td>\n",
       "      <td>138.616210</td>\n",
       "      <td>36.162608</td>\n",
       "      <td>5.356960</td>\n",
       "      <td>0.947050</td>\n",
       "      <td>0.140950</td>\n",
       "      <td>4.682247</td>\n",
       "      <td>-0.032250</td>\n",
       "      <td>0.072482</td>\n",
       "      <td>3.788302</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Unnamed: 0         ghi         dni  \\\n",
       "measurement_time                                                               \n",
       "2014-01-03 14:00:00+00:00  2014-01-03 14:00:00+00:00    0.000000    0.000000   \n",
       "2014-01-03 15:00:00+00:00  2014-01-03 15:00:00+00:00   12.914500   89.245833   \n",
       "2014-01-03 16:00:00+00:00  2014-01-03 16:00:00+00:00  143.775500  636.475000   \n",
       "2014-01-03 17:00:00+00:00  2014-01-03 17:00:00+00:00  289.955000  783.193333   \n",
       "2014-01-03 18:00:00+00:00  2014-01-03 18:00:00+00:00  402.643333  843.050000   \n",
       "\n",
       "                                       PST_Time_meas  solar_zenith  \\\n",
       "measurement_time                                                     \n",
       "2014-01-03 14:00:00+00:00  2014-01-03 06:29:30-08:00    100.276046   \n",
       "2014-01-03 15:00:00+00:00  2014-01-03 07:29:30-08:00     89.724162   \n",
       "2014-01-03 16:00:00+00:00  2014-01-03 08:29:30-08:00     80.146724   \n",
       "2014-01-03 17:00:00+00:00  2014-01-03 09:29:30-08:00     71.988814   \n",
       "2014-01-03 18:00:00+00:00  2014-01-03 10:29:30-08:00     65.816158   \n",
       "\n",
       "                           time_gap_hours  time_gap_norm  day_boundary_flag  \\\n",
       "measurement_time                                                              \n",
       "2014-01-03 14:00:00+00:00        0.166667       0.006944           0.016667   \n",
       "2014-01-03 15:00:00+00:00        0.016667       0.000694           0.000000   \n",
       "2014-01-03 16:00:00+00:00        0.016667       0.000694           0.000000   \n",
       "2014-01-03 17:00:00+00:00        0.016667       0.000694           0.000000   \n",
       "2014-01-03 18:00:00+00:00        0.016667       0.000694           0.000000   \n",
       "\n",
       "                           hour_progression  absolute_hour  ...    ENT(G)  \\\n",
       "measurement_time                                            ...             \n",
       "2014-01-03 14:00:00+00:00          6.491667      24.491667  ...       NaN   \n",
       "2014-01-03 15:00:00+00:00          7.491667      25.491667  ...  5.564803   \n",
       "2014-01-03 16:00:00+00:00          8.491667      26.491667  ...  5.510188   \n",
       "2014-01-03 17:00:00+00:00          9.491667      27.491667  ...  5.478535   \n",
       "2014-01-03 18:00:00+00:00         10.491667      28.491667  ...  5.456043   \n",
       "\n",
       "                               AVG(B)     STD(B)    ENT(B)   AVG(RB)  \\\n",
       "measurement_time                                                       \n",
       "2014-01-03 14:00:00+00:00         NaN        NaN       NaN       NaN   \n",
       "2014-01-03 15:00:00+00:00  151.246458  28.520968  5.360367  0.808373   \n",
       "2014-01-03 16:00:00+00:00  141.713212  33.481258  5.414777  0.909988   \n",
       "2014-01-03 17:00:00+00:00  137.213417  35.403827  5.377718  0.965367   \n",
       "2014-01-03 18:00:00+00:00  138.616210  36.162608  5.356960  0.947050   \n",
       "\n",
       "                            STD(RB)   ENT(RB)  AVG(NRB)  STD(NRB)  ENT(NRB)  \n",
       "measurement_time                                                             \n",
       "2014-01-03 14:00:00+00:00       NaN       NaN       NaN       NaN       NaN  \n",
       "2014-01-03 15:00:00+00:00  0.144332  4.688985 -0.113270  0.086132  4.008980  \n",
       "2014-01-03 16:00:00+00:00  0.148683  4.742913 -0.053787  0.079303  3.913008  \n",
       "2014-01-03 17:00:00+00:00  0.142668  4.686905 -0.022647  0.071275  3.777387  \n",
       "2014-01-03 18:00:00+00:00  0.140950  4.682247 -0.032250  0.072482  3.788302  \n",
       "\n",
       "[5 rows x 85 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295df434",
   "metadata": {},
   "source": [
    "#### Step 05: build_reference_from_existing Investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fc61eb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_col  = \"measurement_time\"       # your main UTC timestamp\n",
    "nam_time_col = \"nam_target_time\"     # NAM valid time (if present)\n",
    "meas_ghi_col = \"ghi\"\n",
    "nam_ghi_col = \"nam_ghi\"\n",
    "cs_ghi_col = \"GHI_cs\"              # clear-sky already in your data\n",
    "actual_csi_col = \"CSI_ghi\"\n",
    "\n",
    "has_nam_time = nam_time_col in base_df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6139e6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _to_utc_series(s):\n",
    "        s = pd.to_datetime(s, utc=True, errors=\"coerce\")\n",
    "        return s\n",
    "target_time = _to_utc_series(base_df[nam_time_col] if has_nam_time else base_df[time_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "347535c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "measurement_time\n",
       "2014-01-03 14:00:00+00:00   2014-01-03 15:00:00+00:00\n",
       "2014-01-03 15:00:00+00:00   2014-01-03 16:00:00+00:00\n",
       "2014-01-03 16:00:00+00:00   2014-01-03 17:00:00+00:00\n",
       "2014-01-03 17:00:00+00:00   2014-01-03 18:00:00+00:00\n",
       "2014-01-03 18:00:00+00:00   2014-01-03 19:00:00+00:00\n",
       "                                       ...           \n",
       "2016-12-30 19:00:00+00:00   2016-12-30 20:00:00+00:00\n",
       "2016-12-30 20:00:00+00:00   2016-12-30 21:00:00+00:00\n",
       "2016-12-30 21:00:00+00:00   2016-12-30 22:00:00+00:00\n",
       "2016-12-30 22:00:00+00:00   2016-12-30 23:00:00+00:00\n",
       "2016-12-30 23:00:00+00:00   2016-12-31 00:00:00+00:00\n",
       "Name: nam_target_time, Length: 11560, dtype: datetime64[ns, UTC]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d105df27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Build reference on this single time key\n",
    "ref = pd.DataFrame(index=pd.Index(target_time, name=\"target_time\"))\n",
    "ref = ref[~ref.index.duplicated(keep=\"first\")].sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "71bddce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-01-03 15:00:00+00:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-03 16:00:00+00:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-03 17:00:00+00:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-03 18:00:00+00:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-03 19:00:00+00:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-30 20:00:00+00:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-30 21:00:00+00:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-30 22:00:00+00:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-30 23:00:00+00:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-31 00:00:00+00:00</th>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11560 rows × 0 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: [2014-01-03 15:00:00+00:00, 2014-01-03 16:00:00+00:00, 2014-01-03 17:00:00+00:00, 2014-01-03 18:00:00+00:00, 2014-01-03 19:00:00+00:00, 2014-01-03 20:00:00+00:00, 2014-01-03 21:00:00+00:00, 2014-01-03 22:00:00+00:00, 2014-01-03 23:00:00+00:00, 2014-01-04 00:00:00+00:00, 2014-01-04 03:00:00+00:00, 2014-01-04 15:00:00+00:00, 2014-01-04 16:00:00+00:00, 2014-01-04 17:00:00+00:00, 2014-01-04 18:00:00+00:00, 2014-01-04 19:00:00+00:00, 2014-01-04 20:00:00+00:00, 2014-01-04 21:00:00+00:00, 2014-01-04 22:00:00+00:00, 2014-01-04 23:00:00+00:00, 2014-01-05 00:00:00+00:00, 2014-01-05 03:00:00+00:00, 2014-01-05 15:00:00+00:00, 2014-01-05 16:00:00+00:00, 2014-01-05 17:00:00+00:00, 2014-01-05 18:00:00+00:00, 2014-01-05 19:00:00+00:00, 2014-01-05 20:00:00+00:00, 2014-01-05 21:00:00+00:00, 2014-01-05 22:00:00+00:00, 2014-01-05 23:00:00+00:00, 2014-01-06 00:00:00+00:00, 2014-01-06 03:00:00+00:00, 2014-01-06 15:00:00+00:00, 2014-01-06 16:00:00+00:00, 2014-01-06 17:00:00+00:00, 2014-01-06 18:00:00+00:00, 2014-01-06 19:00:00+00:00, 2014-01-06 20:00:00+00:00, 2014-01-06 21:00:00+00:00, 2014-01-06 22:00:00+00:00, 2014-01-06 23:00:00+00:00, 2014-01-07 00:00:00+00:00, 2014-01-07 03:00:00+00:00, 2014-01-07 15:00:00+00:00, 2014-01-07 16:00:00+00:00, 2014-01-07 17:00:00+00:00, 2014-01-07 18:00:00+00:00, 2014-01-07 19:00:00+00:00, 2014-01-07 20:00:00+00:00, 2014-01-07 21:00:00+00:00, 2014-01-07 22:00:00+00:00, 2014-01-07 23:00:00+00:00, 2014-01-08 00:00:00+00:00, 2014-01-08 03:00:00+00:00, 2014-01-08 15:00:00+00:00, 2014-01-08 16:00:00+00:00, 2014-01-08 17:00:00+00:00, 2014-01-08 18:00:00+00:00, 2014-01-08 19:00:00+00:00, 2014-01-08 20:00:00+00:00, 2014-01-08 21:00:00+00:00, 2014-01-08 22:00:00+00:00, 2014-01-08 23:00:00+00:00, 2014-01-09 00:00:00+00:00, 2014-01-09 03:00:00+00:00, 2014-01-09 15:00:00+00:00, 2014-01-09 16:00:00+00:00, 2014-01-09 17:00:00+00:00, 2014-01-09 18:00:00+00:00, 2014-01-09 19:00:00+00:00, 2014-01-09 20:00:00+00:00, 2014-01-09 21:00:00+00:00, 2014-01-09 22:00:00+00:00, 2014-01-09 23:00:00+00:00, 2014-01-10 00:00:00+00:00, 2014-01-10 03:00:00+00:00, 2014-01-10 15:00:00+00:00, 2014-01-10 16:00:00+00:00, 2014-01-10 17:00:00+00:00, 2014-01-10 18:00:00+00:00, 2014-01-10 19:00:00+00:00, 2014-01-10 20:00:00+00:00, 2014-01-10 21:00:00+00:00, 2014-01-10 22:00:00+00:00, 2014-01-10 23:00:00+00:00, 2014-01-11 00:00:00+00:00, 2014-01-11 03:00:00+00:00, 2014-01-11 15:00:00+00:00, 2014-01-11 16:00:00+00:00, 2014-01-11 17:00:00+00:00, 2014-01-11 18:00:00+00:00, 2014-01-11 19:00:00+00:00, 2014-01-11 20:00:00+00:00, 2014-01-11 21:00:00+00:00, 2014-01-11 22:00:00+00:00, 2014-01-11 23:00:00+00:00, 2014-01-12 00:00:00+00:00, 2014-01-12 03:00:00+00:00, 2014-01-12 15:00:00+00:00, ...]\n",
       "\n",
       "[11560 rows x 0 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "837d19e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Attach columns that already exist\n",
    "def _attach(colname, outname=None):\n",
    "    if colname in base_df.columns:\n",
    "        s = pd.Series(base_df[colname].values, index=ref.index)\n",
    "        ref[outname or colname] = s\n",
    "\n",
    "_attach(meas_ghi_col,  \"actual_ghi\")\n",
    "_attach(nam_ghi_col,   \"nam_ghi\")\n",
    "_attach(cs_ghi_col,    \"clear_sky_ghi\")\n",
    "_attach(actual_csi_col, \"actual_csi\")  # optional if you already computed CSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "819a127f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual_ghi</th>\n",
       "      <th>nam_ghi</th>\n",
       "      <th>clear_sky_ghi</th>\n",
       "      <th>actual_csi</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-01-03 15:00:00+00:00</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-03 16:00:00+00:00</th>\n",
       "      <td>12.914500</td>\n",
       "      <td>99.625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-03 17:00:00+00:00</th>\n",
       "      <td>143.775500</td>\n",
       "      <td>274.750</td>\n",
       "      <td>35.159500</td>\n",
       "      <td>1.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-03 18:00:00+00:00</th>\n",
       "      <td>289.955000</td>\n",
       "      <td>416.375</td>\n",
       "      <td>184.554711</td>\n",
       "      <td>1.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-03 19:00:00+00:00</th>\n",
       "      <td>402.643333</td>\n",
       "      <td>510.875</td>\n",
       "      <td>321.992462</td>\n",
       "      <td>1.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-30 20:00:00+00:00</th>\n",
       "      <td>433.891667</td>\n",
       "      <td>481.125</td>\n",
       "      <td>412.514321</td>\n",
       "      <td>1.051822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-30 21:00:00+00:00</th>\n",
       "      <td>449.525000</td>\n",
       "      <td>453.875</td>\n",
       "      <td>448.963991</td>\n",
       "      <td>1.001250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-30 22:00:00+00:00</th>\n",
       "      <td>410.876667</td>\n",
       "      <td>370.250</td>\n",
       "      <td>427.167267</td>\n",
       "      <td>0.961864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-30 23:00:00+00:00</th>\n",
       "      <td>313.066667</td>\n",
       "      <td>236.500</td>\n",
       "      <td>349.110038</td>\n",
       "      <td>0.896756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-31 00:00:00+00:00</th>\n",
       "      <td>179.243333</td>\n",
       "      <td>78.625</td>\n",
       "      <td>222.901420</td>\n",
       "      <td>0.804137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11560 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           actual_ghi  nam_ghi  clear_sky_ghi  actual_csi\n",
       "target_time                                                              \n",
       "2014-01-03 15:00:00+00:00    0.000000    0.000       0.000000    0.000000\n",
       "2014-01-03 16:00:00+00:00   12.914500   99.625       0.000000    1.200000\n",
       "2014-01-03 17:00:00+00:00  143.775500  274.750      35.159500    1.200000\n",
       "2014-01-03 18:00:00+00:00  289.955000  416.375     184.554711    1.200000\n",
       "2014-01-03 19:00:00+00:00  402.643333  510.875     321.992462    1.200000\n",
       "...                               ...      ...            ...         ...\n",
       "2016-12-30 20:00:00+00:00  433.891667  481.125     412.514321    1.051822\n",
       "2016-12-30 21:00:00+00:00  449.525000  453.875     448.963991    1.001250\n",
       "2016-12-30 22:00:00+00:00  410.876667  370.250     427.167267    0.961864\n",
       "2016-12-30 23:00:00+00:00  313.066667  236.500     349.110038    0.896756\n",
       "2016-12-31 00:00:00+00:00  179.243333   78.625     222.901420    0.804137\n",
       "\n",
       "[11560 rows x 4 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "de0d407c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3857071/1759580929.py:4: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  ref[\"nam_csi\"].replace([np.inf, -np.inf], np.nan, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "if \"nam_ghi\" in ref.columns and \"clear_sky_ghi\" in ref.columns:\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        ref[\"nam_csi\"] = ref[\"nam_ghi\"] / ref[\"clear_sky_ghi\"]\n",
    "        ref[\"nam_csi\"].replace([np.inf, -np.inf], np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a71723ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual_ghi</th>\n",
       "      <th>nam_ghi</th>\n",
       "      <th>clear_sky_ghi</th>\n",
       "      <th>actual_csi</th>\n",
       "      <th>nam_csi</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-01-03 15:00:00+00:00</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-03 16:00:00+00:00</th>\n",
       "      <td>12.914500</td>\n",
       "      <td>99.625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-03 17:00:00+00:00</th>\n",
       "      <td>143.775500</td>\n",
       "      <td>274.750</td>\n",
       "      <td>35.159500</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>7.814389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-03 18:00:00+00:00</th>\n",
       "      <td>289.955000</td>\n",
       "      <td>416.375</td>\n",
       "      <td>184.554711</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>2.256106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-03 19:00:00+00:00</th>\n",
       "      <td>402.643333</td>\n",
       "      <td>510.875</td>\n",
       "      <td>321.992462</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.586605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-30 20:00:00+00:00</th>\n",
       "      <td>433.891667</td>\n",
       "      <td>481.125</td>\n",
       "      <td>412.514321</td>\n",
       "      <td>1.051822</td>\n",
       "      <td>1.166323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-30 21:00:00+00:00</th>\n",
       "      <td>449.525000</td>\n",
       "      <td>453.875</td>\n",
       "      <td>448.963991</td>\n",
       "      <td>1.001250</td>\n",
       "      <td>1.010939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-30 22:00:00+00:00</th>\n",
       "      <td>410.876667</td>\n",
       "      <td>370.250</td>\n",
       "      <td>427.167267</td>\n",
       "      <td>0.961864</td>\n",
       "      <td>0.866756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-30 23:00:00+00:00</th>\n",
       "      <td>313.066667</td>\n",
       "      <td>236.500</td>\n",
       "      <td>349.110038</td>\n",
       "      <td>0.896756</td>\n",
       "      <td>0.677437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-31 00:00:00+00:00</th>\n",
       "      <td>179.243333</td>\n",
       "      <td>78.625</td>\n",
       "      <td>222.901420</td>\n",
       "      <td>0.804137</td>\n",
       "      <td>0.352734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11560 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           actual_ghi  nam_ghi  clear_sky_ghi  actual_csi  \\\n",
       "target_time                                                                 \n",
       "2014-01-03 15:00:00+00:00    0.000000    0.000       0.000000    0.000000   \n",
       "2014-01-03 16:00:00+00:00   12.914500   99.625       0.000000    1.200000   \n",
       "2014-01-03 17:00:00+00:00  143.775500  274.750      35.159500    1.200000   \n",
       "2014-01-03 18:00:00+00:00  289.955000  416.375     184.554711    1.200000   \n",
       "2014-01-03 19:00:00+00:00  402.643333  510.875     321.992462    1.200000   \n",
       "...                               ...      ...            ...         ...   \n",
       "2016-12-30 20:00:00+00:00  433.891667  481.125     412.514321    1.051822   \n",
       "2016-12-30 21:00:00+00:00  449.525000  453.875     448.963991    1.001250   \n",
       "2016-12-30 22:00:00+00:00  410.876667  370.250     427.167267    0.961864   \n",
       "2016-12-30 23:00:00+00:00  313.066667  236.500     349.110038    0.896756   \n",
       "2016-12-31 00:00:00+00:00  179.243333   78.625     222.901420    0.804137   \n",
       "\n",
       "                            nam_csi  \n",
       "target_time                          \n",
       "2014-01-03 15:00:00+00:00       NaN  \n",
       "2014-01-03 16:00:00+00:00       NaN  \n",
       "2014-01-03 17:00:00+00:00  7.814389  \n",
       "2014-01-03 18:00:00+00:00  2.256106  \n",
       "2014-01-03 19:00:00+00:00  1.586605  \n",
       "...                             ...  \n",
       "2016-12-30 20:00:00+00:00  1.166323  \n",
       "2016-12-30 21:00:00+00:00  1.010939  \n",
       "2016-12-30 22:00:00+00:00  0.866756  \n",
       "2016-12-30 23:00:00+00:00  0.677437  \n",
       "2016-12-31 00:00:00+00:00  0.352734  \n",
       "\n",
       "[11560 rows x 5 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "005f02dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3bcf3acf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual_ghi</th>\n",
       "      <th>nam_ghi</th>\n",
       "      <th>clear_sky_ghi</th>\n",
       "      <th>actual_csi</th>\n",
       "      <th>nam_csi</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-01-03 15:00:00+00:00</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-03 16:00:00+00:00</th>\n",
       "      <td>12.914500</td>\n",
       "      <td>99.625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-03 17:00:00+00:00</th>\n",
       "      <td>143.775500</td>\n",
       "      <td>274.750</td>\n",
       "      <td>35.159500</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>7.814389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-03 18:00:00+00:00</th>\n",
       "      <td>289.955000</td>\n",
       "      <td>416.375</td>\n",
       "      <td>184.554711</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>2.256106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-03 19:00:00+00:00</th>\n",
       "      <td>402.643333</td>\n",
       "      <td>510.875</td>\n",
       "      <td>321.992462</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.586605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-30 20:00:00+00:00</th>\n",
       "      <td>433.891667</td>\n",
       "      <td>481.125</td>\n",
       "      <td>412.514321</td>\n",
       "      <td>1.051822</td>\n",
       "      <td>1.166323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-30 21:00:00+00:00</th>\n",
       "      <td>449.525000</td>\n",
       "      <td>453.875</td>\n",
       "      <td>448.963991</td>\n",
       "      <td>1.001250</td>\n",
       "      <td>1.010939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-30 22:00:00+00:00</th>\n",
       "      <td>410.876667</td>\n",
       "      <td>370.250</td>\n",
       "      <td>427.167267</td>\n",
       "      <td>0.961864</td>\n",
       "      <td>0.866756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-30 23:00:00+00:00</th>\n",
       "      <td>313.066667</td>\n",
       "      <td>236.500</td>\n",
       "      <td>349.110038</td>\n",
       "      <td>0.896756</td>\n",
       "      <td>0.677437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-31 00:00:00+00:00</th>\n",
       "      <td>179.243333</td>\n",
       "      <td>78.625</td>\n",
       "      <td>222.901420</td>\n",
       "      <td>0.804137</td>\n",
       "      <td>0.352734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11560 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           actual_ghi  nam_ghi  clear_sky_ghi  actual_csi  \\\n",
       "target_time                                                                 \n",
       "2014-01-03 15:00:00+00:00    0.000000    0.000       0.000000    0.000000   \n",
       "2014-01-03 16:00:00+00:00   12.914500   99.625       0.000000    1.200000   \n",
       "2014-01-03 17:00:00+00:00  143.775500  274.750      35.159500    1.200000   \n",
       "2014-01-03 18:00:00+00:00  289.955000  416.375     184.554711    1.200000   \n",
       "2014-01-03 19:00:00+00:00  402.643333  510.875     321.992462    1.200000   \n",
       "...                               ...      ...            ...         ...   \n",
       "2016-12-30 20:00:00+00:00  433.891667  481.125     412.514321    1.051822   \n",
       "2016-12-30 21:00:00+00:00  449.525000  453.875     448.963991    1.001250   \n",
       "2016-12-30 22:00:00+00:00  410.876667  370.250     427.167267    0.961864   \n",
       "2016-12-30 23:00:00+00:00  313.066667  236.500     349.110038    0.896756   \n",
       "2016-12-31 00:00:00+00:00  179.243333   78.625     222.901420    0.804137   \n",
       "\n",
       "                            nam_csi  \n",
       "target_time                          \n",
       "2014-01-03 15:00:00+00:00  0.000000  \n",
       "2014-01-03 16:00:00+00:00  0.000000  \n",
       "2014-01-03 17:00:00+00:00  7.814389  \n",
       "2014-01-03 18:00:00+00:00  2.256106  \n",
       "2014-01-03 19:00:00+00:00  1.586605  \n",
       "...                             ...  \n",
       "2016-12-30 20:00:00+00:00  1.166323  \n",
       "2016-12-30 21:00:00+00:00  1.010939  \n",
       "2016-12-30 22:00:00+00:00  0.866756  \n",
       "2016-12-30 23:00:00+00:00  0.677437  \n",
       "2016-12-31 00:00:00+00:00  0.352734  \n",
       "\n",
       "[11560 rows x 5 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5c23c5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muhammadhassan/App_v02/src/evaluation_utils.py:132: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  ref[\"nam_csi\"].replace([np.inf, -np.inf], np.nan, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Build refrence from Original Receipe which has the whole features.\n",
    "from src.evaluation_utils import build_reference_from_existing\n",
    "pipeline.reference_df = build_reference_from_existing(\n",
    "                    base_df,                         \n",
    "                    time_col=\"measurement_time\",\n",
    "                    nam_time_col=\"nam_target_time\",\n",
    "                    meas_ghi_col=\"ghi\",\n",
    "                    nam_ghi_col=\"nam_ghi\",\n",
    "                    cs_ghi_col=\"GHI_cs\",\n",
    "                    actual_csi_col=\"CSI_ghi\",\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0127fb38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual_ghi</th>\n",
       "      <th>nam_ghi</th>\n",
       "      <th>clear_sky_ghi</th>\n",
       "      <th>actual_csi</th>\n",
       "      <th>nam_csi</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-01-03 15:00:00+00:00</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-03 16:00:00+00:00</th>\n",
       "      <td>12.914500</td>\n",
       "      <td>99.625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-03 17:00:00+00:00</th>\n",
       "      <td>143.775500</td>\n",
       "      <td>274.750</td>\n",
       "      <td>35.159500</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>7.814389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-03 18:00:00+00:00</th>\n",
       "      <td>289.955000</td>\n",
       "      <td>416.375</td>\n",
       "      <td>184.554711</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>2.256106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-03 19:00:00+00:00</th>\n",
       "      <td>402.643333</td>\n",
       "      <td>510.875</td>\n",
       "      <td>321.992462</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.586605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-30 20:00:00+00:00</th>\n",
       "      <td>433.891667</td>\n",
       "      <td>481.125</td>\n",
       "      <td>412.514321</td>\n",
       "      <td>1.051822</td>\n",
       "      <td>1.166323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-30 21:00:00+00:00</th>\n",
       "      <td>449.525000</td>\n",
       "      <td>453.875</td>\n",
       "      <td>448.963991</td>\n",
       "      <td>1.001250</td>\n",
       "      <td>1.010939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-30 22:00:00+00:00</th>\n",
       "      <td>410.876667</td>\n",
       "      <td>370.250</td>\n",
       "      <td>427.167267</td>\n",
       "      <td>0.961864</td>\n",
       "      <td>0.866756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-30 23:00:00+00:00</th>\n",
       "      <td>313.066667</td>\n",
       "      <td>236.500</td>\n",
       "      <td>349.110038</td>\n",
       "      <td>0.896756</td>\n",
       "      <td>0.677437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-31 00:00:00+00:00</th>\n",
       "      <td>179.243333</td>\n",
       "      <td>78.625</td>\n",
       "      <td>222.901420</td>\n",
       "      <td>0.804137</td>\n",
       "      <td>0.352734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11560 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           actual_ghi  nam_ghi  clear_sky_ghi  actual_csi  \\\n",
       "target_time                                                                 \n",
       "2014-01-03 15:00:00+00:00    0.000000    0.000       0.000000    0.000000   \n",
       "2014-01-03 16:00:00+00:00   12.914500   99.625       0.000000    1.200000   \n",
       "2014-01-03 17:00:00+00:00  143.775500  274.750      35.159500    1.200000   \n",
       "2014-01-03 18:00:00+00:00  289.955000  416.375     184.554711    1.200000   \n",
       "2014-01-03 19:00:00+00:00  402.643333  510.875     321.992462    1.200000   \n",
       "...                               ...      ...            ...         ...   \n",
       "2016-12-30 20:00:00+00:00  433.891667  481.125     412.514321    1.051822   \n",
       "2016-12-30 21:00:00+00:00  449.525000  453.875     448.963991    1.001250   \n",
       "2016-12-30 22:00:00+00:00  410.876667  370.250     427.167267    0.961864   \n",
       "2016-12-30 23:00:00+00:00  313.066667  236.500     349.110038    0.896756   \n",
       "2016-12-31 00:00:00+00:00  179.243333   78.625     222.901420    0.804137   \n",
       "\n",
       "                            nam_csi  \n",
       "target_time                          \n",
       "2014-01-03 15:00:00+00:00  0.000000  \n",
       "2014-01-03 16:00:00+00:00  0.000000  \n",
       "2014-01-03 17:00:00+00:00  7.814389  \n",
       "2014-01-03 18:00:00+00:00  2.256106  \n",
       "2014-01-03 19:00:00+00:00  1.586605  \n",
       "...                             ...  \n",
       "2016-12-30 20:00:00+00:00  1.166323  \n",
       "2016-12-30 21:00:00+00:00  1.010939  \n",
       "2016-12-30 22:00:00+00:00  0.866756  \n",
       "2016-12-30 23:00:00+00:00  0.677437  \n",
       "2016-12-31 00:00:00+00:00  0.352734  \n",
       "\n",
       "[11560 rows x 5 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.reference_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5438b5",
   "metadata": {},
   "source": [
    "### Step 06: Splits loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5571fca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'exp-004/exp-004rolling_origin_splits.json'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.config.get('splits_file','rolling_origin_splits.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3f3e5c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.utils:Loaded 35 folds from exp-004/exp-004rolling_origin_splits.json\n"
     ]
    }
   ],
   "source": [
    "# Load splits\n",
    "splits_data = pipeline.data_manager.load_rolling_splits(\n",
    "            pipeline.config.get('splits_file', 'rolling_origin_splits.json')\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fe8ec3",
   "metadata": {},
   "source": [
    "### Step 07: Running Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7b418dd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run each fold\n",
    "fold_results = []\n",
    "max_folds = pipeline.config.get('max_folds', len(splits_data['folds']))\n",
    "max_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6c9100ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "        17, 18, 19, 20]),\n",
       " array([21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37,\n",
       "        38]))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_fold_data = splits_data['folds'][0]\n",
    "fold_id = first_fold_data['fold_id']\n",
    "train_idx, val_idx = pipeline.data_manager.get_fold_indices(\n",
    "            X, labels, first_fold_data\n",
    "        )\n",
    "\n",
    "train_idx, val_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f2dbc1d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2014-01-10 00:00:00+00:00', '2014-01-11 00:00:00+00:00',\n",
       "               '2014-01-12 00:00:00+00:00', '2014-01-13 00:00:00+00:00',\n",
       "               '2014-01-14 00:00:00+00:00', '2014-01-15 00:00:00+00:00',\n",
       "               '2014-01-16 00:00:00+00:00', '2014-01-17 00:00:00+00:00',\n",
       "               '2014-01-18 00:00:00+00:00', '2014-01-19 00:00:00+00:00',\n",
       "               ...\n",
       "               '2016-12-21 00:00:00+00:00', '2016-12-22 00:00:00+00:00',\n",
       "               '2016-12-23 00:00:00+00:00', '2016-12-24 00:00:00+00:00',\n",
       "               '2016-12-25 00:00:00+00:00', '2016-12-26 00:00:00+00:00',\n",
       "               '2016-12-27 00:00:00+00:00', '2016-12-28 00:00:00+00:00',\n",
       "               '2016-12-29 00:00:00+00:00', '2016-12-30 00:00:00+00:00'],\n",
       "              dtype='datetime64[ns, UTC]', length=1055, freq=None)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_index = labels.index\n",
    "labels_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96288fb7",
   "metadata": {},
   "source": [
    "### Step 08: Run Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "06421d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X\n",
    "Y = Y\n",
    "\n",
    "train_idx=train_idx\n",
    "val_idx=val_idx\n",
    "\n",
    "fold_id=fold_id\n",
    "labels_index=labels_index\n",
    "\n",
    "reference_df=pipeline.reference_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0e6c213f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((21, 7, 11, 3), (21, 1, 11))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split data\n",
    "X_train, Y_train = X[train_idx], Y[train_idx]\n",
    "X_val, Y_val = X[val_idx], Y[val_idx]\n",
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "418b3dba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0ac6431c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[100.27604635,  24.49166667,   2.        ],\n",
       "       [ 89.72416162,  25.49166667,   2.        ],\n",
       "       [ 80.14672391,  26.49166667,   2.        ],\n",
       "       [ 71.98881356,  27.49166667,   2.        ],\n",
       "       [ 65.81615819,  28.49166667,   2.        ],\n",
       "       [ 62.24091114,  29.49166667,   2.        ],\n",
       "       [ 61.72203334,  30.49166667,   2.        ],\n",
       "       [ 64.33399075,  31.49166667,   2.        ],\n",
       "       [ 69.72279648,  32.49166667,   2.        ],\n",
       "       [ 77.29901123,  33.49166667,   2.        ],\n",
       "       [         nan,          nan,          nan]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f66f636b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Feature scaling (fit on TRAIN only) ---\n",
    "from src.utils import (\n",
    "    fit_feature_scaler, transform_X_with_scaler,\n",
    "    fit_target_scaler, transform_Y_with_scaler\n",
    ")\n",
    "feature_scaler = fit_feature_scaler(X_train)\n",
    "X_train = transform_X_with_scaler(X_train, feature_scaler)\n",
    "X_val   = transform_X_with_scaler(X_val,   feature_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e7458d20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.5943464 , -2.06044081,  0.        ],\n",
       "       [ 0.90764687, -2.05390393,  0.        ],\n",
       "       [ 0.28436278, -2.04736704,  0.        ],\n",
       "       [-0.24654078, -2.04083016,  0.        ],\n",
       "       [-0.64824717, -2.03429328,  0.        ],\n",
       "       [-0.88091844, -2.0277564 ,  0.        ],\n",
       "       [-0.91468616, -2.02121951,  0.        ],\n",
       "       [-0.74470422, -2.01468263,  0.        ],\n",
       "       [-0.39400949, -2.00814575,  0.        ],\n",
       "       [ 0.09903826, -2.00160886,  0.        ],\n",
       "       [        nan,         nan,         nan]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7711c84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.scaler_info = {\n",
    "            \"feature_scaler\": feature_scaler,\n",
    "            \"target_scaler\": False,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "737c01df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_ds = TensorDataset(\n",
    "            torch.from_numpy(X_train).float(),\n",
    "            torch.from_numpy(Y_train).float()\n",
    "        )\n",
    "val_ds = TensorDataset(\n",
    "            torch.from_numpy(X_val).float(),\n",
    "            torch.from_numpy(Y_val).float()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c6a6d24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ds, batch_size=pipeline.config[\"batch_size\"], shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=pipeline.config[\"batch_size\"], shuffle=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d08ed2",
   "metadata": {},
   "source": [
    "### Step 09: Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "795bdfd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b583bc8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = X.shape[-1]  # Number of features\n",
    "horizon_days = Y.shape[1]\n",
    "\n",
    "input_size, horizon_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7ba856ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f7860115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LSTM'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.config['model_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0053dbd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hidden_size': 64,\n",
       " 'num_layers': 2,\n",
       " 'dropout': 0.35,\n",
       " 'bidirectional': True,\n",
       " 'use_attention': False}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config = pipeline.config.get('model_config', {})\n",
    "model_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7681dfe",
   "metadata": {},
   "source": [
    "### Step 10: LSTM Model\n",
    "\n",
    "What do i expect as input to the model\n",
    "Input shape: (B, D, S, F)\n",
    "\t•\tB: batch size\n",
    "\t•\tD: history_days (number of past days)\n",
    "\t•\tS: steps_per_day (e.g., 24 hourly or 48 half-hourly steps), Previous KBins\n",
    "\t•\tF: features per step (e.g., CSI, NAM lag, hour encodings, etc.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ab3901be",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size=input_size # number of features per time step\n",
    "steps_per_day =   X_train.shape[2]   # fixed steps per day on your grid (e.g., 24 or 48)\n",
    "hidden_size=model_config.get('hidden_size')\n",
    "num_layers=model_config.get('num_layers')\n",
    "dropout=model_config.get('dropout')\n",
    "horizon_days=horizon_days\n",
    "bidirectional=model_config.get('bidirectional')\n",
    "attn_heads = 8\n",
    "use_attention = model_config.get(\"use_attention\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a4013998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "68243f93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We flatten (steps_per_day, features) → a single feature vector per day-step\n",
    "flat_in = steps_per_day * input_size\n",
    "flat_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a3782158",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "pipeline.input_projection = nn.Linear(flat_in, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f9df046c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.lstm = nn.LSTM(\n",
    "            input_size=hidden_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2bcd121d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention (created only if configured)\n",
    "if use_attention:\n",
    "    pipeline.attention = nn.MultiheadAttention(\n",
    "    embed_dim=hidden_size * pipeline.num_directions,\n",
    "    num_heads=attn_heads,\n",
    "    dropout=dropout,\n",
    "    batch_first=True,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "18661398",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.dropout = nn.Dropout(dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cb821516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final output projection\n",
    "pipeline.num_directions = 2 if bidirectional else 1\n",
    "pipeline.horizon_days = horizon_days\n",
    "pipeline.steps_per_day = steps_per_day\n",
    "pipeline.output_projection = nn.Sequential(\n",
    "            nn.Linear(hidden_size * pipeline.num_directions, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, pipeline.horizon_days * pipeline.steps_per_day),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "43d72cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x):\n",
    "    \"\"\"\n",
    "        x: (B, D, S, F)\n",
    "    \"\"\"\n",
    "    B, D, S, F = x.shape\n",
    "\n",
    "    # Flatten per-day sequence\n",
    "    x = x.reshape(B, D, S * F)\n",
    "\n",
    "    # Project to LSTM input width\n",
    "    x = self.input_projection(x)\n",
    "\n",
    "    # Encode temporal history\n",
    "    lstm_out, _ = self.lstm(x)  # (B, D, H*num_dir)\n",
    "\n",
    "    # Optionally apply attention over D\n",
    "    if self.use_attention:\n",
    "        lstm_out, _ = self.attention(lstm_out, lstm_out, lstm_out)\n",
    "\n",
    "    # Use the final timestep representation\n",
    "    last_hidden = lstm_out[:, -1, :]\n",
    "\n",
    "    # Predict future day(s)\n",
    "    y = self.output_projection(self.dropout(last_hidden))  # (B, H*S)\n",
    "    y = y.view(B, self.horizon_days, self.steps_per_day)   # (B, H, S)\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "55dff144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'experiment_name': 'TEST_UniLSTM_test_data',\n",
       " 'model_type': 'LSTM',\n",
       " 'model_config': {'hidden_size': 64,\n",
       "  'num_layers': 2,\n",
       "  'dropout': 0.35,\n",
       "  'bidirectional': True,\n",
       "  'use_attention': False},\n",
       " 'data_prefix': 'test_data',\n",
       " 'splits_file': 'exp-004/exp-004rolling_origin_splits.json',\n",
       " 'feature_cols': ['solar_zenith', 'absolute_hour', 'season_flag'],\n",
       " 'feature_selection': ['solar_zenith', 'absolute_hour', 'season_flag'],\n",
       " 'target_col': 'CSI_ghi',\n",
       " 'batch_size': 32,\n",
       " 'num_epochs': 50,\n",
       " 'learning_rate': 0.001,\n",
       " 'loss_function': 'Huber',\n",
       " 'early_stopping_patience': 20,\n",
       " 'max_folds': 3,\n",
       " 'scale_target': False}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "af7e981d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.config[\"model_config\"][\"num_layers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "80649343",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import ImprovedLSTM\n",
    "model = ImprovedLSTM(\n",
    "    input_size=len(feature_cols),\n",
    "    # steps_per_day=pipeline.config[\"model_config\"][\"steps_per_day\"],\n",
    "    steps_per_day=11,\n",
    "    # horizon_days=pipeline.config[\"horizon_days\"],\n",
    "    horizon_days=3,\n",
    "    use_attention=pipeline.config.get(\"use_attention\", False)   # default off\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bcd264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "input_size = X.shape[-1]  # Number of features\n",
    "horizon_days = Y.shape[1]\n",
    "model = pipeline.create_model(\n",
    "    pipeline.config['model_type'],\n",
    "    input_size,\n",
    "    horizon_days\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "23ce0734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Huber'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_function_name = pipeline.config.get('loss_function', 'MSE')\n",
    "loss_function_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d832ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, fold_data in enumerate(splits_data['folds'][:max_folds]):\n",
    "    fold_id = fold_data['fold_id']\n",
    "            \n",
    "    # Get indices for this fold\n",
    "    if labels is not None:\n",
    "        train_idx, val_idx = pipeline.data_manager.get_fold_indices(\n",
    "            X, labels, fold_data\n",
    "        )\n",
    "    else:\n",
    "        # If no labels, use the sizes directly\n",
    "        train_size = fold_data['train_size']\n",
    "        val_size = fold_data['val_size']\n",
    "        train_idx = np.arange(train_size)\n",
    "        val_idx = np.arange(train_size, train_size + val_size)\n",
    "            \n",
    "    # Run fold\n",
    "    labels_index = labels.index\n",
    "    fold_result = pipeline.run_fold(\n",
    "                X,\n",
    "                Y,\n",
    "                train_idx,\n",
    "                val_idx,\n",
    "                fold_id,\n",
    "                labels_index=labels_index,\n",
    "                reference_df=pipeline.reference_df,\n",
    "            )\n",
    "    fold_results.append(fold_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb63bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will now work after you apply the fixes below\n",
    "pipeline = SolarForecastingPipeline(LSTM_CONFIG)\n",
    "_, summary = pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee43e09",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fac779e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocessing import build_model_arrays, to_fixedgrid_multiindex\n",
    "\n",
    "TARGET_COL = \"CSI_ghi\"\n",
    "timestamp_col=\"measurement_time\"\n",
    "\n",
    "TEST_df = df_interpolated[['solar_zenith', 'CSI_ghi', 'absolute_hour', 'season_flag' ]]\n",
    "\n",
    "\n",
    "feature_cols = [c for c in TEST_df.columns.tolist() if c != TARGET_COL]\n",
    "\n",
    "# ----------------- to_fixedgrid_multiindex -----------------\n",
    "# fixed_df = to_fixedgrid_multiindex(df_phase1, timestamp_col=\"measurement_time\", expected_T=None) \n",
    "\n",
    "test_df = df_phase1.copy(deep=True)\n",
    "\n",
    "test_df.reset_index(inplace=True)\n",
    "test_df[\"UTC_date\"] = test_df[timestamp_col].dt.tz_convert(\"UTC\").dt.date\n",
    "test_df[\"PST_date\"] = (\n",
    "    test_df[timestamp_col]\n",
    "    .dt.tz_convert(\"US/Pacific\")  # Converts UTC to Pacific Time (handles DST automatically)\n",
    "    .dt.date\n",
    ")\n",
    "test_df[\"bin_id\"] = test_df.groupby(\"PST_date\").cumcount()\n",
    "test_df = test_df.set_index([\"PST_date\", \"bin_id\"]).sort_index()\n",
    "\n",
    "# -------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "434725dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11560, 15)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5cf57b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================= build_model_arrays() ===================\n",
    "\n",
    "# ph1_X, ph1_Y, ph1_labels_list = build_model_arrays(\n",
    "#         test_df,\n",
    "#         feature_cols=feature_cols,  # <-- This is correct!\n",
    "#         target_col=TARGET_COL,\n",
    "#         history_days=2,\n",
    "#         horizon_days=1,\n",
    "#     )\n",
    "\n",
    "dates = list(test_df.index.get_level_values(\"PST_date\").unique())\n",
    "K = int(test_df.index.get_level_values(\"bin_id\").max()) + 1\n",
    "F = len(feature_cols)\n",
    "X = np.full((len(dates), K, F), np.nan, dtype=float)\n",
    "\n",
    "for j, col in enumerate(feature_cols):\n",
    "    if col not in test_df.columns:\n",
    "        continue\n",
    "\n",
    "    \n",
    "\n",
    "    x_val = test_df['solar_zenith'].groupby(level=[\"PST_date\", \"bin_id\"]).first().unstack(\"bin_id\").reindex(index=dates, columns=range(K))\n",
    "    X[:, :, j] = x_val.values \n",
    "    \n",
    "\n",
    "# Target Tensor Building\n",
    "y_col = ['CSI_ghi']\n",
    "\n",
    "dates = list(test_df.index.get_level_values(\"PST_date\").unique())\n",
    "K = int(test_df.index.get_level_values(\"bin_id\").max()) + 1\n",
    "F = len(y_col)\n",
    "Y = np.full((len(dates), K, F), np.nan, dtype=float)\n",
    "\n",
    "for j, col in enumerate(y_col):\n",
    "    if col not in test_df.columns:\n",
    "        continue\n",
    "\n",
    "    y_val = test_df['solar_zenith'].groupby(level=[\"PST_date\", \"bin_id\"]).first().unstack(\"bin_id\").reindex(index=dates, columns=range(K))\n",
    "    Y[:, :, j] = x_val.values \n",
    "Y = Y[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6470a9e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1051, (1051, 11, 3), (1051, 11))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dates), X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "71486748",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_days = 3\n",
    "horizon_days = 1\n",
    "num_days = X.shape[0]   # Avaible dates\n",
    "K = X.shape[1]          # Hours records\n",
    "F = X.shape[2]          # Avaible Features\n",
    "\n",
    "samples = max(0, num_days - history_days - horizon_days + 1)\n",
    "X_list, Y_list, labels = [], [], []\n",
    "for i in range(samples):\n",
    "    past = slice(i, i+history_days)\n",
    "    fut  = slice(i+history_days, i+history_days+horizon_days)\n",
    "    X_i = X[past,:,:]\n",
    "    Y_i = Y[fut,:]\n",
    "    X_list.append(X_i)\n",
    "    Y_list.append(Y_i)\n",
    "    labels.append(dates[i+history_days+horizon_days-1])\n",
    "\n",
    "\n",
    " # converts the list of individual samples into the final, single, massive tensor required by the deep learning model.\n",
    "X_tensor = np.stack(X_list, axis=0) if X_list else np.empty((0,history_days,K,F))\n",
    "Y_tensor = np.stack(Y_list, axis=0) if Y_list else np.empty((0,horizon_days,K))\n",
    "# X: A 4D tensor of shape (samples, history_days, K, F)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0015ac7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1048, 3, 11, 3),\n",
       " (1048, 1, 11),\n",
       " 1048,\n",
       " [datetime.date(2014, 2, 11),\n",
       "  datetime.date(2014, 2, 12),\n",
       "  datetime.date(2014, 2, 19),\n",
       "  datetime.date(2014, 2, 20),\n",
       "  datetime.date(2014, 2, 21),\n",
       "  datetime.date(2014, 2, 22)])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tensor.shape, Y_tensor.shape, len(labels), labels[30:36]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dfb6a100",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.utils:Saved arrays to data/Test_dataFile_prefix_*.npy\n",
      "INFO:src.utils:X shape: (1048, 3, 11, 3), Y shape: (1048, 1, 11)\n"
     ]
    }
   ],
   "source": [
    "from src.utils import DataManager\n",
    "\n",
    "data_manager = DataManager()\n",
    "data_manager.save_arrays(\n",
    "    X_tensor, Y_tensor,\n",
    "    pd.DataFrame(\n",
    "        index=pd.to_datetime(labels, utc=True)),\n",
    "    filename_prefix='Test_dataFile_prefix', \n",
    "    feature_cols=feature_cols,\n",
    "    target_col=TARGET_COL,\n",
    "    metadata={\n",
    "        \"input_csv\": \"data/processed/df_1h_lag_BLV_spatial_images.csv\",\n",
    "        \"timestamp_col\": \"measurement_time\",\n",
    "        \"feature_set\": feature_cols,\n",
    "        \"history_days\": 7,\n",
    "        \"horizon_days\": 1,\n",
    "        \"k_bins\": None,\n",
    "    }\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "faab0a12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['solar_zenith', 'absolute_hour', 'season_flag'],\n",
       " 'CSI_ghi',\n",
       " <function list.index(value, start=0, stop=9223372036854775807, /)>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_cols, TARGET_COL, labels.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b491657b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('measurement_time',\n",
       " 'data/processed/df_1h_lag_BLV_spatial_images.csv',\n",
       " SiteLocation(latitude=38.678, longitude=-121.176, altitude=220, timezone='America/Los_Angeles'))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Block 4: Configure and Run ---\n",
    "LSTM_CONFIG = {\n",
    "    \"experiment_name\": \"TESTEXP_UniLSTM_p1_exp01_essentialF\",\n",
    "    \"model_type\": \"LSTM\",\n",
    "    \"model_config\": {\n",
    "        \"hidden_size\": 64,\n",
    "        \"num_layers\": 2,\n",
    "        \"dropout\": 0.35,\n",
    "        \"bidirectional\": True,\n",
    "    },\n",
    "    \"data_prefix\": \"TEST_data_PREFIX\", \n",
    "    \"splits_file\": \"exp-004/exp-004rolling_origin_splits.json\",\n",
    "    \"feature_cols\": feature_cols,\n",
    "    \"feature_selection\": feature_cols, # <-- This is correct!\n",
    "    \"target_col\": TARGET_COL,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_epochs\": 50,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"loss_function\": \"Huber\",  # <-- This setting needs the fix below\n",
    "    \"early_stopping_patience\": 20,\n",
    "    \"max_folds\": 3,\n",
    "    \"site_latitude\": 38.678,\n",
    "    \"site_longitude\": -121.176,\n",
    "    \"site_altitude\": 220,\n",
    "    \"site_timezone\": \"America/Los_Angeles\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "}\n",
    "metadata={\n",
    "        \"input_csv\": \"data/processed/df_1h_lag_BLV_spatial_images.csv\",\n",
    "        \"timestamp_col\": \"measurement_time\",\n",
    "        \"feature_set\": feature_cols,\n",
    "        \"history_days\": 7,\n",
    "        \"horizon_days\": 1\n",
    "    }\n",
    "\n",
    "expected_features = LSTM_CONFIG.get(\"feature_cols\")\n",
    "\n",
    "from src.evaluation_utils import build_reference_fixedgrid_frame, SiteLocation\n",
    "\n",
    "ts_col = metadata.get(\"timestamp_col\", \"measurement_time\")\n",
    "csv_path = metadata.get(\"input_csv\")\n",
    "\n",
    "site_config = SiteLocation(\n",
    "                latitude=LSTM_CONFIG.get('site_latitude'),\n",
    "                longitude=LSTM_CONFIG.get('site_longitude'),\n",
    "                altitude=LSTM_CONFIG.get('site_altitude'),\n",
    "                timezone=LSTM_CONFIG.get('site_timezone'),\n",
    "            )\n",
    "ts_col, csv_path, site_config\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2b1f69ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test build_reference_fixedgrid_frame\n",
    "\n",
    "# STEP1 ------------ _load_processed_dataframe -------\n",
    "# Providing the following drop_columns=[\"nam_target_time\", \"forecast_issue_time\", 'issue_time',  'horizon']\n",
    "# 1. read csv provided\n",
    "\n",
    "drop_columns=['valtime','Unnamed: 0', 'timestamp','PST_Time_nam', 'PST_Time_meas', \"forecast_issue_time\", 'issue_time',  'horizon']\n",
    "_load_processed_dataframe_df = pd.read_csv(\n",
    "        csv_path,\n",
    "        parse_dates=[\"measurement_time\"], \n",
    "    )\n",
    "\n",
    "_load_processed_dataframe_df = _load_processed_dataframe_df.drop(columns=list(drop_columns), errors=\"ignore\")\n",
    "\n",
    "numeric_cols = [\n",
    "        'ghi', 'dni', 'solar_zenith',\n",
    "       'time_gap_hours', 'time_gap_norm', 'day_boundary_flag',\n",
    "       'hour_progression', 'absolute_hour', 'GHI_cs', 'DNI_cs', 'CSI_ghi',\n",
    "       'CSI_dni', 'season_flag', 'hour_sin', 'hour_cos', 'month_sin',\n",
    "       'month_cos', 'nam_ghi', 'nam_dni', 'nam_cc',\n",
    "       'B_CSI_ghi_8h', 'V_CSI_ghi_8h', 'L_CSI_ghi_8h', 'B_CSI_ghi_9h',\n",
    "       'V_CSI_ghi_9h', 'L_CSI_ghi_9h', 'B_CSI_ghi_10h', 'V_CSI_ghi_10h',\n",
    "       'L_CSI_ghi_10h', 'B_CSI_ghi_11h', 'V_CSI_ghi_11h', 'L_CSI_ghi_11h',\n",
    "       'B_CSI_ghi_12h', 'V_CSI_ghi_12h', 'L_CSI_ghi_12h', 'B_CSI_ghi_13h',\n",
    "       'V_CSI_ghi_13h', 'L_CSI_ghi_13h', 'B_CSI_ghi_14h', 'V_CSI_ghi_14h',\n",
    "       'L_CSI_ghi_14h', 'B_CSI_ghi_15h', 'V_CSI_ghi_15h', 'L_CSI_ghi_15h',\n",
    "       'B_CSI_ghi_16h', 'V_CSI_ghi_16h', 'L_CSI_ghi_16h', 'B_CSI_ghi_17h',\n",
    "       'V_CSI_ghi_17h', 'L_CSI_ghi_17h', 'B_CSI_ghi_18h', 'V_CSI_ghi_18h',\n",
    "       'L_CSI_ghi_18h', 'B_CSI_ghi_19h', 'V_CSI_ghi_19h', 'L_CSI_ghi_19h',\n",
    "       '80_dwsw', 'valtime', '80_cloud_cover', '56_dwsw', '56_cloud_cover',\n",
    "       '20_dwsw', '20_cloud_cover', '88_dwsw', '88_cloud_cover',\n",
    "       'AVG(R)', 'STD(R)', 'ENT(R)', 'AVG(G)',\n",
    "       'STD(G)', 'ENT(G)', 'AVG(B)', 'STD(B)', 'ENT(B)', 'AVG(RB)', 'STD(RB)',\n",
    "       'ENT(RB)', 'AVG(NRB)', 'STD(NRB)', 'ENT(NRB)'\n",
    "    ]\n",
    "for col in numeric_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "_load_processed_dataframe_df = _load_processed_dataframe_df.sort_values(\"measurement_time\")\n",
    "_load_processed_dataframe_df = _load_processed_dataframe_df.set_index(\"measurement_time\")\n",
    "\n",
    "\n",
    "tz: str = \"UTC\"\n",
    "if tz:\n",
    "    df.index = df.index.tz_convert(tz) if df.index.tz else df.index.tz_localize(tz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b674e1b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ghi', 'dni', 'solar_zenith', 'time_gap_hours', 'time_gap_norm',\n",
       "       'day_boundary_flag', 'hour_progression', 'absolute_hour', 'GHI_cs',\n",
       "       'DNI_cs', 'CSI_ghi', 'CSI_dni', 'season_flag', 'hour_sin', 'hour_cos',\n",
       "       'month_sin', 'month_cos', 'nam_ghi', 'nam_dni', 'nam_cc',\n",
       "       'nam_target_time', 'B_CSI_ghi_8h', 'V_CSI_ghi_8h', 'L_CSI_ghi_8h',\n",
       "       'B_CSI_ghi_9h', 'V_CSI_ghi_9h', 'L_CSI_ghi_9h', 'B_CSI_ghi_10h',\n",
       "       'V_CSI_ghi_10h', 'L_CSI_ghi_10h', 'B_CSI_ghi_11h', 'V_CSI_ghi_11h',\n",
       "       'L_CSI_ghi_11h', 'B_CSI_ghi_12h', 'V_CSI_ghi_12h', 'L_CSI_ghi_12h',\n",
       "       'B_CSI_ghi_13h', 'V_CSI_ghi_13h', 'L_CSI_ghi_13h', 'B_CSI_ghi_14h',\n",
       "       'V_CSI_ghi_14h', 'L_CSI_ghi_14h', 'B_CSI_ghi_15h', 'V_CSI_ghi_15h',\n",
       "       'L_CSI_ghi_15h', 'B_CSI_ghi_16h', 'V_CSI_ghi_16h', 'L_CSI_ghi_16h',\n",
       "       'B_CSI_ghi_17h', 'V_CSI_ghi_17h', 'L_CSI_ghi_17h', 'B_CSI_ghi_18h',\n",
       "       'V_CSI_ghi_18h', 'L_CSI_ghi_18h', 'B_CSI_ghi_19h', 'V_CSI_ghi_19h',\n",
       "       'L_CSI_ghi_19h', '80_dwsw', '80_cloud_cover', '56_dwsw',\n",
       "       '56_cloud_cover', '20_dwsw', '20_cloud_cover', '88_dwsw',\n",
       "       '88_cloud_cover', 'AVG(R)', 'STD(R)', 'ENT(R)', 'AVG(G)', 'STD(G)',\n",
       "       'ENT(G)', 'AVG(B)', 'STD(B)', 'ENT(B)', 'AVG(RB)', 'STD(RB)', 'ENT(RB)',\n",
       "       'AVG(NRB)', 'STD(NRB)', 'ENT(NRB)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_load_processed_dataframe_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "449eb298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ghi</th>\n",
       "      <th>dni</th>\n",
       "      <th>solar_zenith</th>\n",
       "      <th>time_gap_hours</th>\n",
       "      <th>time_gap_norm</th>\n",
       "      <th>day_boundary_flag</th>\n",
       "      <th>hour_progression</th>\n",
       "      <th>absolute_hour</th>\n",
       "      <th>GHI_cs</th>\n",
       "      <th>DNI_cs</th>\n",
       "      <th>...</th>\n",
       "      <th>ENT(G)</th>\n",
       "      <th>AVG(B)</th>\n",
       "      <th>STD(B)</th>\n",
       "      <th>ENT(B)</th>\n",
       "      <th>AVG(RB)</th>\n",
       "      <th>STD(RB)</th>\n",
       "      <th>ENT(RB)</th>\n",
       "      <th>AVG(NRB)</th>\n",
       "      <th>STD(NRB)</th>\n",
       "      <th>ENT(NRB)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>measurement_time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-01-03 14:00:00+00:00</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.276046</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.006944</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>6.491667</td>\n",
       "      <td>24.491667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-03 15:00:00+00:00</th>\n",
       "      <td>12.914500</td>\n",
       "      <td>89.245833</td>\n",
       "      <td>89.724162</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.491667</td>\n",
       "      <td>25.491667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.564803</td>\n",
       "      <td>151.246458</td>\n",
       "      <td>28.520968</td>\n",
       "      <td>5.360367</td>\n",
       "      <td>0.808373</td>\n",
       "      <td>0.144332</td>\n",
       "      <td>4.688985</td>\n",
       "      <td>-0.113270</td>\n",
       "      <td>0.086132</td>\n",
       "      <td>4.008980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-03 16:00:00+00:00</th>\n",
       "      <td>143.775500</td>\n",
       "      <td>636.475000</td>\n",
       "      <td>80.146724</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.491667</td>\n",
       "      <td>26.491667</td>\n",
       "      <td>35.159500</td>\n",
       "      <td>182.297840</td>\n",
       "      <td>...</td>\n",
       "      <td>5.510188</td>\n",
       "      <td>141.713212</td>\n",
       "      <td>33.481258</td>\n",
       "      <td>5.414777</td>\n",
       "      <td>0.909988</td>\n",
       "      <td>0.148683</td>\n",
       "      <td>4.742913</td>\n",
       "      <td>-0.053787</td>\n",
       "      <td>0.079303</td>\n",
       "      <td>3.913008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-03 17:00:00+00:00</th>\n",
       "      <td>289.955000</td>\n",
       "      <td>783.193333</td>\n",
       "      <td>71.988814</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.491667</td>\n",
       "      <td>27.491667</td>\n",
       "      <td>184.554711</td>\n",
       "      <td>540.120302</td>\n",
       "      <td>...</td>\n",
       "      <td>5.478535</td>\n",
       "      <td>137.213417</td>\n",
       "      <td>35.403827</td>\n",
       "      <td>5.377718</td>\n",
       "      <td>0.965367</td>\n",
       "      <td>0.142668</td>\n",
       "      <td>4.686905</td>\n",
       "      <td>-0.022647</td>\n",
       "      <td>0.071275</td>\n",
       "      <td>3.777387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-03 18:00:00+00:00</th>\n",
       "      <td>402.643333</td>\n",
       "      <td>843.050000</td>\n",
       "      <td>65.816158</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.491667</td>\n",
       "      <td>28.491667</td>\n",
       "      <td>321.992462</td>\n",
       "      <td>692.514065</td>\n",
       "      <td>...</td>\n",
       "      <td>5.456043</td>\n",
       "      <td>138.616210</td>\n",
       "      <td>36.162608</td>\n",
       "      <td>5.356960</td>\n",
       "      <td>0.947050</td>\n",
       "      <td>0.140950</td>\n",
       "      <td>4.682247</td>\n",
       "      <td>-0.032250</td>\n",
       "      <td>0.072482</td>\n",
       "      <td>3.788302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-30 19:00:00+00:00</th>\n",
       "      <td>433.891667</td>\n",
       "      <td>753.421667</td>\n",
       "      <td>62.502377</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.491667</td>\n",
       "      <td>26237.491667</td>\n",
       "      <td>412.514321</td>\n",
       "      <td>762.629492</td>\n",
       "      <td>...</td>\n",
       "      <td>5.545958</td>\n",
       "      <td>135.665343</td>\n",
       "      <td>42.458667</td>\n",
       "      <td>5.359012</td>\n",
       "      <td>0.849203</td>\n",
       "      <td>0.178163</td>\n",
       "      <td>4.844802</td>\n",
       "      <td>-0.092885</td>\n",
       "      <td>0.101738</td>\n",
       "      <td>4.111970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-30 20:00:00+00:00</th>\n",
       "      <td>449.525000</td>\n",
       "      <td>747.595000</td>\n",
       "      <td>62.079248</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.491667</td>\n",
       "      <td>26238.491667</td>\n",
       "      <td>448.963991</td>\n",
       "      <td>783.891589</td>\n",
       "      <td>...</td>\n",
       "      <td>5.617600</td>\n",
       "      <td>128.237255</td>\n",
       "      <td>44.904030</td>\n",
       "      <td>5.412868</td>\n",
       "      <td>0.902123</td>\n",
       "      <td>0.172672</td>\n",
       "      <td>4.814140</td>\n",
       "      <td>-0.059772</td>\n",
       "      <td>0.093093</td>\n",
       "      <td>4.012365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-30 21:00:00+00:00</th>\n",
       "      <td>410.876667</td>\n",
       "      <td>735.498333</td>\n",
       "      <td>64.765899</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.491667</td>\n",
       "      <td>26239.491667</td>\n",
       "      <td>427.167267</td>\n",
       "      <td>771.455229</td>\n",
       "      <td>...</td>\n",
       "      <td>5.643208</td>\n",
       "      <td>119.943472</td>\n",
       "      <td>42.973342</td>\n",
       "      <td>5.470670</td>\n",
       "      <td>0.880178</td>\n",
       "      <td>0.188148</td>\n",
       "      <td>4.893065</td>\n",
       "      <td>-0.074325</td>\n",
       "      <td>0.104203</td>\n",
       "      <td>4.123958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-30 22:00:00+00:00</th>\n",
       "      <td>313.066667</td>\n",
       "      <td>684.428333</td>\n",
       "      <td>70.203098</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.491667</td>\n",
       "      <td>26240.491667</td>\n",
       "      <td>349.110038</td>\n",
       "      <td>719.384366</td>\n",
       "      <td>...</td>\n",
       "      <td>5.685198</td>\n",
       "      <td>123.339068</td>\n",
       "      <td>38.577063</td>\n",
       "      <td>5.459773</td>\n",
       "      <td>0.789770</td>\n",
       "      <td>0.209435</td>\n",
       "      <td>4.991567</td>\n",
       "      <td>-0.132832</td>\n",
       "      <td>0.127398</td>\n",
       "      <td>4.354853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-30 23:00:00+00:00</th>\n",
       "      <td>179.243333</td>\n",
       "      <td>520.965000</td>\n",
       "      <td>77.805433</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.491667</td>\n",
       "      <td>26241.491667</td>\n",
       "      <td>222.901420</td>\n",
       "      <td>597.677942</td>\n",
       "      <td>...</td>\n",
       "      <td>5.668687</td>\n",
       "      <td>130.965260</td>\n",
       "      <td>33.780193</td>\n",
       "      <td>5.382033</td>\n",
       "      <td>0.750620</td>\n",
       "      <td>0.212100</td>\n",
       "      <td>4.998280</td>\n",
       "      <td>-0.158547</td>\n",
       "      <td>0.133200</td>\n",
       "      <td>4.417220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11560 rows × 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  ghi         dni  solar_zenith  \\\n",
       "measurement_time                                                  \n",
       "2014-01-03 14:00:00+00:00    0.000000    0.000000    100.276046   \n",
       "2014-01-03 15:00:00+00:00   12.914500   89.245833     89.724162   \n",
       "2014-01-03 16:00:00+00:00  143.775500  636.475000     80.146724   \n",
       "2014-01-03 17:00:00+00:00  289.955000  783.193333     71.988814   \n",
       "2014-01-03 18:00:00+00:00  402.643333  843.050000     65.816158   \n",
       "...                               ...         ...           ...   \n",
       "2016-12-30 19:00:00+00:00  433.891667  753.421667     62.502377   \n",
       "2016-12-30 20:00:00+00:00  449.525000  747.595000     62.079248   \n",
       "2016-12-30 21:00:00+00:00  410.876667  735.498333     64.765899   \n",
       "2016-12-30 22:00:00+00:00  313.066667  684.428333     70.203098   \n",
       "2016-12-30 23:00:00+00:00  179.243333  520.965000     77.805433   \n",
       "\n",
       "                           time_gap_hours  time_gap_norm  day_boundary_flag  \\\n",
       "measurement_time                                                              \n",
       "2014-01-03 14:00:00+00:00        0.166667       0.006944           0.016667   \n",
       "2014-01-03 15:00:00+00:00        0.016667       0.000694           0.000000   \n",
       "2014-01-03 16:00:00+00:00        0.016667       0.000694           0.000000   \n",
       "2014-01-03 17:00:00+00:00        0.016667       0.000694           0.000000   \n",
       "2014-01-03 18:00:00+00:00        0.016667       0.000694           0.000000   \n",
       "...                                   ...            ...                ...   \n",
       "2016-12-30 19:00:00+00:00        0.016667       0.000694           0.000000   \n",
       "2016-12-30 20:00:00+00:00        0.016667       0.000694           0.000000   \n",
       "2016-12-30 21:00:00+00:00        0.016667       0.000694           0.000000   \n",
       "2016-12-30 22:00:00+00:00        0.016667       0.000694           0.000000   \n",
       "2016-12-30 23:00:00+00:00        0.016667       0.000694           0.000000   \n",
       "\n",
       "                           hour_progression  absolute_hour      GHI_cs  \\\n",
       "measurement_time                                                         \n",
       "2014-01-03 14:00:00+00:00          6.491667      24.491667    0.000000   \n",
       "2014-01-03 15:00:00+00:00          7.491667      25.491667    0.000000   \n",
       "2014-01-03 16:00:00+00:00          8.491667      26.491667   35.159500   \n",
       "2014-01-03 17:00:00+00:00          9.491667      27.491667  184.554711   \n",
       "2014-01-03 18:00:00+00:00         10.491667      28.491667  321.992462   \n",
       "...                                     ...            ...         ...   \n",
       "2016-12-30 19:00:00+00:00         11.491667   26237.491667  412.514321   \n",
       "2016-12-30 20:00:00+00:00         12.491667   26238.491667  448.963991   \n",
       "2016-12-30 21:00:00+00:00         13.491667   26239.491667  427.167267   \n",
       "2016-12-30 22:00:00+00:00         14.491667   26240.491667  349.110038   \n",
       "2016-12-30 23:00:00+00:00         15.491667   26241.491667  222.901420   \n",
       "\n",
       "                               DNI_cs  ...    ENT(G)      AVG(B)     STD(B)  \\\n",
       "measurement_time                       ...                                    \n",
       "2014-01-03 14:00:00+00:00    0.000000  ...       NaN         NaN        NaN   \n",
       "2014-01-03 15:00:00+00:00    0.000000  ...  5.564803  151.246458  28.520968   \n",
       "2014-01-03 16:00:00+00:00  182.297840  ...  5.510188  141.713212  33.481258   \n",
       "2014-01-03 17:00:00+00:00  540.120302  ...  5.478535  137.213417  35.403827   \n",
       "2014-01-03 18:00:00+00:00  692.514065  ...  5.456043  138.616210  36.162608   \n",
       "...                               ...  ...       ...         ...        ...   \n",
       "2016-12-30 19:00:00+00:00  762.629492  ...  5.545958  135.665343  42.458667   \n",
       "2016-12-30 20:00:00+00:00  783.891589  ...  5.617600  128.237255  44.904030   \n",
       "2016-12-30 21:00:00+00:00  771.455229  ...  5.643208  119.943472  42.973342   \n",
       "2016-12-30 22:00:00+00:00  719.384366  ...  5.685198  123.339068  38.577063   \n",
       "2016-12-30 23:00:00+00:00  597.677942  ...  5.668687  130.965260  33.780193   \n",
       "\n",
       "                             ENT(B)   AVG(RB)   STD(RB)   ENT(RB)  AVG(NRB)  \\\n",
       "measurement_time                                                              \n",
       "2014-01-03 14:00:00+00:00       NaN       NaN       NaN       NaN       NaN   \n",
       "2014-01-03 15:00:00+00:00  5.360367  0.808373  0.144332  4.688985 -0.113270   \n",
       "2014-01-03 16:00:00+00:00  5.414777  0.909988  0.148683  4.742913 -0.053787   \n",
       "2014-01-03 17:00:00+00:00  5.377718  0.965367  0.142668  4.686905 -0.022647   \n",
       "2014-01-03 18:00:00+00:00  5.356960  0.947050  0.140950  4.682247 -0.032250   \n",
       "...                             ...       ...       ...       ...       ...   \n",
       "2016-12-30 19:00:00+00:00  5.359012  0.849203  0.178163  4.844802 -0.092885   \n",
       "2016-12-30 20:00:00+00:00  5.412868  0.902123  0.172672  4.814140 -0.059772   \n",
       "2016-12-30 21:00:00+00:00  5.470670  0.880178  0.188148  4.893065 -0.074325   \n",
       "2016-12-30 22:00:00+00:00  5.459773  0.789770  0.209435  4.991567 -0.132832   \n",
       "2016-12-30 23:00:00+00:00  5.382033  0.750620  0.212100  4.998280 -0.158547   \n",
       "\n",
       "                           STD(NRB)  ENT(NRB)  \n",
       "measurement_time                               \n",
       "2014-01-03 14:00:00+00:00       NaN       NaN  \n",
       "2014-01-03 15:00:00+00:00  0.086132  4.008980  \n",
       "2014-01-03 16:00:00+00:00  0.079303  3.913008  \n",
       "2014-01-03 17:00:00+00:00  0.071275  3.777387  \n",
       "2014-01-03 18:00:00+00:00  0.072482  3.788302  \n",
       "...                             ...       ...  \n",
       "2016-12-30 19:00:00+00:00  0.101738  4.111970  \n",
       "2016-12-30 20:00:00+00:00  0.093093  4.012365  \n",
       "2016-12-30 21:00:00+00:00  0.104203  4.123958  \n",
       "2016-12-30 22:00:00+00:00  0.127398  4.354853  \n",
       "2016-12-30 23:00:00+00:00  0.133200  4.417220  \n",
       "\n",
       "[11560 rows x 80 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_load_processed_dataframe_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "faf3f1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2 to_fixedgrid_multiindex\n",
    "# timestamp_col=timestamp_col  --> measurement_time\n",
    "# Convert a single-index time series (fixed steps per day) into the\n",
    "#    MultiIndex shape expected by build_model_arrays: index = [date, bin_id].\n",
    "\n",
    "ref_df = _load_processed_dataframe_df.copy()\n",
    "timestamp_col = \"measurement_time\"\n",
    "\n",
    "if ref_df.index.name == timestamp_col:\n",
    "    ref_df = ref_df.reset_index()\n",
    "\n",
    "\n",
    "ref_df[timestamp_col] = pd.to_datetime(ref_df[timestamp_col], utc=True)\n",
    "ref_df = ref_df.sort_values(timestamp_col)\n",
    "\n",
    "ref_df[\"UTC_date\"] = ref_df[timestamp_col].dt.tz_convert(\"UTC\").dt.date\n",
    "ref_df[\"PST_date\"] = (\n",
    "    ref_df[timestamp_col]\n",
    "    .dt.tz_convert(\"US/Pacific\")  # Converts UTC to Pacific Time (handles DST automatically)\n",
    "    .dt.date\n",
    ")\n",
    "\n",
    "ref_df[\"bin_id\"] = ref_df.groupby(\"PST_date\").cumcount()\n",
    "\n",
    "\n",
    "ref_df = ref_df.set_index([\"PST_date\", \"bin_id\"]).sort_index()\n",
    "ref_df.drop(columns=[timestamp_col], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d2ce365b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>ghi</th>\n",
       "      <th>dni</th>\n",
       "      <th>solar_zenith</th>\n",
       "      <th>time_gap_hours</th>\n",
       "      <th>time_gap_norm</th>\n",
       "      <th>day_boundary_flag</th>\n",
       "      <th>hour_progression</th>\n",
       "      <th>absolute_hour</th>\n",
       "      <th>GHI_cs</th>\n",
       "      <th>DNI_cs</th>\n",
       "      <th>...</th>\n",
       "      <th>AVG(B)</th>\n",
       "      <th>STD(B)</th>\n",
       "      <th>ENT(B)</th>\n",
       "      <th>AVG(RB)</th>\n",
       "      <th>STD(RB)</th>\n",
       "      <th>ENT(RB)</th>\n",
       "      <th>AVG(NRB)</th>\n",
       "      <th>STD(NRB)</th>\n",
       "      <th>ENT(NRB)</th>\n",
       "      <th>UTC_date</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PST_date</th>\n",
       "      <th>bin_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"11\" valign=\"top\">2014-01-03</th>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.276046</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.006944</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>6.491667</td>\n",
       "      <td>24.491667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.914500</td>\n",
       "      <td>89.245833</td>\n",
       "      <td>89.724162</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.491667</td>\n",
       "      <td>25.491667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>151.246458</td>\n",
       "      <td>28.520968</td>\n",
       "      <td>5.360367</td>\n",
       "      <td>0.808373</td>\n",
       "      <td>0.144332</td>\n",
       "      <td>4.688985</td>\n",
       "      <td>-0.113270</td>\n",
       "      <td>0.086132</td>\n",
       "      <td>4.008980</td>\n",
       "      <td>2014-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>143.775500</td>\n",
       "      <td>636.475000</td>\n",
       "      <td>80.146724</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.491667</td>\n",
       "      <td>26.491667</td>\n",
       "      <td>35.159500</td>\n",
       "      <td>182.297840</td>\n",
       "      <td>...</td>\n",
       "      <td>141.713212</td>\n",
       "      <td>33.481258</td>\n",
       "      <td>5.414777</td>\n",
       "      <td>0.909988</td>\n",
       "      <td>0.148683</td>\n",
       "      <td>4.742913</td>\n",
       "      <td>-0.053787</td>\n",
       "      <td>0.079303</td>\n",
       "      <td>3.913008</td>\n",
       "      <td>2014-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>289.955000</td>\n",
       "      <td>783.193333</td>\n",
       "      <td>71.988814</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.491667</td>\n",
       "      <td>27.491667</td>\n",
       "      <td>184.554711</td>\n",
       "      <td>540.120302</td>\n",
       "      <td>...</td>\n",
       "      <td>137.213417</td>\n",
       "      <td>35.403827</td>\n",
       "      <td>5.377718</td>\n",
       "      <td>0.965367</td>\n",
       "      <td>0.142668</td>\n",
       "      <td>4.686905</td>\n",
       "      <td>-0.022647</td>\n",
       "      <td>0.071275</td>\n",
       "      <td>3.777387</td>\n",
       "      <td>2014-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>402.643333</td>\n",
       "      <td>843.050000</td>\n",
       "      <td>65.816158</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.491667</td>\n",
       "      <td>28.491667</td>\n",
       "      <td>321.992462</td>\n",
       "      <td>692.514065</td>\n",
       "      <td>...</td>\n",
       "      <td>138.616210</td>\n",
       "      <td>36.162608</td>\n",
       "      <td>5.356960</td>\n",
       "      <td>0.947050</td>\n",
       "      <td>0.140950</td>\n",
       "      <td>4.682247</td>\n",
       "      <td>-0.032250</td>\n",
       "      <td>0.072482</td>\n",
       "      <td>3.788302</td>\n",
       "      <td>2014-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>467.720000</td>\n",
       "      <td>865.350000</td>\n",
       "      <td>62.240911</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.491667</td>\n",
       "      <td>29.491667</td>\n",
       "      <td>415.059808</td>\n",
       "      <td>759.195823</td>\n",
       "      <td>...</td>\n",
       "      <td>140.477427</td>\n",
       "      <td>36.514718</td>\n",
       "      <td>5.394765</td>\n",
       "      <td>0.942470</td>\n",
       "      <td>0.136820</td>\n",
       "      <td>4.604605</td>\n",
       "      <td>-0.034303</td>\n",
       "      <td>0.070148</td>\n",
       "      <td>3.716638</td>\n",
       "      <td>2014-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>477.143333</td>\n",
       "      <td>858.016667</td>\n",
       "      <td>61.722033</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.491667</td>\n",
       "      <td>30.491667</td>\n",
       "      <td>453.296762</td>\n",
       "      <td>781.432725</td>\n",
       "      <td>...</td>\n",
       "      <td>138.281643</td>\n",
       "      <td>36.980443</td>\n",
       "      <td>5.469040</td>\n",
       "      <td>0.941263</td>\n",
       "      <td>0.137760</td>\n",
       "      <td>4.562415</td>\n",
       "      <td>-0.035098</td>\n",
       "      <td>0.069278</td>\n",
       "      <td>3.685610</td>\n",
       "      <td>2014-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>431.993333</td>\n",
       "      <td>829.916667</td>\n",
       "      <td>64.333991</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.491667</td>\n",
       "      <td>31.491667</td>\n",
       "      <td>433.164386</td>\n",
       "      <td>770.036682</td>\n",
       "      <td>...</td>\n",
       "      <td>139.764325</td>\n",
       "      <td>37.120003</td>\n",
       "      <td>5.526557</td>\n",
       "      <td>0.823313</td>\n",
       "      <td>0.149278</td>\n",
       "      <td>4.659553</td>\n",
       "      <td>-0.104018</td>\n",
       "      <td>0.086640</td>\n",
       "      <td>3.955157</td>\n",
       "      <td>2014-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>334.925000</td>\n",
       "      <td>762.288333</td>\n",
       "      <td>69.722796</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.491667</td>\n",
       "      <td>32.491667</td>\n",
       "      <td>356.490073</td>\n",
       "      <td>719.649581</td>\n",
       "      <td>...</td>\n",
       "      <td>128.990700</td>\n",
       "      <td>34.709772</td>\n",
       "      <td>5.512125</td>\n",
       "      <td>0.817497</td>\n",
       "      <td>0.162657</td>\n",
       "      <td>4.847197</td>\n",
       "      <td>-0.110182</td>\n",
       "      <td>0.096112</td>\n",
       "      <td>4.153647</td>\n",
       "      <td>2014-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>202.008333</td>\n",
       "      <td>624.703333</td>\n",
       "      <td>77.299011</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.491667</td>\n",
       "      <td>33.491667</td>\n",
       "      <td>231.142819</td>\n",
       "      <td>601.929031</td>\n",
       "      <td>...</td>\n",
       "      <td>135.389552</td>\n",
       "      <td>29.963780</td>\n",
       "      <td>5.411160</td>\n",
       "      <td>0.791556</td>\n",
       "      <td>0.151100</td>\n",
       "      <td>4.733716</td>\n",
       "      <td>-0.125276</td>\n",
       "      <td>0.090712</td>\n",
       "      <td>4.073776</td>\n",
       "      <td>2014-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>107.755671</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.491667</td>\n",
       "      <td>36.491667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"11\" valign=\"top\">2014-01-04</th>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.295605</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.006944</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>6.491667</td>\n",
       "      <td>48.491667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.812000</td>\n",
       "      <td>106.038500</td>\n",
       "      <td>89.730501</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.491667</td>\n",
       "      <td>49.491667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>151.830285</td>\n",
       "      <td>28.118777</td>\n",
       "      <td>5.336317</td>\n",
       "      <td>0.817175</td>\n",
       "      <td>0.141500</td>\n",
       "      <td>4.673292</td>\n",
       "      <td>-0.107768</td>\n",
       "      <td>0.084160</td>\n",
       "      <td>3.978880</td>\n",
       "      <td>2014-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>149.329167</td>\n",
       "      <td>666.153333</td>\n",
       "      <td>80.135435</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.491667</td>\n",
       "      <td>50.491667</td>\n",
       "      <td>35.057149</td>\n",
       "      <td>180.894721</td>\n",
       "      <td>...</td>\n",
       "      <td>144.922088</td>\n",
       "      <td>34.831455</td>\n",
       "      <td>5.430310</td>\n",
       "      <td>0.861165</td>\n",
       "      <td>0.148620</td>\n",
       "      <td>4.710982</td>\n",
       "      <td>-0.081877</td>\n",
       "      <td>0.084528</td>\n",
       "      <td>3.953470</td>\n",
       "      <td>2014-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>295.938333</td>\n",
       "      <td>781.373333</td>\n",
       "      <td>71.955238</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.491667</td>\n",
       "      <td>51.491667</td>\n",
       "      <td>184.686491</td>\n",
       "      <td>538.868705</td>\n",
       "      <td>...</td>\n",
       "      <td>141.770771</td>\n",
       "      <td>37.428997</td>\n",
       "      <td>5.427462</td>\n",
       "      <td>0.922444</td>\n",
       "      <td>0.133137</td>\n",
       "      <td>4.575595</td>\n",
       "      <td>-0.045360</td>\n",
       "      <td>0.070730</td>\n",
       "      <td>3.731963</td>\n",
       "      <td>2014-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>412.915000</td>\n",
       "      <td>849.933333</td>\n",
       "      <td>65.756256</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.491667</td>\n",
       "      <td>52.491667</td>\n",
       "      <td>322.545238</td>\n",
       "      <td>691.633035</td>\n",
       "      <td>...</td>\n",
       "      <td>141.079027</td>\n",
       "      <td>38.324485</td>\n",
       "      <td>5.381808</td>\n",
       "      <td>0.929362</td>\n",
       "      <td>0.129098</td>\n",
       "      <td>4.519402</td>\n",
       "      <td>-0.040990</td>\n",
       "      <td>0.068018</td>\n",
       "      <td>3.663718</td>\n",
       "      <td>2014-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>481.871667</td>\n",
       "      <td>879.666667</td>\n",
       "      <td>62.153090</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.491667</td>\n",
       "      <td>53.491667</td>\n",
       "      <td>416.086275</td>\n",
       "      <td>758.579087</td>\n",
       "      <td>...</td>\n",
       "      <td>142.142238</td>\n",
       "      <td>38.756502</td>\n",
       "      <td>5.413757</td>\n",
       "      <td>0.937803</td>\n",
       "      <td>0.121530</td>\n",
       "      <td>4.416320</td>\n",
       "      <td>-0.035870</td>\n",
       "      <td>0.063268</td>\n",
       "      <td>3.543185</td>\n",
       "      <td>2014-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>494.761667</td>\n",
       "      <td>878.983333</td>\n",
       "      <td>61.608929</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.491667</td>\n",
       "      <td>54.491667</td>\n",
       "      <td>454.796742</td>\n",
       "      <td>781.046822</td>\n",
       "      <td>...</td>\n",
       "      <td>137.990735</td>\n",
       "      <td>38.398243</td>\n",
       "      <td>5.496002</td>\n",
       "      <td>0.925777</td>\n",
       "      <td>0.126680</td>\n",
       "      <td>4.448887</td>\n",
       "      <td>-0.042710</td>\n",
       "      <td>0.066027</td>\n",
       "      <td>3.600390</td>\n",
       "      <td>2014-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>451.253333</td>\n",
       "      <td>863.600000</td>\n",
       "      <td>64.202096</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.491667</td>\n",
       "      <td>55.491667</td>\n",
       "      <td>435.096089</td>\n",
       "      <td>769.931103</td>\n",
       "      <td>...</td>\n",
       "      <td>130.036534</td>\n",
       "      <td>37.598811</td>\n",
       "      <td>5.532799</td>\n",
       "      <td>0.891243</td>\n",
       "      <td>0.139253</td>\n",
       "      <td>4.602927</td>\n",
       "      <td>-0.063598</td>\n",
       "      <td>0.075476</td>\n",
       "      <td>3.802853</td>\n",
       "      <td>2014-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>351.236667</td>\n",
       "      <td>800.726667</td>\n",
       "      <td>69.579709</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.491667</td>\n",
       "      <td>56.491667</td>\n",
       "      <td>358.769963</td>\n",
       "      <td>720.027496</td>\n",
       "      <td>...</td>\n",
       "      <td>128.273690</td>\n",
       "      <td>34.207882</td>\n",
       "      <td>5.503617</td>\n",
       "      <td>0.829955</td>\n",
       "      <td>0.152173</td>\n",
       "      <td>4.785798</td>\n",
       "      <td>-0.100913</td>\n",
       "      <td>0.088750</td>\n",
       "      <td>4.071645</td>\n",
       "      <td>2014-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>215.715000</td>\n",
       "      <td>674.296667</td>\n",
       "      <td>77.150845</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.491667</td>\n",
       "      <td>57.491667</td>\n",
       "      <td>233.621874</td>\n",
       "      <td>603.446990</td>\n",
       "      <td>...</td>\n",
       "      <td>133.456664</td>\n",
       "      <td>29.692976</td>\n",
       "      <td>5.396316</td>\n",
       "      <td>0.839192</td>\n",
       "      <td>0.139892</td>\n",
       "      <td>4.672840</td>\n",
       "      <td>-0.094444</td>\n",
       "      <td>0.080616</td>\n",
       "      <td>3.947868</td>\n",
       "      <td>2014-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>107.608339</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.491667</td>\n",
       "      <td>60.491667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014-01-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">2014-01-05</th>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.309373</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.006944</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>6.491667</td>\n",
       "      <td>72.491667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014-01-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.258500</td>\n",
       "      <td>49.677333</td>\n",
       "      <td>89.730709</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.491667</td>\n",
       "      <td>73.491667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>151.511322</td>\n",
       "      <td>29.537898</td>\n",
       "      <td>5.348990</td>\n",
       "      <td>0.763902</td>\n",
       "      <td>0.179317</td>\n",
       "      <td>4.823355</td>\n",
       "      <td>-0.145243</td>\n",
       "      <td>0.111385</td>\n",
       "      <td>4.205522</td>\n",
       "      <td>2014-01-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>140.775333</td>\n",
       "      <td>524.465000</td>\n",
       "      <td>80.117609</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.491667</td>\n",
       "      <td>74.491667</td>\n",
       "      <td>35.034670</td>\n",
       "      <td>179.822095</td>\n",
       "      <td>...</td>\n",
       "      <td>142.721440</td>\n",
       "      <td>36.619222</td>\n",
       "      <td>5.430267</td>\n",
       "      <td>0.801897</td>\n",
       "      <td>0.179108</td>\n",
       "      <td>4.867320</td>\n",
       "      <td>-0.120825</td>\n",
       "      <td>0.108308</td>\n",
       "      <td>4.193485</td>\n",
       "      <td>2014-01-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>291.186667</td>\n",
       "      <td>716.588333</td>\n",
       "      <td>71.914706</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.491667</td>\n",
       "      <td>75.491667</td>\n",
       "      <td>184.942525</td>\n",
       "      <td>537.803992</td>\n",
       "      <td>...</td>\n",
       "      <td>144.722077</td>\n",
       "      <td>39.350747</td>\n",
       "      <td>5.383417</td>\n",
       "      <td>0.807070</td>\n",
       "      <td>0.173953</td>\n",
       "      <td>4.815242</td>\n",
       "      <td>-0.116730</td>\n",
       "      <td>0.104862</td>\n",
       "      <td>4.134953</td>\n",
       "      <td>2014-01-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>406.573333</td>\n",
       "      <td>786.248333</td>\n",
       "      <td>65.689066</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.491667</td>\n",
       "      <td>76.491667</td>\n",
       "      <td>323.229977</td>\n",
       "      <td>690.862149</td>\n",
       "      <td>...</td>\n",
       "      <td>143.262590</td>\n",
       "      <td>40.734672</td>\n",
       "      <td>5.347068</td>\n",
       "      <td>0.832162</td>\n",
       "      <td>0.169723</td>\n",
       "      <td>4.767225</td>\n",
       "      <td>-0.101165</td>\n",
       "      <td>0.099938</td>\n",
       "      <td>4.060762</td>\n",
       "      <td>2014-01-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>472.313333</td>\n",
       "      <td>807.516667</td>\n",
       "      <td>62.057876</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.491667</td>\n",
       "      <td>77.491667</td>\n",
       "      <td>417.244519</td>\n",
       "      <td>758.041489</td>\n",
       "      <td>...</td>\n",
       "      <td>141.804710</td>\n",
       "      <td>41.252360</td>\n",
       "      <td>5.378102</td>\n",
       "      <td>0.827838</td>\n",
       "      <td>0.170982</td>\n",
       "      <td>4.767675</td>\n",
       "      <td>-0.103633</td>\n",
       "      <td>0.100970</td>\n",
       "      <td>4.065813</td>\n",
       "      <td>2014-01-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>484.011667</td>\n",
       "      <td>803.938333</td>\n",
       "      <td>61.488672</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.491667</td>\n",
       "      <td>78.491667</td>\n",
       "      <td>456.424134</td>\n",
       "      <td>780.727637</td>\n",
       "      <td>...</td>\n",
       "      <td>140.669158</td>\n",
       "      <td>41.227707</td>\n",
       "      <td>5.440040</td>\n",
       "      <td>0.802408</td>\n",
       "      <td>0.173300</td>\n",
       "      <td>4.737590</td>\n",
       "      <td>-0.119337</td>\n",
       "      <td>0.103533</td>\n",
       "      <td>4.065168</td>\n",
       "      <td>2014-01-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>436.386667</td>\n",
       "      <td>772.050000</td>\n",
       "      <td>64.063642</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.491667</td>\n",
       "      <td>79.491667</td>\n",
       "      <td>437.147758</td>\n",
       "      <td>769.890264</td>\n",
       "      <td>...</td>\n",
       "      <td>132.115195</td>\n",
       "      <td>40.579995</td>\n",
       "      <td>5.493640</td>\n",
       "      <td>0.783880</td>\n",
       "      <td>0.180847</td>\n",
       "      <td>4.808515</td>\n",
       "      <td>-0.132252</td>\n",
       "      <td>0.110368</td>\n",
       "      <td>4.165503</td>\n",
       "      <td>2014-01-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          ghi         dni  solar_zenith  time_gap_hours  \\\n",
       "PST_date   bin_id                                                         \n",
       "2014-01-03 0         0.000000    0.000000    100.276046        0.166667   \n",
       "           1        12.914500   89.245833     89.724162        0.016667   \n",
       "           2       143.775500  636.475000     80.146724        0.016667   \n",
       "           3       289.955000  783.193333     71.988814        0.016667   \n",
       "           4       402.643333  843.050000     65.816158        0.016667   \n",
       "           5       467.720000  865.350000     62.240911        0.016667   \n",
       "           6       477.143333  858.016667     61.722033        0.016667   \n",
       "           7       431.993333  829.916667     64.333991        0.016667   \n",
       "           8       334.925000  762.288333     69.722796        0.016667   \n",
       "           9       202.008333  624.703333     77.299011        0.016667   \n",
       "           10        0.000000    0.000000    107.755671        0.016667   \n",
       "2014-01-04 0         0.000000    0.000000    100.295605        0.166667   \n",
       "           1        13.812000  106.038500     89.730501        0.016667   \n",
       "           2       149.329167  666.153333     80.135435        0.016667   \n",
       "           3       295.938333  781.373333     71.955238        0.016667   \n",
       "           4       412.915000  849.933333     65.756256        0.016667   \n",
       "           5       481.871667  879.666667     62.153090        0.016667   \n",
       "           6       494.761667  878.983333     61.608929        0.016667   \n",
       "           7       451.253333  863.600000     64.202096        0.016667   \n",
       "           8       351.236667  800.726667     69.579709        0.016667   \n",
       "           9       215.715000  674.296667     77.150845        0.016667   \n",
       "           10        0.000000    0.000000    107.608339        0.016667   \n",
       "2014-01-05 0         0.000000    0.000000    100.309373        0.166667   \n",
       "           1        12.258500   49.677333     89.730709        0.016667   \n",
       "           2       140.775333  524.465000     80.117609        0.016667   \n",
       "           3       291.186667  716.588333     71.914706        0.016667   \n",
       "           4       406.573333  786.248333     65.689066        0.016667   \n",
       "           5       472.313333  807.516667     62.057876        0.016667   \n",
       "           6       484.011667  803.938333     61.488672        0.016667   \n",
       "           7       436.386667  772.050000     64.063642        0.016667   \n",
       "\n",
       "                   time_gap_norm  day_boundary_flag  hour_progression  \\\n",
       "PST_date   bin_id                                                       \n",
       "2014-01-03 0            0.006944           0.016667          6.491667   \n",
       "           1            0.000694           0.000000          7.491667   \n",
       "           2            0.000694           0.000000          8.491667   \n",
       "           3            0.000694           0.000000          9.491667   \n",
       "           4            0.000694           0.000000         10.491667   \n",
       "           5            0.000694           0.000000         11.491667   \n",
       "           6            0.000694           0.000000         12.491667   \n",
       "           7            0.000694           0.000000         13.491667   \n",
       "           8            0.000694           0.000000         14.491667   \n",
       "           9            0.000694           0.000000         15.491667   \n",
       "           10           0.000694           0.000000         18.491667   \n",
       "2014-01-04 0            0.006944           0.016667          6.491667   \n",
       "           1            0.000694           0.000000          7.491667   \n",
       "           2            0.000694           0.000000          8.491667   \n",
       "           3            0.000694           0.000000          9.491667   \n",
       "           4            0.000694           0.000000         10.491667   \n",
       "           5            0.000694           0.000000         11.491667   \n",
       "           6            0.000694           0.000000         12.491667   \n",
       "           7            0.000694           0.000000         13.491667   \n",
       "           8            0.000694           0.000000         14.491667   \n",
       "           9            0.000694           0.000000         15.491667   \n",
       "           10           0.000694           0.000000         18.491667   \n",
       "2014-01-05 0            0.006944           0.016667          6.491667   \n",
       "           1            0.000694           0.000000          7.491667   \n",
       "           2            0.000694           0.000000          8.491667   \n",
       "           3            0.000694           0.000000          9.491667   \n",
       "           4            0.000694           0.000000         10.491667   \n",
       "           5            0.000694           0.000000         11.491667   \n",
       "           6            0.000694           0.000000         12.491667   \n",
       "           7            0.000694           0.000000         13.491667   \n",
       "\n",
       "                   absolute_hour      GHI_cs      DNI_cs  ...      AVG(B)  \\\n",
       "PST_date   bin_id                                         ...               \n",
       "2014-01-03 0           24.491667    0.000000    0.000000  ...         NaN   \n",
       "           1           25.491667    0.000000    0.000000  ...  151.246458   \n",
       "           2           26.491667   35.159500  182.297840  ...  141.713212   \n",
       "           3           27.491667  184.554711  540.120302  ...  137.213417   \n",
       "           4           28.491667  321.992462  692.514065  ...  138.616210   \n",
       "           5           29.491667  415.059808  759.195823  ...  140.477427   \n",
       "           6           30.491667  453.296762  781.432725  ...  138.281643   \n",
       "           7           31.491667  433.164386  770.036682  ...  139.764325   \n",
       "           8           32.491667  356.490073  719.649581  ...  128.990700   \n",
       "           9           33.491667  231.142819  601.929031  ...  135.389552   \n",
       "           10          36.491667    0.000000    0.000000  ...         NaN   \n",
       "2014-01-04 0           48.491667    0.000000    0.000000  ...         NaN   \n",
       "           1           49.491667    0.000000    0.000000  ...  151.830285   \n",
       "           2           50.491667   35.057149  180.894721  ...  144.922088   \n",
       "           3           51.491667  184.686491  538.868705  ...  141.770771   \n",
       "           4           52.491667  322.545238  691.633035  ...  141.079027   \n",
       "           5           53.491667  416.086275  758.579087  ...  142.142238   \n",
       "           6           54.491667  454.796742  781.046822  ...  137.990735   \n",
       "           7           55.491667  435.096089  769.931103  ...  130.036534   \n",
       "           8           56.491667  358.769963  720.027496  ...  128.273690   \n",
       "           9           57.491667  233.621874  603.446990  ...  133.456664   \n",
       "           10          60.491667    0.000000    0.000000  ...         NaN   \n",
       "2014-01-05 0           72.491667    0.000000    0.000000  ...         NaN   \n",
       "           1           73.491667    0.000000    0.000000  ...  151.511322   \n",
       "           2           74.491667   35.034670  179.822095  ...  142.721440   \n",
       "           3           75.491667  184.942525  537.803992  ...  144.722077   \n",
       "           4           76.491667  323.229977  690.862149  ...  143.262590   \n",
       "           5           77.491667  417.244519  758.041489  ...  141.804710   \n",
       "           6           78.491667  456.424134  780.727637  ...  140.669158   \n",
       "           7           79.491667  437.147758  769.890264  ...  132.115195   \n",
       "\n",
       "                      STD(B)    ENT(B)   AVG(RB)   STD(RB)   ENT(RB)  \\\n",
       "PST_date   bin_id                                                      \n",
       "2014-01-03 0             NaN       NaN       NaN       NaN       NaN   \n",
       "           1       28.520968  5.360367  0.808373  0.144332  4.688985   \n",
       "           2       33.481258  5.414777  0.909988  0.148683  4.742913   \n",
       "           3       35.403827  5.377718  0.965367  0.142668  4.686905   \n",
       "           4       36.162608  5.356960  0.947050  0.140950  4.682247   \n",
       "           5       36.514718  5.394765  0.942470  0.136820  4.604605   \n",
       "           6       36.980443  5.469040  0.941263  0.137760  4.562415   \n",
       "           7       37.120003  5.526557  0.823313  0.149278  4.659553   \n",
       "           8       34.709772  5.512125  0.817497  0.162657  4.847197   \n",
       "           9       29.963780  5.411160  0.791556  0.151100  4.733716   \n",
       "           10            NaN       NaN       NaN       NaN       NaN   \n",
       "2014-01-04 0             NaN       NaN       NaN       NaN       NaN   \n",
       "           1       28.118777  5.336317  0.817175  0.141500  4.673292   \n",
       "           2       34.831455  5.430310  0.861165  0.148620  4.710982   \n",
       "           3       37.428997  5.427462  0.922444  0.133137  4.575595   \n",
       "           4       38.324485  5.381808  0.929362  0.129098  4.519402   \n",
       "           5       38.756502  5.413757  0.937803  0.121530  4.416320   \n",
       "           6       38.398243  5.496002  0.925777  0.126680  4.448887   \n",
       "           7       37.598811  5.532799  0.891243  0.139253  4.602927   \n",
       "           8       34.207882  5.503617  0.829955  0.152173  4.785798   \n",
       "           9       29.692976  5.396316  0.839192  0.139892  4.672840   \n",
       "           10            NaN       NaN       NaN       NaN       NaN   \n",
       "2014-01-05 0             NaN       NaN       NaN       NaN       NaN   \n",
       "           1       29.537898  5.348990  0.763902  0.179317  4.823355   \n",
       "           2       36.619222  5.430267  0.801897  0.179108  4.867320   \n",
       "           3       39.350747  5.383417  0.807070  0.173953  4.815242   \n",
       "           4       40.734672  5.347068  0.832162  0.169723  4.767225   \n",
       "           5       41.252360  5.378102  0.827838  0.170982  4.767675   \n",
       "           6       41.227707  5.440040  0.802408  0.173300  4.737590   \n",
       "           7       40.579995  5.493640  0.783880  0.180847  4.808515   \n",
       "\n",
       "                   AVG(NRB)  STD(NRB)  ENT(NRB)    UTC_date  \n",
       "PST_date   bin_id                                            \n",
       "2014-01-03 0            NaN       NaN       NaN  2014-01-03  \n",
       "           1      -0.113270  0.086132  4.008980  2014-01-03  \n",
       "           2      -0.053787  0.079303  3.913008  2014-01-03  \n",
       "           3      -0.022647  0.071275  3.777387  2014-01-03  \n",
       "           4      -0.032250  0.072482  3.788302  2014-01-03  \n",
       "           5      -0.034303  0.070148  3.716638  2014-01-03  \n",
       "           6      -0.035098  0.069278  3.685610  2014-01-03  \n",
       "           7      -0.104018  0.086640  3.955157  2014-01-03  \n",
       "           8      -0.110182  0.096112  4.153647  2014-01-03  \n",
       "           9      -0.125276  0.090712  4.073776  2014-01-03  \n",
       "           10           NaN       NaN       NaN  2014-01-04  \n",
       "2014-01-04 0            NaN       NaN       NaN  2014-01-04  \n",
       "           1      -0.107768  0.084160  3.978880  2014-01-04  \n",
       "           2      -0.081877  0.084528  3.953470  2014-01-04  \n",
       "           3      -0.045360  0.070730  3.731963  2014-01-04  \n",
       "           4      -0.040990  0.068018  3.663718  2014-01-04  \n",
       "           5      -0.035870  0.063268  3.543185  2014-01-04  \n",
       "           6      -0.042710  0.066027  3.600390  2014-01-04  \n",
       "           7      -0.063598  0.075476  3.802853  2014-01-04  \n",
       "           8      -0.100913  0.088750  4.071645  2014-01-04  \n",
       "           9      -0.094444  0.080616  3.947868  2014-01-04  \n",
       "           10           NaN       NaN       NaN  2014-01-05  \n",
       "2014-01-05 0            NaN       NaN       NaN  2014-01-05  \n",
       "           1      -0.145243  0.111385  4.205522  2014-01-05  \n",
       "           2      -0.120825  0.108308  4.193485  2014-01-05  \n",
       "           3      -0.116730  0.104862  4.134953  2014-01-05  \n",
       "           4      -0.101165  0.099938  4.060762  2014-01-05  \n",
       "           5      -0.103633  0.100970  4.065813  2014-01-05  \n",
       "           6      -0.119337  0.103533  4.065168  2014-01-05  \n",
       "           7      -0.132252  0.110368  4.165503  2014-01-05  \n",
       "\n",
       "[30 rows x 81 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b55da1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation_utils import build_reference_fixedgrid_frame, SiteLocation\n",
    "\n",
    "reference_df = build_reference_fixedgrid_frame(\n",
    "    csv_path,\n",
    "    site_config,\n",
    "    timestamp_col= \"measurement_time\",\n",
    "    target_time_col= \"nam_target_time\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f7a210be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>ghi</th>\n",
       "      <th>dni</th>\n",
       "      <th>solar_zenith</th>\n",
       "      <th>time_gap_hours</th>\n",
       "      <th>time_gap_norm</th>\n",
       "      <th>day_boundary_flag</th>\n",
       "      <th>hour_progression</th>\n",
       "      <th>absolute_hour</th>\n",
       "      <th>GHI_cs</th>\n",
       "      <th>DNI_cs</th>\n",
       "      <th>...</th>\n",
       "      <th>STD(RB)</th>\n",
       "      <th>ENT(RB)</th>\n",
       "      <th>AVG(NRB)</th>\n",
       "      <th>STD(NRB)</th>\n",
       "      <th>ENT(NRB)</th>\n",
       "      <th>clear_sky_ghi</th>\n",
       "      <th>clear_sky_dni</th>\n",
       "      <th>actual_csi</th>\n",
       "      <th>actual_ghi</th>\n",
       "      <th>nam_csi</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th>bin_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">2014-01-03</th>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.276046</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.006944</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>6.491667</td>\n",
       "      <td>24.491667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.914500</td>\n",
       "      <td>89.245833</td>\n",
       "      <td>89.724162</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.491667</td>\n",
       "      <td>25.491667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144332</td>\n",
       "      <td>4.688985</td>\n",
       "      <td>-0.113270</td>\n",
       "      <td>0.086132</td>\n",
       "      <td>4.008980</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>143.775500</td>\n",
       "      <td>636.475000</td>\n",
       "      <td>80.146724</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.491667</td>\n",
       "      <td>26.491667</td>\n",
       "      <td>35.159500</td>\n",
       "      <td>182.297840</td>\n",
       "      <td>...</td>\n",
       "      <td>0.148683</td>\n",
       "      <td>4.742913</td>\n",
       "      <td>-0.053787</td>\n",
       "      <td>0.079303</td>\n",
       "      <td>3.913008</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>289.955000</td>\n",
       "      <td>783.193333</td>\n",
       "      <td>71.988814</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.491667</td>\n",
       "      <td>27.491667</td>\n",
       "      <td>184.554711</td>\n",
       "      <td>540.120302</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142668</td>\n",
       "      <td>4.686905</td>\n",
       "      <td>-0.022647</td>\n",
       "      <td>0.071275</td>\n",
       "      <td>3.777387</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>402.643333</td>\n",
       "      <td>843.050000</td>\n",
       "      <td>65.816158</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.491667</td>\n",
       "      <td>28.491667</td>\n",
       "      <td>321.992462</td>\n",
       "      <td>692.514065</td>\n",
       "      <td>...</td>\n",
       "      <td>0.140950</td>\n",
       "      <td>4.682247</td>\n",
       "      <td>-0.032250</td>\n",
       "      <td>0.072482</td>\n",
       "      <td>3.788302</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>467.720000</td>\n",
       "      <td>865.350000</td>\n",
       "      <td>62.240911</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.491667</td>\n",
       "      <td>29.491667</td>\n",
       "      <td>415.059808</td>\n",
       "      <td>759.195823</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136820</td>\n",
       "      <td>4.604605</td>\n",
       "      <td>-0.034303</td>\n",
       "      <td>0.070148</td>\n",
       "      <td>3.716638</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.126874</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>477.143333</td>\n",
       "      <td>858.016667</td>\n",
       "      <td>61.722033</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.491667</td>\n",
       "      <td>30.491667</td>\n",
       "      <td>453.296762</td>\n",
       "      <td>781.432725</td>\n",
       "      <td>...</td>\n",
       "      <td>0.137760</td>\n",
       "      <td>4.562415</td>\n",
       "      <td>-0.035098</td>\n",
       "      <td>0.069278</td>\n",
       "      <td>3.685610</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.052607</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>431.993333</td>\n",
       "      <td>829.916667</td>\n",
       "      <td>64.333991</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.491667</td>\n",
       "      <td>31.491667</td>\n",
       "      <td>433.164386</td>\n",
       "      <td>770.036682</td>\n",
       "      <td>...</td>\n",
       "      <td>0.149278</td>\n",
       "      <td>4.659553</td>\n",
       "      <td>-0.104018</td>\n",
       "      <td>0.086640</td>\n",
       "      <td>3.955157</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.997297</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>334.925000</td>\n",
       "      <td>762.288333</td>\n",
       "      <td>69.722796</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.491667</td>\n",
       "      <td>32.491667</td>\n",
       "      <td>356.490073</td>\n",
       "      <td>719.649581</td>\n",
       "      <td>...</td>\n",
       "      <td>0.162657</td>\n",
       "      <td>4.847197</td>\n",
       "      <td>-0.110182</td>\n",
       "      <td>0.096112</td>\n",
       "      <td>4.153647</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.939507</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>202.008333</td>\n",
       "      <td>624.703333</td>\n",
       "      <td>77.299011</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.491667</td>\n",
       "      <td>33.491667</td>\n",
       "      <td>231.142819</td>\n",
       "      <td>601.929031</td>\n",
       "      <td>...</td>\n",
       "      <td>0.151100</td>\n",
       "      <td>4.733716</td>\n",
       "      <td>-0.125276</td>\n",
       "      <td>0.090712</td>\n",
       "      <td>4.073776</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.873955</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"11\" valign=\"top\">2014-01-04</th>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>107.755671</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.491667</td>\n",
       "      <td>36.491667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.295605</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.006944</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>6.491667</td>\n",
       "      <td>48.491667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.812000</td>\n",
       "      <td>106.038500</td>\n",
       "      <td>89.730501</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.491667</td>\n",
       "      <td>49.491667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.141500</td>\n",
       "      <td>4.673292</td>\n",
       "      <td>-0.107768</td>\n",
       "      <td>0.084160</td>\n",
       "      <td>3.978880</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>149.329167</td>\n",
       "      <td>666.153333</td>\n",
       "      <td>80.135435</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.491667</td>\n",
       "      <td>50.491667</td>\n",
       "      <td>35.057149</td>\n",
       "      <td>180.894721</td>\n",
       "      <td>...</td>\n",
       "      <td>0.148620</td>\n",
       "      <td>4.710982</td>\n",
       "      <td>-0.081877</td>\n",
       "      <td>0.084528</td>\n",
       "      <td>3.953470</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>295.938333</td>\n",
       "      <td>781.373333</td>\n",
       "      <td>71.955238</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.491667</td>\n",
       "      <td>51.491667</td>\n",
       "      <td>184.686491</td>\n",
       "      <td>538.868705</td>\n",
       "      <td>...</td>\n",
       "      <td>0.133137</td>\n",
       "      <td>4.575595</td>\n",
       "      <td>-0.045360</td>\n",
       "      <td>0.070730</td>\n",
       "      <td>3.731963</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>412.915000</td>\n",
       "      <td>849.933333</td>\n",
       "      <td>65.756256</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.491667</td>\n",
       "      <td>52.491667</td>\n",
       "      <td>322.545238</td>\n",
       "      <td>691.633035</td>\n",
       "      <td>...</td>\n",
       "      <td>0.129098</td>\n",
       "      <td>4.519402</td>\n",
       "      <td>-0.040990</td>\n",
       "      <td>0.068018</td>\n",
       "      <td>3.663718</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>481.871667</td>\n",
       "      <td>879.666667</td>\n",
       "      <td>62.153090</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.491667</td>\n",
       "      <td>53.491667</td>\n",
       "      <td>416.086275</td>\n",
       "      <td>758.579087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121530</td>\n",
       "      <td>4.416320</td>\n",
       "      <td>-0.035870</td>\n",
       "      <td>0.063268</td>\n",
       "      <td>3.543185</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.158105</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>494.761667</td>\n",
       "      <td>878.983333</td>\n",
       "      <td>61.608929</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.491667</td>\n",
       "      <td>54.491667</td>\n",
       "      <td>454.796742</td>\n",
       "      <td>781.046822</td>\n",
       "      <td>...</td>\n",
       "      <td>0.126680</td>\n",
       "      <td>4.448887</td>\n",
       "      <td>-0.042710</td>\n",
       "      <td>0.066027</td>\n",
       "      <td>3.600390</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.087874</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>451.253333</td>\n",
       "      <td>863.600000</td>\n",
       "      <td>64.202096</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.491667</td>\n",
       "      <td>55.491667</td>\n",
       "      <td>435.096089</td>\n",
       "      <td>769.931103</td>\n",
       "      <td>...</td>\n",
       "      <td>0.139253</td>\n",
       "      <td>4.602927</td>\n",
       "      <td>-0.063598</td>\n",
       "      <td>0.075476</td>\n",
       "      <td>3.802853</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.037135</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>351.236667</td>\n",
       "      <td>800.726667</td>\n",
       "      <td>69.579709</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.491667</td>\n",
       "      <td>56.491667</td>\n",
       "      <td>358.769963</td>\n",
       "      <td>720.027496</td>\n",
       "      <td>...</td>\n",
       "      <td>0.152173</td>\n",
       "      <td>4.785798</td>\n",
       "      <td>-0.100913</td>\n",
       "      <td>0.088750</td>\n",
       "      <td>4.071645</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.979002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>215.715000</td>\n",
       "      <td>674.296667</td>\n",
       "      <td>77.150845</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.491667</td>\n",
       "      <td>57.491667</td>\n",
       "      <td>233.621874</td>\n",
       "      <td>603.446990</td>\n",
       "      <td>...</td>\n",
       "      <td>0.139892</td>\n",
       "      <td>4.672840</td>\n",
       "      <td>-0.094444</td>\n",
       "      <td>0.080616</td>\n",
       "      <td>3.947868</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.923351</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">2014-01-05</th>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>107.608339</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.491667</td>\n",
       "      <td>60.491667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.309373</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.006944</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>6.491667</td>\n",
       "      <td>72.491667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.258500</td>\n",
       "      <td>49.677333</td>\n",
       "      <td>89.730709</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.491667</td>\n",
       "      <td>73.491667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.179317</td>\n",
       "      <td>4.823355</td>\n",
       "      <td>-0.145243</td>\n",
       "      <td>0.111385</td>\n",
       "      <td>4.205522</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>140.775333</td>\n",
       "      <td>524.465000</td>\n",
       "      <td>80.117609</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.491667</td>\n",
       "      <td>74.491667</td>\n",
       "      <td>35.034670</td>\n",
       "      <td>179.822095</td>\n",
       "      <td>...</td>\n",
       "      <td>0.179108</td>\n",
       "      <td>4.867320</td>\n",
       "      <td>-0.120825</td>\n",
       "      <td>0.108308</td>\n",
       "      <td>4.193485</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>291.186667</td>\n",
       "      <td>716.588333</td>\n",
       "      <td>71.914706</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.491667</td>\n",
       "      <td>75.491667</td>\n",
       "      <td>184.942525</td>\n",
       "      <td>537.803992</td>\n",
       "      <td>...</td>\n",
       "      <td>0.173953</td>\n",
       "      <td>4.815242</td>\n",
       "      <td>-0.116730</td>\n",
       "      <td>0.104862</td>\n",
       "      <td>4.134953</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>406.573333</td>\n",
       "      <td>786.248333</td>\n",
       "      <td>65.689066</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.491667</td>\n",
       "      <td>76.491667</td>\n",
       "      <td>323.229977</td>\n",
       "      <td>690.862149</td>\n",
       "      <td>...</td>\n",
       "      <td>0.169723</td>\n",
       "      <td>4.767225</td>\n",
       "      <td>-0.101165</td>\n",
       "      <td>0.099938</td>\n",
       "      <td>4.060762</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>472.313333</td>\n",
       "      <td>807.516667</td>\n",
       "      <td>62.057876</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.491667</td>\n",
       "      <td>77.491667</td>\n",
       "      <td>417.244519</td>\n",
       "      <td>758.041489</td>\n",
       "      <td>...</td>\n",
       "      <td>0.170982</td>\n",
       "      <td>4.767675</td>\n",
       "      <td>-0.103633</td>\n",
       "      <td>0.100970</td>\n",
       "      <td>4.065813</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.131982</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>484.011667</td>\n",
       "      <td>803.938333</td>\n",
       "      <td>61.488672</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.491667</td>\n",
       "      <td>78.491667</td>\n",
       "      <td>456.424134</td>\n",
       "      <td>780.727637</td>\n",
       "      <td>...</td>\n",
       "      <td>0.173300</td>\n",
       "      <td>4.737590</td>\n",
       "      <td>-0.119337</td>\n",
       "      <td>0.103533</td>\n",
       "      <td>4.065168</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.060443</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>436.386667</td>\n",
       "      <td>772.050000</td>\n",
       "      <td>64.063642</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.491667</td>\n",
       "      <td>79.491667</td>\n",
       "      <td>437.147758</td>\n",
       "      <td>769.890264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.180847</td>\n",
       "      <td>4.808515</td>\n",
       "      <td>-0.132252</td>\n",
       "      <td>0.110368</td>\n",
       "      <td>4.165503</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.998259</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30 rows × 85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          ghi         dni  solar_zenith  time_gap_hours  \\\n",
       "date       bin_id                                                         \n",
       "2014-01-03 0         0.000000    0.000000    100.276046        0.166667   \n",
       "           1        12.914500   89.245833     89.724162        0.016667   \n",
       "           2       143.775500  636.475000     80.146724        0.016667   \n",
       "           3       289.955000  783.193333     71.988814        0.016667   \n",
       "           4       402.643333  843.050000     65.816158        0.016667   \n",
       "           5       467.720000  865.350000     62.240911        0.016667   \n",
       "           6       477.143333  858.016667     61.722033        0.016667   \n",
       "           7       431.993333  829.916667     64.333991        0.016667   \n",
       "           8       334.925000  762.288333     69.722796        0.016667   \n",
       "           9       202.008333  624.703333     77.299011        0.016667   \n",
       "2014-01-04 0         0.000000    0.000000    107.755671        0.016667   \n",
       "           1         0.000000    0.000000    100.295605        0.166667   \n",
       "           2        13.812000  106.038500     89.730501        0.016667   \n",
       "           3       149.329167  666.153333     80.135435        0.016667   \n",
       "           4       295.938333  781.373333     71.955238        0.016667   \n",
       "           5       412.915000  849.933333     65.756256        0.016667   \n",
       "           6       481.871667  879.666667     62.153090        0.016667   \n",
       "           7       494.761667  878.983333     61.608929        0.016667   \n",
       "           8       451.253333  863.600000     64.202096        0.016667   \n",
       "           9       351.236667  800.726667     69.579709        0.016667   \n",
       "           10      215.715000  674.296667     77.150845        0.016667   \n",
       "2014-01-05 0         0.000000    0.000000    107.608339        0.016667   \n",
       "           1         0.000000    0.000000    100.309373        0.166667   \n",
       "           2        12.258500   49.677333     89.730709        0.016667   \n",
       "           3       140.775333  524.465000     80.117609        0.016667   \n",
       "           4       291.186667  716.588333     71.914706        0.016667   \n",
       "           5       406.573333  786.248333     65.689066        0.016667   \n",
       "           6       472.313333  807.516667     62.057876        0.016667   \n",
       "           7       484.011667  803.938333     61.488672        0.016667   \n",
       "           8       436.386667  772.050000     64.063642        0.016667   \n",
       "\n",
       "                   time_gap_norm  day_boundary_flag  hour_progression  \\\n",
       "date       bin_id                                                       \n",
       "2014-01-03 0            0.006944           0.016667          6.491667   \n",
       "           1            0.000694           0.000000          7.491667   \n",
       "           2            0.000694           0.000000          8.491667   \n",
       "           3            0.000694           0.000000          9.491667   \n",
       "           4            0.000694           0.000000         10.491667   \n",
       "           5            0.000694           0.000000         11.491667   \n",
       "           6            0.000694           0.000000         12.491667   \n",
       "           7            0.000694           0.000000         13.491667   \n",
       "           8            0.000694           0.000000         14.491667   \n",
       "           9            0.000694           0.000000         15.491667   \n",
       "2014-01-04 0            0.000694           0.000000         18.491667   \n",
       "           1            0.006944           0.016667          6.491667   \n",
       "           2            0.000694           0.000000          7.491667   \n",
       "           3            0.000694           0.000000          8.491667   \n",
       "           4            0.000694           0.000000          9.491667   \n",
       "           5            0.000694           0.000000         10.491667   \n",
       "           6            0.000694           0.000000         11.491667   \n",
       "           7            0.000694           0.000000         12.491667   \n",
       "           8            0.000694           0.000000         13.491667   \n",
       "           9            0.000694           0.000000         14.491667   \n",
       "           10           0.000694           0.000000         15.491667   \n",
       "2014-01-05 0            0.000694           0.000000         18.491667   \n",
       "           1            0.006944           0.016667          6.491667   \n",
       "           2            0.000694           0.000000          7.491667   \n",
       "           3            0.000694           0.000000          8.491667   \n",
       "           4            0.000694           0.000000          9.491667   \n",
       "           5            0.000694           0.000000         10.491667   \n",
       "           6            0.000694           0.000000         11.491667   \n",
       "           7            0.000694           0.000000         12.491667   \n",
       "           8            0.000694           0.000000         13.491667   \n",
       "\n",
       "                   absolute_hour      GHI_cs      DNI_cs  ...   STD(RB)  \\\n",
       "date       bin_id                                         ...             \n",
       "2014-01-03 0           24.491667    0.000000    0.000000  ...       NaN   \n",
       "           1           25.491667    0.000000    0.000000  ...  0.144332   \n",
       "           2           26.491667   35.159500  182.297840  ...  0.148683   \n",
       "           3           27.491667  184.554711  540.120302  ...  0.142668   \n",
       "           4           28.491667  321.992462  692.514065  ...  0.140950   \n",
       "           5           29.491667  415.059808  759.195823  ...  0.136820   \n",
       "           6           30.491667  453.296762  781.432725  ...  0.137760   \n",
       "           7           31.491667  433.164386  770.036682  ...  0.149278   \n",
       "           8           32.491667  356.490073  719.649581  ...  0.162657   \n",
       "           9           33.491667  231.142819  601.929031  ...  0.151100   \n",
       "2014-01-04 0           36.491667    0.000000    0.000000  ...       NaN   \n",
       "           1           48.491667    0.000000    0.000000  ...       NaN   \n",
       "           2           49.491667    0.000000    0.000000  ...  0.141500   \n",
       "           3           50.491667   35.057149  180.894721  ...  0.148620   \n",
       "           4           51.491667  184.686491  538.868705  ...  0.133137   \n",
       "           5           52.491667  322.545238  691.633035  ...  0.129098   \n",
       "           6           53.491667  416.086275  758.579087  ...  0.121530   \n",
       "           7           54.491667  454.796742  781.046822  ...  0.126680   \n",
       "           8           55.491667  435.096089  769.931103  ...  0.139253   \n",
       "           9           56.491667  358.769963  720.027496  ...  0.152173   \n",
       "           10          57.491667  233.621874  603.446990  ...  0.139892   \n",
       "2014-01-05 0           60.491667    0.000000    0.000000  ...       NaN   \n",
       "           1           72.491667    0.000000    0.000000  ...       NaN   \n",
       "           2           73.491667    0.000000    0.000000  ...  0.179317   \n",
       "           3           74.491667   35.034670  179.822095  ...  0.179108   \n",
       "           4           75.491667  184.942525  537.803992  ...  0.173953   \n",
       "           5           76.491667  323.229977  690.862149  ...  0.169723   \n",
       "           6           77.491667  417.244519  758.041489  ...  0.170982   \n",
       "           7           78.491667  456.424134  780.727637  ...  0.173300   \n",
       "           8           79.491667  437.147758  769.890264  ...  0.180847   \n",
       "\n",
       "                    ENT(RB)  AVG(NRB)  STD(NRB)  ENT(NRB)  clear_sky_ghi  \\\n",
       "date       bin_id                                                          \n",
       "2014-01-03 0            NaN       NaN       NaN       NaN            NaN   \n",
       "           1       4.688985 -0.113270  0.086132  4.008980            NaN   \n",
       "           2       4.742913 -0.053787  0.079303  3.913008            NaN   \n",
       "           3       4.686905 -0.022647  0.071275  3.777387            NaN   \n",
       "           4       4.682247 -0.032250  0.072482  3.788302            NaN   \n",
       "           5       4.604605 -0.034303  0.070148  3.716638            NaN   \n",
       "           6       4.562415 -0.035098  0.069278  3.685610            NaN   \n",
       "           7       4.659553 -0.104018  0.086640  3.955157            NaN   \n",
       "           8       4.847197 -0.110182  0.096112  4.153647            NaN   \n",
       "           9       4.733716 -0.125276  0.090712  4.073776            NaN   \n",
       "2014-01-04 0            NaN       NaN       NaN       NaN            NaN   \n",
       "           1            NaN       NaN       NaN       NaN            NaN   \n",
       "           2       4.673292 -0.107768  0.084160  3.978880            NaN   \n",
       "           3       4.710982 -0.081877  0.084528  3.953470            NaN   \n",
       "           4       4.575595 -0.045360  0.070730  3.731963            NaN   \n",
       "           5       4.519402 -0.040990  0.068018  3.663718            NaN   \n",
       "           6       4.416320 -0.035870  0.063268  3.543185            NaN   \n",
       "           7       4.448887 -0.042710  0.066027  3.600390            NaN   \n",
       "           8       4.602927 -0.063598  0.075476  3.802853            NaN   \n",
       "           9       4.785798 -0.100913  0.088750  4.071645            NaN   \n",
       "           10      4.672840 -0.094444  0.080616  3.947868            NaN   \n",
       "2014-01-05 0            NaN       NaN       NaN       NaN            NaN   \n",
       "           1            NaN       NaN       NaN       NaN            NaN   \n",
       "           2       4.823355 -0.145243  0.111385  4.205522            NaN   \n",
       "           3       4.867320 -0.120825  0.108308  4.193485            NaN   \n",
       "           4       4.815242 -0.116730  0.104862  4.134953            NaN   \n",
       "           5       4.767225 -0.101165  0.099938  4.060762            NaN   \n",
       "           6       4.767675 -0.103633  0.100970  4.065813            NaN   \n",
       "           7       4.737590 -0.119337  0.103533  4.065168            NaN   \n",
       "           8       4.808515 -0.132252  0.110368  4.165503            NaN   \n",
       "\n",
       "                   clear_sky_dni  actual_csi  actual_ghi  nam_csi  \n",
       "date       bin_id                                                  \n",
       "2014-01-03 0                 NaN    0.000000         NaN      NaN  \n",
       "           1                 NaN    1.200000         NaN      NaN  \n",
       "           2                 NaN    1.200000         NaN      NaN  \n",
       "           3                 NaN    1.200000         NaN      NaN  \n",
       "           4                 NaN    1.200000         NaN      NaN  \n",
       "           5                 NaN    1.126874         NaN      NaN  \n",
       "           6                 NaN    1.052607         NaN      NaN  \n",
       "           7                 NaN    0.997297         NaN      NaN  \n",
       "           8                 NaN    0.939507         NaN      NaN  \n",
       "           9                 NaN    0.873955         NaN      NaN  \n",
       "2014-01-04 0                 NaN    0.000000         NaN      NaN  \n",
       "           1                 NaN    0.000000         NaN      NaN  \n",
       "           2                 NaN    1.200000         NaN      NaN  \n",
       "           3                 NaN    1.200000         NaN      NaN  \n",
       "           4                 NaN    1.200000         NaN      NaN  \n",
       "           5                 NaN    1.200000         NaN      NaN  \n",
       "           6                 NaN    1.158105         NaN      NaN  \n",
       "           7                 NaN    1.087874         NaN      NaN  \n",
       "           8                 NaN    1.037135         NaN      NaN  \n",
       "           9                 NaN    0.979002         NaN      NaN  \n",
       "           10                NaN    0.923351         NaN      NaN  \n",
       "2014-01-05 0                 NaN    0.000000         NaN      NaN  \n",
       "           1                 NaN    0.000000         NaN      NaN  \n",
       "           2                 NaN    1.200000         NaN      NaN  \n",
       "           3                 NaN    1.200000         NaN      NaN  \n",
       "           4                 NaN    1.200000         NaN      NaN  \n",
       "           5                 NaN    1.200000         NaN      NaN  \n",
       "           6                 NaN    1.131982         NaN      NaN  \n",
       "           7                 NaN    1.060443         NaN      NaN  \n",
       "           8                 NaN    0.998259         NaN      NaN  \n",
       "\n",
       "[30 rows x 85 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66085cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pipeline import SolarForecastingPipeline\n",
    "\n",
    "pipeline = SolarForecastingPipeline(LSTM_CONFIG)\n",
    "_, summary = pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f128d9",
   "metadata": {},
   "source": [
    "### End of Data Preparation Validation \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fb472b",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3777e2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_phase1 = df_interpolated[['solar_zenith', 'CSI_ghi','time_gap_hours',\n",
    "       'time_gap_norm', 'day_boundary_flag', 'hour_progression',\n",
    "       'absolute_hour',\n",
    "       'season_flag', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos']]\n",
    "\n",
    "\n",
    "df_phase2 = df_interpolated[['solar_zenith', 'CSI_ghi','time_gap_hours',\n",
    "       'time_gap_norm', 'day_boundary_flag', 'hour_progression',\n",
    "       'absolute_hour',\n",
    "       'season_flag', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos', 'B_CSI_ghi_8h', 'V_CSI_ghi_8h', 'L_CSI_ghi_8h', 'B_CSI_ghi_9h',\n",
    "       'V_CSI_ghi_9h', 'L_CSI_ghi_9h', 'B_CSI_ghi_10h', 'V_CSI_ghi_10h',\n",
    "       'L_CSI_ghi_10h', 'B_CSI_ghi_11h', 'V_CSI_ghi_11h', 'L_CSI_ghi_11h',\n",
    "       'B_CSI_ghi_12h', 'V_CSI_ghi_12h', 'L_CSI_ghi_12h', 'B_CSI_ghi_13h',\n",
    "       'V_CSI_ghi_13h', 'L_CSI_ghi_13h', 'B_CSI_ghi_14h', 'V_CSI_ghi_14h',\n",
    "       'L_CSI_ghi_14h', 'B_CSI_ghi_15h', 'V_CSI_ghi_15h', 'L_CSI_ghi_15h',\n",
    "       'B_CSI_ghi_16h', 'V_CSI_ghi_16h', 'L_CSI_ghi_16h', 'B_CSI_ghi_17h',\n",
    "       'V_CSI_ghi_17h', 'L_CSI_ghi_17h', 'B_CSI_ghi_18h', 'V_CSI_ghi_18h',\n",
    "       'L_CSI_ghi_18h', 'B_CSI_ghi_19h', 'V_CSI_ghi_19h', 'L_CSI_ghi_19h']]\n",
    "\n",
    "df_phase3 = df_interpolated[['solar_zenith', 'CSI_ghi','time_gap_hours',\n",
    "       'time_gap_norm', 'day_boundary_flag', 'hour_progression',\n",
    "       'absolute_hour',\n",
    "       'season_flag', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos', 'B_CSI_ghi_8h', 'V_CSI_ghi_8h', 'L_CSI_ghi_8h', 'B_CSI_ghi_9h',\n",
    "       'V_CSI_ghi_9h', 'L_CSI_ghi_9h', 'B_CSI_ghi_10h', 'V_CSI_ghi_10h',\n",
    "       'L_CSI_ghi_10h', 'B_CSI_ghi_11h', 'V_CSI_ghi_11h', 'L_CSI_ghi_11h',\n",
    "       'B_CSI_ghi_12h', 'V_CSI_ghi_12h', 'L_CSI_ghi_12h', 'B_CSI_ghi_13h',\n",
    "       'V_CSI_ghi_13h', 'L_CSI_ghi_13h', 'B_CSI_ghi_14h', 'V_CSI_ghi_14h',\n",
    "       'L_CSI_ghi_14h', 'B_CSI_ghi_15h', 'V_CSI_ghi_15h', 'L_CSI_ghi_15h',\n",
    "       'B_CSI_ghi_16h', 'V_CSI_ghi_16h', 'L_CSI_ghi_16h', 'B_CSI_ghi_17h',\n",
    "       'V_CSI_ghi_17h', 'L_CSI_ghi_17h', 'B_CSI_ghi_18h', 'V_CSI_ghi_18h',\n",
    "       'L_CSI_ghi_18h', 'B_CSI_ghi_19h', 'V_CSI_ghi_19h', 'L_CSI_ghi_19h',\n",
    "       'nam_ghi', 'nam_cc']]\n",
    "\n",
    "df_phase4 = df_interpolated[['solar_zenith', 'CSI_ghi','time_gap_hours',\n",
    "       'time_gap_norm', 'day_boundary_flag', 'hour_progression',\n",
    "       'absolute_hour',\n",
    "       'season_flag', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos', 'B_CSI_ghi_8h', 'V_CSI_ghi_8h', 'L_CSI_ghi_8h', 'B_CSI_ghi_9h',\n",
    "       'V_CSI_ghi_9h', 'L_CSI_ghi_9h', 'B_CSI_ghi_10h', 'V_CSI_ghi_10h',\n",
    "       'L_CSI_ghi_10h', 'B_CSI_ghi_11h', 'V_CSI_ghi_11h', 'L_CSI_ghi_11h',\n",
    "       'B_CSI_ghi_12h', 'V_CSI_ghi_12h', 'L_CSI_ghi_12h', 'B_CSI_ghi_13h',\n",
    "       'V_CSI_ghi_13h', 'L_CSI_ghi_13h', 'B_CSI_ghi_14h', 'V_CSI_ghi_14h',\n",
    "       'L_CSI_ghi_14h', 'B_CSI_ghi_15h', 'V_CSI_ghi_15h', 'L_CSI_ghi_15h',\n",
    "       'B_CSI_ghi_16h', 'V_CSI_ghi_16h', 'L_CSI_ghi_16h', 'B_CSI_ghi_17h',\n",
    "       'V_CSI_ghi_17h', 'L_CSI_ghi_17h', 'B_CSI_ghi_18h', 'V_CSI_ghi_18h',\n",
    "       'L_CSI_ghi_18h', 'B_CSI_ghi_19h', 'V_CSI_ghi_19h', 'L_CSI_ghi_19h',\n",
    "       'nam_ghi', 'nam_cc', '80_dwsw', 'valtime', '80_cloud_cover', '56_dwsw', '56_cloud_cover',\n",
    "       '20_dwsw', '20_cloud_cover', '88_dwsw', '88_cloud_cover']]\n",
    "\n",
    "df_phase5 = df_interpolated[['solar_zenith', 'CSI_ghi','time_gap_hours',\n",
    "       'time_gap_norm', 'day_boundary_flag', 'hour_progression',\n",
    "       'absolute_hour',\n",
    "       'season_flag', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos', 'B_CSI_ghi_8h', 'V_CSI_ghi_8h', 'L_CSI_ghi_8h', 'B_CSI_ghi_9h',\n",
    "       'V_CSI_ghi_9h', 'L_CSI_ghi_9h', 'B_CSI_ghi_10h', 'V_CSI_ghi_10h',\n",
    "       'L_CSI_ghi_10h', 'B_CSI_ghi_11h', 'V_CSI_ghi_11h', 'L_CSI_ghi_11h',\n",
    "       'B_CSI_ghi_12h', 'V_CSI_ghi_12h', 'L_CSI_ghi_12h', 'B_CSI_ghi_13h',\n",
    "       'V_CSI_ghi_13h', 'L_CSI_ghi_13h', 'B_CSI_ghi_14h', 'V_CSI_ghi_14h',\n",
    "       'L_CSI_ghi_14h', 'B_CSI_ghi_15h', 'V_CSI_ghi_15h', 'L_CSI_ghi_15h',\n",
    "       'B_CSI_ghi_16h', 'V_CSI_ghi_16h', 'L_CSI_ghi_16h', 'B_CSI_ghi_17h',\n",
    "       'V_CSI_ghi_17h', 'L_CSI_ghi_17h', 'B_CSI_ghi_18h', 'V_CSI_ghi_18h',\n",
    "       'L_CSI_ghi_18h', 'B_CSI_ghi_19h', 'V_CSI_ghi_19h', 'L_CSI_ghi_19h',\n",
    "       'nam_ghi', 'nam_cc','80_cloud_cover',  '56_cloud_cover',\n",
    "        '20_cloud_cover', '88_cloud_cover']]\n",
    "\n",
    "df_phase6 = df_interpolated[['solar_zenith', 'CSI_ghi','time_gap_hours',\n",
    "       'time_gap_norm', 'day_boundary_flag', 'hour_progression',\n",
    "       'absolute_hour',\n",
    "       'season_flag', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos', 'B_CSI_ghi_8h', 'V_CSI_ghi_8h', 'L_CSI_ghi_8h', 'B_CSI_ghi_9h',\n",
    "       'V_CSI_ghi_9h', 'L_CSI_ghi_9h', 'B_CSI_ghi_10h', 'V_CSI_ghi_10h',\n",
    "       'L_CSI_ghi_10h', 'B_CSI_ghi_11h', 'V_CSI_ghi_11h', 'L_CSI_ghi_11h',\n",
    "       'B_CSI_ghi_12h', 'V_CSI_ghi_12h', 'L_CSI_ghi_12h', 'B_CSI_ghi_13h',\n",
    "       'V_CSI_ghi_13h', 'L_CSI_ghi_13h', 'B_CSI_ghi_14h', 'V_CSI_ghi_14h',\n",
    "       'L_CSI_ghi_14h', 'B_CSI_ghi_15h', 'V_CSI_ghi_15h', 'L_CSI_ghi_15h',\n",
    "       'B_CSI_ghi_16h', 'V_CSI_ghi_16h', 'L_CSI_ghi_16h', 'B_CSI_ghi_17h',\n",
    "       'V_CSI_ghi_17h', 'L_CSI_ghi_17h', 'B_CSI_ghi_18h', 'V_CSI_ghi_18h',\n",
    "       'L_CSI_ghi_18h', 'B_CSI_ghi_19h', 'V_CSI_ghi_19h', 'L_CSI_ghi_19h',\n",
    "       'nam_ghi', 'nam_cc', '80_dwsw', 'valtime', '80_cloud_cover', '56_dwsw', '56_cloud_cover',\n",
    "       '20_dwsw', '20_cloud_cover', '88_dwsw', '88_cloud_cover'\n",
    "      ]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "903f48e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['solar_zenith', 'CSI_ghi', 'time_gap_hours', 'time_gap_norm',\n",
       "       'day_boundary_flag', 'hour_progression', 'absolute_hour', 'season_flag',\n",
       "       'hour_sin', 'hour_cos', 'month_sin', 'month_cos'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_phase1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a8d1a6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.utils:Saved arrays to data/phas1_data_*.npy\n",
      "INFO:src.utils:X shape: (1055, 7, 11, 11), Y shape: (1055, 1, 11)\n",
      "INFO:src.pipeline:Loading data...\n",
      "INFO:src.utils:Loaded arrays from data/phas1_data_*.npy\n",
      "INFO:src.utils:X shape: (1055, 7, 11, 11), Y shape: (1055, 1, 11)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 2: Building model arrays (X, Y) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.pipeline:Building simple reference from existing columns (no pvlib, no regridding)...\n",
      "/home/muhammadhassan/App_v02/src/evaluation_utils.py:132: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  ref[\"nam_csi\"].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
      "INFO:src.pipeline:Reference dataframe prepared (11560 rows).\n",
      "INFO:src.utils:Loaded 35 folds from exp-004/exp-004rolling_origin_splits.json\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 1 ===\n",
      "INFO:src.pipeline:Train samples: 21, Val samples: 18\n",
      "INFO:src.pipeline:Model parameters: 182,667\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 72\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# This will now work after you apply the fixes below\u001b[39;00m\n\u001b[1;32m     71\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m SolarForecastingPipeline(LSTM_CONFIG)\n\u001b[0;32m---> 72\u001b[0m _, summary \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/App_v02/src/pipeline.py:309\u001b[0m, in \u001b[0;36mSolarForecastingPipeline.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    306\u001b[0m         val_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(train_size, train_size \u001b[38;5;241m+\u001b[39m val_size)\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;66;03m# Run fold\u001b[39;00m\n\u001b[0;32m--> 309\u001b[0m     fold_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_fold\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfold_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreference_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreference_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m     fold_results\u001b[38;5;241m.\u001b[39mappend(fold_result)\n\u001b[1;32m    320\u001b[0m \u001b[38;5;66;03m# Summarize results\u001b[39;00m\n",
      "File \u001b[0;32m~/App_v02/src/pipeline.py:146\u001b[0m, in \u001b[0;36mSolarForecastingPipeline.run_fold\u001b[0;34m(self, X, Y, train_idx, val_idx, fold_id, labels_index, reference_df)\u001b[0m\n\u001b[1;32m    144\u001b[0m loss_function_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_function\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMSE\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m--> 146\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_epochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlearning_rate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_function_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_patience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mearly_stopping_patience\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[1;32m    158\u001b[0m scaler_info \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_scaler\u001b[39m\u001b[38;5;124m'\u001b[39m: feature_scaler,\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_scaler\u001b[39m\u001b[38;5;124m'\u001b[39m: target_scaler\n\u001b[1;32m    161\u001b[0m }\n",
      "File \u001b[0;32m~/App_v02/src/engine.py:82\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, num_epochs, learning_rate, loss_function, device, early_stopping_patience)\u001b[0m\n\u001b[1;32m     79\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     81\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m---> 82\u001b[0m     train_mae \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mmean_absolute_error\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m     train_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Validation\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:218\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    214\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    215\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    216\u001b[0m         )\n\u001b[1;32m    217\u001b[0m     ):\n\u001b[0;32m--> 218\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    224\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    227\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    228\u001b[0m     )\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/sklearn/metrics/_regression.py:284\u001b[0m, in \u001b[0;36mmean_absolute_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Mean absolute error regression loss.\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \n\u001b[1;32m    230\u001b[0m \u001b[38;5;124;03mThe mean absolute error is a non-negative floating point value, where best value\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;124;03m0.85...\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    281\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(y_true, y_pred, sample_weight, multioutput)\n\u001b[1;32m    283\u001b[0m _, y_true, y_pred, sample_weight, multioutput \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 284\u001b[0m     \u001b[43m_check_reg_targets_with_floating_dtype\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m )\n\u001b[1;32m    289\u001b[0m output_errors \u001b[38;5;241m=\u001b[39m _average(\n\u001b[1;32m    290\u001b[0m     xp\u001b[38;5;241m.\u001b[39mabs(y_pred \u001b[38;5;241m-\u001b[39m y_true), weights\u001b[38;5;241m=\u001b[39msample_weight, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, xp\u001b[38;5;241m=\u001b[39mxp\n\u001b[1;32m    291\u001b[0m )\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(multioutput, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/sklearn/metrics/_regression.py:209\u001b[0m, in \u001b[0;36m_check_reg_targets_with_floating_dtype\u001b[0;34m(y_true, y_pred, sample_weight, multioutput, xp)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Ensures y_true, y_pred, and sample_weight correspond to same regression task.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03mExtends `_check_reg_targets` by automatically selecting a suitable floating-point\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03m    correct keyword.\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    207\u001b[0m dtype_name \u001b[38;5;241m=\u001b[39m _find_matching_floating_dtype(y_true, y_pred, sample_weight, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[0;32m--> 209\u001b[0m y_type, y_true, y_pred, sample_weight, multioutput \u001b[38;5;241m=\u001b[39m \u001b[43m_check_reg_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y_type, y_true, y_pred, sample_weight, multioutput\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/sklearn/metrics/_regression.py:115\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, sample_weight, multioutput, dtype, xp)\u001b[0m\n\u001b[1;32m    112\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(y_true, y_pred, multioutput, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[1;32m    114\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m--> 115\u001b[0m y_true \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m check_array(y_pred, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py:1105\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1099\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1100\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m while dim <= 2 is required\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1102\u001b[0m     )\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[0;32m-> 1105\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1107\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[1;32m   1113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[1;32m   1114\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py:120\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py:169\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m     )\n\u001b[0;32m--> 169\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN."
     ]
    }
   ],
   "source": [
    "from src.preprocessing import build_model_arrays, to_fixedgrid_multiindex\n",
    "import pandas as pd\n",
    "from src.utils import DataManager\n",
    "from src.pipeline import SolarForecastingPipeline\n",
    "\n",
    "# --- Block 1: Define variables ---\n",
    "\n",
    "TARGET_COL = \"CSI_ghi\"  # <-- FIX 1: Define Target FIRST\n",
    "history_days = 7\n",
    "horizon_days = 1\n",
    "\n",
    "# This list now correctly excludes the target\n",
    "feature_cols = [c for c in df_phase1.columns.tolist() if c != TARGET_COL]\n",
    "\n",
    "# --- Block 2: Build model arrays (Your code was correct here) ---\n",
    "\n",
    "\n",
    "print(\"\\n--- Step 2: Building model arrays (X, Y) ---\")\n",
    "fixed_df = to_fixedgrid_multiindex(df_phase1, timestamp_col=\"measurement_time\", expected_T=None)  # or set T\n",
    "    \n",
    "# You correctly passed the filtered 'feature_cols' here, fixing the data leak!\n",
    "ph1_X, ph1_Y, ph1_labels_list = build_model_arrays(\n",
    "        fixed_df,\n",
    "        feature_cols=feature_cols,  \n",
    "        target_col=TARGET_COL,\n",
    "        history_days=history_days,\n",
    "        horizon_days=horizon_days,\n",
    "    )\n",
    "\n",
    "# --- Block 3: Save arrays ---\n",
    "data_manager = DataManager()\n",
    "data_manager.save_arrays(\n",
    "    ph1_X, ph1_Y,\n",
    "    pd.DataFrame(index=pd.to_datetime(ph1_labels_list, utc=True)),\n",
    "    filename_prefix='phas1_data',  # <-- You are saving as 'phas2_data'\n",
    "    feature_cols=feature_cols,\n",
    "    target_col=TARGET_COL,\n",
    "    metadata={\n",
    "        \"input_csv\": \"data/processed/df_1h_lag_BLV_spatial_images.csv\",\n",
    "        \"timestamp_col\": \"measurement_time\",\n",
    "        \"feature_set\": feature_cols,\n",
    "        \"history_days\": 7,\n",
    "        \"horizon_days\": 1,\n",
    "    }\n",
    ")\n",
    "\n",
    "# --- Block 4: Configure and Run ---\n",
    "LSTM_CONFIG = {\n",
    "    \"experiment_name\": \"UniLSTM_p1_exp01_essentialF\",\n",
    "    \"model_type\": \"LSTM\",\n",
    "    \"model_config\": {\n",
    "        \"hidden_size\": 64,\n",
    "        \"num_layers\": 2,\n",
    "        \"dropout\": 0.35,\n",
    "        \"bidirectional\": True,\n",
    "    },\n",
    "    \"data_prefix\": \"phas1_data\",  # <-- FIX 2: Match the filename_prefix\n",
    "    \"splits_file\": \"exp-004/exp-004rolling_origin_splits.json\",\n",
    "    \"feature_cols\": feature_cols,\n",
    "    \"feature_selection\": feature_cols, # <-- This is correct!\n",
    "    \"target_col\": TARGET_COL,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_epochs\": 50,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"loss_function\": \"Huber\",  # <-- This setting needs the fix below\n",
    "    \"early_stopping_patience\": 20,\n",
    "    \"max_folds\": 35,\n",
    "}\n",
    "\n",
    "# This will now work after you apply the fixes below\n",
    "pipeline = SolarForecastingPipeline(LSTM_CONFIG)\n",
    "_, summary = pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f26ed5",
   "metadata": {},
   "source": [
    "The model has learned the easiest possible trick: it's predicting that the value at the next time step will be the same as the value at the current time step.\n",
    "\n",
    "It is a Naive (Persistence Model)\n",
    "\n",
    "\n",
    "\"Lazy learning\" (the effect) is a model's symptom. \"Incorrect sequencing\" (the cause) is the disease. Investigation needed.\n",
    "    - Solution1: Adding time_gap_hours',\n",
    "       'time_gap_norm', 'day_boundary_flag', 'hour_progression',\n",
    "       'absolute_hour', to the model, as well the cyclical time features problem is still. \n",
    "\n",
    "    - Solution 2 adding more statistical features about historical Lag like 'B_CSI_ghi_8h', 'V_CSI_ghi_8h', 'L_CSI_ghi_8h', 'B_CSI_ghi_9h',\n",
    "       'V_CSI_ghi_9h', 'L_CSI_ghi_9h', 'B_CSI_ghi_10h', 'V_CSI_ghi_10h',\n",
    "       'L_CSI_ghi_10h', 'B_CSI_ghi_11h', 'V_CSI_ghi_11h', 'L_CSI_ghi_11h',\n",
    "       'B_CSI_ghi_12h', 'V_CSI_ghi_12h', 'L_CSI_ghi_12h', 'B_CSI_ghi_13h',\n",
    "       'V_CSI_ghi_13h', 'L_CSI_ghi_13h', 'B_CSI_ghi_14h', 'V_CSI_ghi_14h',\n",
    "       'L_CSI_ghi_14h', 'B_CSI_ghi_15h', 'V_CSI_ghi_15h', 'L_CSI_ghi_15h',\n",
    "       'B_CSI_ghi_16h', 'V_CSI_ghi_16h', 'L_CSI_ghi_16h', 'B_CSI_ghi_17h',\n",
    "       'V_CSI_ghi_17h', 'L_CSI_ghi_17h', 'B_CSI_ghi_18h', 'V_CSI_ghi_18h',\n",
    "       'L_CSI_ghi_18h', 'B_CSI_ghi_19h', 'V_CSI_ghi_19h', 'L_CSI_ghi_19h'\n",
    "    - Problem is still so i will move to the other \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a4d784",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9572a8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.utils:Saved arrays to data/phas2_data_*.npy\n",
      "INFO:src.utils:X shape: (1055, 7, 11, 47), Y shape: (1055, 1, 11)\n",
      "INFO:src.pipeline:Loading data...\n",
      "INFO:src.utils:Loaded arrays from data/phas2_data_*.npy\n",
      "INFO:src.utils:X shape: (1055, 7, 11, 47), Y shape: (1055, 1, 11)\n",
      "INFO:src.pipeline:Building FIXED-GRID reference frame (K_bins is None)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 2: Building model arrays (X, Y) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:src.pipeline:Failed to construct reference dataframe for NAM comparison: 'target_time'\n",
      "INFO:src.utils:Loaded 35 folds from exp-004/exp-004rolling_origin_splits.json\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 1 ===\n",
      "INFO:src.pipeline:Train samples: 21, Val samples: 18\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.069708, Val Loss: 0.099907, Train MAE: 0.282255, Val MAE: 0.374032, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.054642, Val Loss: 0.101493, Train MAE: 0.239465, Val MAE: 0.379981, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.054945, Val Loss: 0.100272, Train MAE: 0.248461, Val MAE: 0.386173, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 31\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 1.\n",
      "INFO:src.pipeline:Fold 1 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.200547\n",
      "INFO:src.pipeline:  rmse: 0.447825\n",
      "INFO:src.pipeline:  mae: 0.386084\n",
      "INFO:src.pipeline:  r2: 0.083255\n",
      "INFO:src.pipeline:  mape: 111.452530\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 2 ===\n",
      "INFO:src.pipeline:Train samples: 40, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.071297, Val Loss: 0.060338, Train MAE: 0.329235, Val MAE: 0.309224, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.060832, Val Loss: 0.058418, Train MAE: 0.297003, Val MAE: 0.305215, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.057110, Val Loss: 0.053860, Train MAE: 0.281203, Val MAE: 0.284067, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.035655, Val Loss: 0.072416, Train MAE: 0.203179, Val MAE: 0.348237, LR: 0.001000\n",
      "INFO:src.engine:Epoch [50/50] - Train Loss: 0.046234, Val Loss: 0.142530, Train MAE: 0.210655, Val MAE: 0.475623, LR: 0.000500\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 2.\n",
      "INFO:src.pipeline:Fold 2 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.285060\n",
      "INFO:src.pipeline:  rmse: 0.533910\n",
      "INFO:src.pipeline:  mae: 0.475623\n",
      "INFO:src.pipeline:  r2: -1.412765\n",
      "INFO:src.pipeline:  mape: 63.425835\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 3 ===\n",
      "INFO:src.pipeline:Train samples: 71, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.066946, Val Loss: 0.031758, Train MAE: 0.303205, Val MAE: 0.217697, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.050030, Val Loss: 0.029642, Train MAE: 0.258705, Val MAE: 0.201703, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.039096, Val Loss: 0.027947, Train MAE: 0.212193, Val MAE: 0.187695, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.036369, Val Loss: 0.028375, Train MAE: 0.206798, Val MAE: 0.192656, LR: 0.000500\n",
      "INFO:src.engine:Epoch [50/50] - Train Loss: 0.037343, Val Loss: 0.038362, Train MAE: 0.216984, Val MAE: 0.239442, LR: 0.000500\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 3.\n",
      "INFO:src.pipeline:Fold 3 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.076724\n",
      "INFO:src.pipeline:  rmse: 0.276992\n",
      "INFO:src.pipeline:  mae: 0.239442\n",
      "INFO:src.pipeline:  r2: 0.075373\n",
      "INFO:src.pipeline:  mape: 36.028419\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 4 ===\n",
      "INFO:src.pipeline:Train samples: 101, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.060483, Val Loss: 0.020286, Train MAE: 0.279674, Val MAE: 0.186462, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.053720, Val Loss: 0.016987, Train MAE: 0.257586, Val MAE: 0.164015, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.038307, Val Loss: 0.014079, Train MAE: 0.210791, Val MAE: 0.127659, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.037617, Val Loss: 0.015268, Train MAE: 0.206786, Val MAE: 0.110029, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 46\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 4.\n",
      "INFO:src.pipeline:Fold 4 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.039052\n",
      "INFO:src.pipeline:  rmse: 0.197617\n",
      "INFO:src.pipeline:  mae: 0.130020\n",
      "INFO:src.pipeline:  r2: 0.162173\n",
      "INFO:src.pipeline:  mape: 18.426487\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 5 ===\n",
      "INFO:src.pipeline:Train samples: 132, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.040414, Val Loss: 0.012189, Train MAE: 0.216376, Val MAE: 0.133552, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.033239, Val Loss: 0.012342, Train MAE: 0.193066, Val MAE: 0.138385, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.028102, Val Loss: 0.018214, Train MAE: 0.175395, Val MAE: 0.169374, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 38\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 5.\n",
      "INFO:src.pipeline:Fold 5 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.046919\n",
      "INFO:src.pipeline:  rmse: 0.216608\n",
      "INFO:src.pipeline:  mae: 0.191900\n",
      "INFO:src.pipeline:  r2: -0.757909\n",
      "INFO:src.pipeline:  mape: 20.081072\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 6 ===\n",
      "INFO:src.pipeline:Train samples: 162, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.035106, Val Loss: 0.012256, Train MAE: 0.182668, Val MAE: 0.089284, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.030579, Val Loss: 0.012442, Train MAE: 0.176014, Val MAE: 0.083532, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 24\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 6.\n",
      "INFO:src.pipeline:Fold 6 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.025981\n",
      "INFO:src.pipeline:  rmse: 0.161185\n",
      "INFO:src.pipeline:  mae: 0.083808\n",
      "INFO:src.pipeline:  r2: 0.348323\n",
      "INFO:src.pipeline:  mape: 15.781154\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 7 ===\n",
      "INFO:src.pipeline:Train samples: 193, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.047292, Val Loss: 0.021580, Train MAE: 0.216231, Val MAE: 0.118821, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.023386, Val Loss: 0.022284, Train MAE: 0.144005, Val MAE: 0.122197, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 24\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 7.\n",
      "INFO:src.pipeline:Fold 7 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.044398\n",
      "INFO:src.pipeline:  rmse: 0.210709\n",
      "INFO:src.pipeline:  mae: 0.109533\n",
      "INFO:src.pipeline:  r2: 0.374838\n",
      "INFO:src.pipeline:  mape: 32.507805\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 8 ===\n",
      "INFO:src.pipeline:Train samples: 224, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.029068, Val Loss: 0.025458, Train MAE: 0.166871, Val MAE: 0.127317, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.024175, Val Loss: 0.027832, Train MAE: 0.143792, Val MAE: 0.147388, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 27\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 8.\n",
      "INFO:src.pipeline:Fold 8 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.055535\n",
      "INFO:src.pipeline:  rmse: 0.235658\n",
      "INFO:src.pipeline:  mae: 0.150315\n",
      "INFO:src.pipeline:  r2: 0.315218\n",
      "INFO:src.pipeline:  mape: 31.190931\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 9 ===\n",
      "INFO:src.pipeline:Train samples: 254, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.026953, Val Loss: 0.029423, Train MAE: 0.158668, Val MAE: 0.157048, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.023769, Val Loss: 0.026277, Train MAE: 0.144941, Val MAE: 0.168545, LR: 0.000500\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.021863, Val Loss: 0.027338, Train MAE: 0.139020, Val MAE: 0.176919, LR: 0.000250\n",
      "INFO:src.engine:Early stopping at epoch 35\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 9.\n",
      "INFO:src.pipeline:Fold 9 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.060271\n",
      "INFO:src.pipeline:  rmse: 0.245502\n",
      "INFO:src.pipeline:  mae: 0.187911\n",
      "INFO:src.pipeline:  r2: 0.535430\n",
      "INFO:src.pipeline:  mape: 21.007057\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 10 ===\n",
      "INFO:src.pipeline:Train samples: 285, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.027996, Val Loss: 0.043053, Train MAE: 0.160688, Val MAE: 0.223395, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.023068, Val Loss: 0.045004, Train MAE: 0.137409, Val MAE: 0.215032, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 28\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 10.\n",
      "INFO:src.pipeline:Fold 10 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.088650\n",
      "INFO:src.pipeline:  rmse: 0.297742\n",
      "INFO:src.pipeline:  mae: 0.231621\n",
      "INFO:src.pipeline:  r2: 0.506362\n",
      "INFO:src.pipeline:  mape: 62.391060\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 11 ===\n",
      "INFO:src.pipeline:Train samples: 315, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.028458, Val Loss: 0.059267, Train MAE: 0.161197, Val MAE: 0.283681, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.024290, Val Loss: 0.054534, Train MAE: 0.149296, Val MAE: 0.276157, LR: 0.001000\n",
      "INFO:src.engine:Early stopping at epoch 29\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 11.\n",
      "INFO:src.pipeline:Fold 11 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.127757\n",
      "INFO:src.pipeline:  rmse: 0.357431\n",
      "INFO:src.pipeline:  mae: 0.283234\n",
      "INFO:src.pipeline:  r2: 0.379513\n",
      "INFO:src.pipeline:  mape: 76.641579\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 12 ===\n",
      "INFO:src.pipeline:Train samples: 346, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.031632, Val Loss: 0.036493, Train MAE: 0.174705, Val MAE: 0.234742, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.026559, Val Loss: 0.032512, Train MAE: 0.157724, Val MAE: 0.206402, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.024062, Val Loss: 0.035140, Train MAE: 0.152777, Val MAE: 0.217228, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.021298, Val Loss: 0.038435, Train MAE: 0.140071, Val MAE: 0.204533, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 40\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 12.\n",
      "INFO:src.pipeline:Fold 12 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.076870\n",
      "INFO:src.pipeline:  rmse: 0.277254\n",
      "INFO:src.pipeline:  mae: 0.204533\n",
      "INFO:src.pipeline:  r2: 0.610223\n",
      "INFO:src.pipeline:  mape: 47.403477\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 13 ===\n",
      "INFO:src.pipeline:Train samples: 377, Val samples: 26\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.033713, Val Loss: 0.052261, Train MAE: 0.188196, Val MAE: 0.247929, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.026715, Val Loss: 0.040148, Train MAE: 0.162534, Val MAE: 0.222808, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.025226, Val Loss: 0.042282, Train MAE: 0.159336, Val MAE: 0.203717, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.021183, Val Loss: 0.040458, Train MAE: 0.140921, Val MAE: 0.206910, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 43\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 13.\n",
      "INFO:src.pipeline:Fold 13 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.082360\n",
      "INFO:src.pipeline:  rmse: 0.286984\n",
      "INFO:src.pipeline:  mae: 0.202866\n",
      "INFO:src.pipeline:  r2: 0.579078\n",
      "INFO:src.pipeline:  mape: 71.825966\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 14 ===\n",
      "INFO:src.pipeline:Train samples: 405, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.033351, Val Loss: 0.033548, Train MAE: 0.187002, Val MAE: 0.189299, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.028748, Val Loss: 0.044076, Train MAE: 0.169841, Val MAE: 0.219650, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 21\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 14.\n",
      "INFO:src.pipeline:Fold 14 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.083442\n",
      "INFO:src.pipeline:  rmse: 0.288863\n",
      "INFO:src.pipeline:  mae: 0.205615\n",
      "INFO:src.pipeline:  r2: 0.015472\n",
      "INFO:src.pipeline:  mape: 27.462873\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 15 ===\n",
      "INFO:src.pipeline:Train samples: 436, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.032753, Val Loss: 0.022789, Train MAE: 0.183818, Val MAE: 0.153451, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.025641, Val Loss: 0.022630, Train MAE: 0.159034, Val MAE: 0.141895, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 26\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 15.\n",
      "INFO:src.pipeline:Fold 15 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.046192\n",
      "INFO:src.pipeline:  rmse: 0.214923\n",
      "INFO:src.pipeline:  mae: 0.143499\n",
      "INFO:src.pipeline:  r2: 0.451462\n",
      "INFO:src.pipeline:  mape: 37.901726\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 16 ===\n",
      "INFO:src.pipeline:Train samples: 466, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.033065, Val Loss: 0.017829, Train MAE: 0.185445, Val MAE: 0.143260, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.029379, Val Loss: 0.018719, Train MAE: 0.173973, Val MAE: 0.119846, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.024462, Val Loss: 0.018884, Train MAE: 0.155659, Val MAE: 0.131763, LR: 0.000500\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.021107, Val Loss: 0.018524, Train MAE: 0.145147, Val MAE: 0.121873, LR: 0.000250\n",
      "INFO:src.engine:Early stopping at epoch 46\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 16.\n",
      "INFO:src.pipeline:Fold 16 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.038067\n",
      "INFO:src.pipeline:  rmse: 0.195108\n",
      "INFO:src.pipeline:  mae: 0.123596\n",
      "INFO:src.pipeline:  r2: 0.319556\n",
      "INFO:src.pipeline:  mape: 42.778938\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 17 ===\n",
      "INFO:src.pipeline:Train samples: 497, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.032051, Val Loss: 0.016688, Train MAE: 0.183026, Val MAE: 0.119073, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.026796, Val Loss: 0.016329, Train MAE: 0.161395, Val MAE: 0.113219, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.023147, Val Loss: 0.015676, Train MAE: 0.150882, Val MAE: 0.114732, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.020238, Val Loss: 0.017061, Train MAE: 0.141801, Val MAE: 0.147692, LR: 0.001000\n",
      "INFO:src.engine:Epoch [50/50] - Train Loss: 0.019224, Val Loss: 0.022044, Train MAE: 0.133959, Val MAE: 0.185099, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 50\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 17.\n",
      "INFO:src.pipeline:Fold 17 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.044087\n",
      "INFO:src.pipeline:  rmse: 0.209970\n",
      "INFO:src.pipeline:  mae: 0.185099\n",
      "INFO:src.pipeline:  r2: 0.248843\n",
      "INFO:src.pipeline:  mape: 27.380783\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 18 ===\n",
      "INFO:src.pipeline:Train samples: 527, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.030642, Val Loss: 0.015118, Train MAE: 0.175178, Val MAE: 0.113069, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.027484, Val Loss: 0.015010, Train MAE: 0.166736, Val MAE: 0.092975, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.024977, Val Loss: 0.014785, Train MAE: 0.153384, Val MAE: 0.106731, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.018870, Val Loss: 0.015703, Train MAE: 0.132451, Val MAE: 0.120641, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 47\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 18.\n",
      "INFO:src.pipeline:Fold 18 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.030888\n",
      "INFO:src.pipeline:  rmse: 0.175751\n",
      "INFO:src.pipeline:  mae: 0.101933\n",
      "INFO:src.pipeline:  r2: 0.285650\n",
      "INFO:src.pipeline:  mape: 19.591917\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 19 ===\n",
      "INFO:src.pipeline:Train samples: 558, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.028810, Val Loss: 0.025356, Train MAE: 0.170262, Val MAE: 0.127450, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.023884, Val Loss: 0.025723, Train MAE: 0.153779, Val MAE: 0.124933, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 22\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 19.\n",
      "INFO:src.pipeline:Fold 19 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.051770\n",
      "INFO:src.pipeline:  rmse: 0.227531\n",
      "INFO:src.pipeline:  mae: 0.128492\n",
      "INFO:src.pipeline:  r2: 0.343632\n",
      "INFO:src.pipeline:  mape: 13.546510\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 20 ===\n",
      "INFO:src.pipeline:Train samples: 589, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.029679, Val Loss: 0.019289, Train MAE: 0.170801, Val MAE: 0.128000, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.026448, Val Loss: 0.022518, Train MAE: 0.158644, Val MAE: 0.152143, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 28\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 20.\n",
      "INFO:src.pipeline:Fold 20 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.047519\n",
      "INFO:src.pipeline:  rmse: 0.217989\n",
      "INFO:src.pipeline:  mae: 0.140707\n",
      "INFO:src.pipeline:  r2: 0.404164\n",
      "INFO:src.pipeline:  mape: 21.565231\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 21 ===\n",
      "INFO:src.pipeline:Train samples: 619, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.028063, Val Loss: 0.016777, Train MAE: 0.166865, Val MAE: 0.144713, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.025840, Val Loss: 0.018147, Train MAE: 0.157901, Val MAE: 0.147700, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.023751, Val Loss: 0.023259, Train MAE: 0.146647, Val MAE: 0.169856, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 32\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 21.\n",
      "INFO:src.pipeline:Fold 21 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.042274\n",
      "INFO:src.pipeline:  rmse: 0.205607\n",
      "INFO:src.pipeline:  mae: 0.151696\n",
      "INFO:src.pipeline:  r2: 0.668827\n",
      "INFO:src.pipeline:  mape: 19.009689\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 22 ===\n",
      "INFO:src.pipeline:Train samples: 650, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.030323, Val Loss: 0.039374, Train MAE: 0.171777, Val MAE: 0.190860, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.026058, Val Loss: 0.036152, Train MAE: 0.159686, Val MAE: 0.180274, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.022740, Val Loss: 0.036187, Train MAE: 0.147360, Val MAE: 0.162720, LR: 0.001000\n",
      "INFO:src.engine:Early stopping at epoch 39\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 22.\n",
      "INFO:src.pipeline:Fold 22 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.074200\n",
      "INFO:src.pipeline:  rmse: 0.272397\n",
      "INFO:src.pipeline:  mae: 0.153505\n",
      "INFO:src.pipeline:  r2: 0.543201\n",
      "INFO:src.pipeline:  mape: 54.145229\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 23 ===\n",
      "INFO:src.pipeline:Train samples: 680, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.029602, Val Loss: 0.038943, Train MAE: 0.169441, Val MAE: 0.207298, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.024076, Val Loss: 0.039555, Train MAE: 0.153239, Val MAE: 0.215409, LR: 0.000500\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.022878, Val Loss: 0.043065, Train MAE: 0.144620, Val MAE: 0.225480, LR: 0.000500\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.019319, Val Loss: 0.035660, Train MAE: 0.134846, Val MAE: 0.200742, LR: 0.000250\n",
      "INFO:src.engine:Epoch [50/50] - Train Loss: 0.018620, Val Loss: 0.038225, Train MAE: 0.131635, Val MAE: 0.206380, LR: 0.000250\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 23.\n",
      "INFO:src.pipeline:Fold 23 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.076451\n",
      "INFO:src.pipeline:  rmse: 0.276498\n",
      "INFO:src.pipeline:  mae: 0.206380\n",
      "INFO:src.pipeline:  r2: 0.612695\n",
      "INFO:src.pipeline:  mape: 60.132450\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 24 ===\n",
      "INFO:src.pipeline:Train samples: 711, Val samples: 21\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.029184, Val Loss: 0.059321, Train MAE: 0.171558, Val MAE: 0.258159, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.025504, Val Loss: 0.068156, Train MAE: 0.156088, Val MAE: 0.275530, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.023049, Val Loss: 0.073914, Train MAE: 0.147411, Val MAE: 0.287933, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 30\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 24.\n",
      "INFO:src.pipeline:Fold 24 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.148162\n",
      "INFO:src.pipeline:  rmse: 0.384918\n",
      "INFO:src.pipeline:  mae: 0.287933\n",
      "INFO:src.pipeline:  r2: 0.291115\n",
      "INFO:src.pipeline:  mape: 116.461517\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 25 ===\n",
      "INFO:src.pipeline:Train samples: 734, Val samples: 23\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.030355, Val Loss: 0.056458, Train MAE: 0.178783, Val MAE: 0.223382, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.026950, Val Loss: 0.059195, Train MAE: 0.161871, Val MAE: 0.228541, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 22\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 25.\n",
      "INFO:src.pipeline:Fold 25 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.120882\n",
      "INFO:src.pipeline:  rmse: 0.347681\n",
      "INFO:src.pipeline:  mae: 0.242033\n",
      "INFO:src.pipeline:  r2: 0.399151\n",
      "INFO:src.pipeline:  mape: 22.916613\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 26 ===\n",
      "INFO:src.pipeline:Train samples: 759, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.031259, Val Loss: 0.047343, Train MAE: 0.178107, Val MAE: 0.228298, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.027730, Val Loss: 0.054746, Train MAE: 0.162503, Val MAE: 0.227166, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.024748, Val Loss: 0.071817, Train MAE: 0.151450, Val MAE: 0.250043, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 30\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 26.\n",
      "INFO:src.pipeline:Fold 26 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.143860\n",
      "INFO:src.pipeline:  rmse: 0.379289\n",
      "INFO:src.pipeline:  mae: 0.250043\n",
      "INFO:src.pipeline:  r2: 0.013170\n",
      "INFO:src.pipeline:  mape: 106.275436\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 27 ===\n",
      "INFO:src.pipeline:Train samples: 790, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.032212, Val Loss: 0.034497, Train MAE: 0.179972, Val MAE: 0.156680, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.028789, Val Loss: 0.032400, Train MAE: 0.169032, Val MAE: 0.151099, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.025620, Val Loss: 0.033257, Train MAE: 0.156461, Val MAE: 0.158897, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 38\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 27.\n",
      "INFO:src.pipeline:Fold 27 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.070488\n",
      "INFO:src.pipeline:  rmse: 0.265495\n",
      "INFO:src.pipeline:  mae: 0.159138\n",
      "INFO:src.pipeline:  r2: 0.350060\n",
      "INFO:src.pipeline:  mape: 60.672714\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 28 ===\n",
      "INFO:src.pipeline:Train samples: 820, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.032517, Val Loss: 0.026590, Train MAE: 0.182505, Val MAE: 0.152828, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.030257, Val Loss: 0.026406, Train MAE: 0.172240, Val MAE: 0.167247, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.026451, Val Loss: 0.028435, Train MAE: 0.160754, Val MAE: 0.158159, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.022329, Val Loss: 0.031425, Train MAE: 0.144578, Val MAE: 0.152379, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 48\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 28.\n",
      "INFO:src.pipeline:Fold 28 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.065999\n",
      "INFO:src.pipeline:  rmse: 0.256903\n",
      "INFO:src.pipeline:  mae: 0.162492\n",
      "INFO:src.pipeline:  r2: 0.204466\n",
      "INFO:src.pipeline:  mape: 42.454018\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 29 ===\n",
      "INFO:src.pipeline:Train samples: 851, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.031448, Val Loss: 0.011600, Train MAE: 0.180403, Val MAE: 0.135821, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.027904, Val Loss: 0.014283, Train MAE: 0.167740, Val MAE: 0.145141, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 28\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 29.\n",
      "INFO:src.pipeline:Fold 29 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.022882\n",
      "INFO:src.pipeline:  rmse: 0.151269\n",
      "INFO:src.pipeline:  mae: 0.126509\n",
      "INFO:src.pipeline:  r2: 0.442710\n",
      "INFO:src.pipeline:  mape: 15.262622\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 30 ===\n",
      "INFO:src.pipeline:Train samples: 881, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.030212, Val Loss: 0.021741, Train MAE: 0.174979, Val MAE: 0.109310, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.026775, Val Loss: 0.019812, Train MAE: 0.163410, Val MAE: 0.091064, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.025486, Val Loss: 0.019844, Train MAE: 0.154771, Val MAE: 0.080703, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.023286, Val Loss: 0.019858, Train MAE: 0.149189, Val MAE: 0.079436, LR: 0.001000\n",
      "INFO:src.engine:Epoch [50/50] - Train Loss: 0.021812, Val Loss: 0.019857, Train MAE: 0.141233, Val MAE: 0.076493, LR: 0.001000\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 30.\n",
      "INFO:src.pipeline:Fold 30 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.040011\n",
      "INFO:src.pipeline:  rmse: 0.200029\n",
      "INFO:src.pipeline:  mae: 0.076493\n",
      "INFO:src.pipeline:  r2: 0.303424\n",
      "INFO:src.pipeline:  mape: 4.290060\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 31 ===\n",
      "INFO:src.pipeline:Train samples: 911, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.030223, Val Loss: 0.004239, Train MAE: 0.172812, Val MAE: 0.047787, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.027424, Val Loss: 0.006397, Train MAE: 0.161207, Val MAE: 0.084725, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.023318, Val Loss: 0.004449, Train MAE: 0.147149, Val MAE: 0.058556, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 38\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 31.\n",
      "INFO:src.pipeline:Fold 31 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.010317\n",
      "INFO:src.pipeline:  rmse: 0.101575\n",
      "INFO:src.pipeline:  mae: 0.072256\n",
      "INFO:src.pipeline:  r2: 0.734357\n",
      "INFO:src.pipeline:  mape: 9.135936\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 32 ===\n",
      "INFO:src.pipeline:Train samples: 942, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.029568, Val Loss: 0.016371, Train MAE: 0.169942, Val MAE: 0.139858, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.027192, Val Loss: 0.010563, Train MAE: 0.159530, Val MAE: 0.060020, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.025255, Val Loss: 0.014457, Train MAE: 0.152642, Val MAE: 0.099677, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.020575, Val Loss: 0.016036, Train MAE: 0.134416, Val MAE: 0.099288, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 44\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 32.\n",
      "INFO:src.pipeline:Fold 32 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.032984\n",
      "INFO:src.pipeline:  rmse: 0.181614\n",
      "INFO:src.pipeline:  mae: 0.109172\n",
      "INFO:src.pipeline:  r2: 0.439807\n",
      "INFO:src.pipeline:  mape: 13.453928\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 33 ===\n",
      "INFO:src.pipeline:Train samples: 972, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.029277, Val Loss: 0.051016, Train MAE: 0.166797, Val MAE: 0.231493, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.027548, Val Loss: 0.063681, Train MAE: 0.160065, Val MAE: 0.219237, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.023772, Val Loss: 0.065148, Train MAE: 0.144731, Val MAE: 0.230129, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 31\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 33.\n",
      "INFO:src.pipeline:Fold 33 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.143002\n",
      "INFO:src.pipeline:  rmse: 0.378156\n",
      "INFO:src.pipeline:  mae: 0.236933\n",
      "INFO:src.pipeline:  r2: 0.234017\n",
      "INFO:src.pipeline:  mape: 120.922127\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 34 ===\n",
      "INFO:src.pipeline:Train samples: 1003, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.029629, Val Loss: 0.035004, Train MAE: 0.168593, Val MAE: 0.197775, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.025972, Val Loss: 0.035033, Train MAE: 0.153805, Val MAE: 0.188470, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.023952, Val Loss: 0.037328, Train MAE: 0.145870, Val MAE: 0.209493, LR: 0.000500\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.022434, Val Loss: 0.037220, Train MAE: 0.141654, Val MAE: 0.194459, LR: 0.000500\n",
      "INFO:src.engine:Epoch [50/50] - Train Loss: 0.019390, Val Loss: 0.044755, Train MAE: 0.130324, Val MAE: 0.216068, LR: 0.000250\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 34.\n",
      "INFO:src.pipeline:Fold 34 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.089777\n",
      "INFO:src.pipeline:  rmse: 0.299629\n",
      "INFO:src.pipeline:  mae: 0.216068\n",
      "INFO:src.pipeline:  r2: 0.483180\n",
      "INFO:src.pipeline:  mape: 52.878044\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 35 ===\n",
      "INFO:src.pipeline:Train samples: 1033, Val samples: 21\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.029735, Val Loss: 0.068609, Train MAE: 0.170992, Val MAE: 0.295495, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.027835, Val Loss: 0.059811, Train MAE: 0.160628, Val MAE: 0.274515, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.024753, Val Loss: 0.067594, Train MAE: 0.150997, Val MAE: 0.307640, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.023041, Val Loss: 0.078587, Train MAE: 0.141076, Val MAE: 0.319353, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 41\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 35.\n",
      "INFO:src.pipeline:Fold 35 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.145530\n",
      "INFO:src.pipeline:  rmse: 0.381483\n",
      "INFO:src.pipeline:  mae: 0.284352\n",
      "INFO:src.pipeline:  r2: 0.398958\n",
      "INFO:src.pipeline:  mape: 69.868317\n",
      "INFO:src.pipeline:\n",
      "=== Experiment Summary ===\n",
      "INFO:src.pipeline:Average Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.079397 ± 0.055498\n",
      "INFO:src.pipeline:  rmse: 0.267374 ± 0.088930\n",
      "INFO:src.pipeline:  mae: 0.189738 ± 0.083971\n",
      "INFO:src.pipeline:  r2: 0.285344 ± 0.386973\n",
      "INFO:src.pipeline:  mape: 44.750573 ± 31.478089\n"
     ]
    }
   ],
   "source": [
    "from src.preprocessing import KBinConfig, process_splits_to_kbins, build_model_arrays, to_fixedgrid_multiindex\n",
    "import pandas as pd\n",
    "from src.utils import DataManager\n",
    "from src.pipeline import SolarForecastingPipeline\n",
    "\n",
    "# --- Block 1: Define variables ---\n",
    "USE_KBINS = False\n",
    "k_bins = 60\n",
    "TARGET_COL = \"CSI_ghi\"  \n",
    "history_days = 7\n",
    "horizon_days = 1\n",
    "\n",
    "# This list now correctly excludes the target\n",
    "feature_cols = [c for c in df_phase2.columns.tolist() if c != TARGET_COL]\n",
    "\n",
    "# --- Block 2: Build model arrays (Your code was correct here) ---\n",
    "if USE_KBINS:\n",
    "        print(\"\\n--- Step 2: Converting full dataset to K-Bins format ---\")\n",
    "        # ... (rest of your K-Bins code) ...\n",
    "else:\n",
    "    print(\"\\n--- Step 2: Building model arrays (X, Y) ---\")\n",
    "    fixed_df = to_fixedgrid_multiindex(df_phase2, timestamp_col=\"measurement_time\", expected_T=None)  # or set T\n",
    "    \n",
    "    # You correctly passed the filtered 'feature_cols' here, fixing the data leak!\n",
    "    ph2_X, ph2_Y, ph2_labels_list = build_model_arrays(\n",
    "        fixed_df,\n",
    "        feature_cols=feature_cols,  # <-- This is correct!\n",
    "        target_col=TARGET_COL,\n",
    "        history_days=history_days,\n",
    "        horizon_days=horizon_days,\n",
    "    )\n",
    "\n",
    "# --- Block 3: Save arrays ---\n",
    "data_manager = DataManager()\n",
    "data_manager.save_arrays(\n",
    "    ph2_X, ph2_Y,\n",
    "    pd.DataFrame(index=pd.to_datetime(ph2_labels_list, utc=True)),\n",
    "    filename_prefix='phas2_data',  # <-- You are saving as 'phas2_data'\n",
    "    feature_cols=feature_cols,\n",
    "    target_col=TARGET_COL,\n",
    "    metadata={\n",
    "        \"input_csv\": \"data/processed/df_1h_lag_BLV_spatial_images.csv\",\n",
    "        \"timestamp_col\": \"measurement_time\",\n",
    "        \"feature_set\": feature_cols,\n",
    "        \"history_days\": 7,\n",
    "        \"horizon_days\": 1,\n",
    "        \"k_bins\": None,\n",
    "    }\n",
    ")\n",
    "\n",
    "# --- Block 4: Configure and Run ---\n",
    "LSTM_CONFIG = {\n",
    "    \"experiment_name\": \"UniLSTM_p2_exp01_BLVF\",\n",
    "    \"model_type\": \"LSTM\",\n",
    "    \"model_config\": {\n",
    "        \"hidden_size\": 64,\n",
    "        \"num_layers\": 2,\n",
    "        \"dropout\": 0.35,\n",
    "        \"bidirectional\": True,\n",
    "    },\n",
    "    \"data_prefix\": \"phas2_data\",  # <-- FIX 2: Match the filename_prefix\n",
    "    \"splits_file\": \"exp-004/exp-004rolling_origin_splits.json\",\n",
    "    \"feature_cols\": feature_cols,\n",
    "    \"feature_selection\": feature_cols, # <-- This is correct!\n",
    "    \"target_col\": TARGET_COL,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_epochs\": 50,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"loss_function\": \"Huber\",  # <-- This setting needs the fix below\n",
    "    \"early_stopping_patience\": 20,\n",
    "    \"max_folds\": 35,\n",
    "}\n",
    "\n",
    "# This will now work after you apply the fixes below\n",
    "pipeline = SolarForecastingPipeline(LSTM_CONFIG)\n",
    "_, summary = pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf41c240",
   "metadata": {},
   "source": [
    "Increasing the Hisory days "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb883c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocessing import KBinConfig, process_splits_to_kbins, build_model_arrays, to_fixedgrid_multiindex\n",
    "import pandas as pd\n",
    "from src.utils import DataManager\n",
    "from src.pipeline import SolarForecastingPipeline\n",
    "\n",
    "# --- Block 1: Define variables ---\n",
    "USE_KBINS = False\n",
    "k_bins = 60\n",
    "TARGET_COL = \"CSI_ghi\"  \n",
    "history_days = 7\n",
    "horizon_days = 1\n",
    "\n",
    "# This list now correctly excludes the target\n",
    "feature_cols = [c for c in df_phase2.columns.tolist() if c != TARGET_COL]\n",
    "\n",
    "# --- Block 2: Build model arrays (Your code was correct here) ---\n",
    "if USE_KBINS:\n",
    "        print(\"\\n--- Step 2: Converting full dataset to K-Bins format ---\")\n",
    "        # ... (rest of your K-Bins code) ...\n",
    "else:\n",
    "    print(\"\\n--- Step 2: Building model arrays (X, Y) ---\")\n",
    "    fixed_df = to_fixedgrid_multiindex(df_phase2, timestamp_col=\"measurement_time\", expected_T=None)  # or set T\n",
    "    \n",
    "    # You correctly passed the filtered 'feature_cols' here, fixing the data leak!\n",
    "    ph2_X, ph2_Y, ph2_labels_list = build_model_arrays(\n",
    "        fixed_df,\n",
    "        feature_cols=feature_cols,  # <-- This is correct!\n",
    "        target_col=TARGET_COL,\n",
    "        history_days=history_days,\n",
    "        horizon_days=horizon_days,\n",
    "    )\n",
    "\n",
    "# --- Block 3: Save arrays ---\n",
    "data_manager = DataManager()\n",
    "data_manager.save_arrays(\n",
    "    ph2_X, ph2_Y,\n",
    "    pd.DataFrame(index=pd.to_datetime(ph2_labels_list, utc=True)),\n",
    "    filename_prefix='phas2_data',  # <-- You are saving as 'phas2_data'\n",
    "    feature_cols=feature_cols,\n",
    "    target_col=TARGET_COL,\n",
    "    metadata={\n",
    "        \"input_csv\": \"data/processed/df_1h_lag_BLV_spatial_images.csv\",\n",
    "        \"timestamp_col\": \"measurement_time\",\n",
    "        \"feature_set\": feature_cols,\n",
    "        \"history_days\": 7,\n",
    "        \"horizon_days\": 1,\n",
    "        \"k_bins\": None,\n",
    "    }\n",
    ")\n",
    "\n",
    "# --- Block 4: Configure and Run ---\n",
    "LSTM_CONFIG = {\n",
    "    \"experiment_name\": \"UniLSTM_p2_exp01_BLVF\",\n",
    "    \"model_type\": \"LSTM\",\n",
    "    \"model_config\": {\n",
    "        \"hidden_size\": 64,\n",
    "        \"num_layers\": 2,\n",
    "        \"dropout\": 0.35,\n",
    "        \"bidirectional\": True,\n",
    "    },\n",
    "    \"data_prefix\": \"phas2_data\",  # <-- FIX 2: Match the filename_prefix\n",
    "    \"splits_file\": \"exp-004/exp-004rolling_origin_splits.json\",\n",
    "    \"feature_cols\": feature_cols,\n",
    "    \"feature_selection\": feature_cols, # <-- This is correct!\n",
    "    \"target_col\": TARGET_COL,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_epochs\": 50,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"loss_function\": \"Huber\",  # <-- This setting needs the fix below\n",
    "    \"early_stopping_patience\": 20,\n",
    "    \"max_folds\": 35,\n",
    "}\n",
    "\n",
    "# This will now work after you apply the fixes below\n",
    "pipeline = SolarForecastingPipeline(LSTM_CONFIG)\n",
    "_, summary = pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e87f38",
   "metadata": {},
   "source": [
    "•\tDuring training, the model shows decreasing loss and MAE on the training set, which is expected as the model learns the underlying patterns. However, the validation loss and MAE do not always decrease at the same rate, indicating some fluctuation in generalization.\n",
    "•\tEarly Stopping: Many of the folds use early stopping, indicating that the model reached a point where further training did not result in significant improvements or even caused overfitting.\n",
    "\n",
    "the model’s performance can fluctuate depending on the fold. This suggests potential instability in model predictions for some subsets of the data.\n",
    "\n",
    "CSI metrics show that the model consistently outperforms the NAM model.\n",
    "\n",
    "The performance drops on validation while the training loss improves, which suggests that the model is memorizing the training data instead of generalizing well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b087680",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d755b25b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71736538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 11,561 records\n",
      "Date range: 2014-01-03 14:00:00+00:00 to 2016-12-31 02:00:00+00:00\n",
      "Timezone: UTC\n"
     ]
    }
   ],
   "source": [
    "from src.data_preparation import load_data\n",
    "PROCESSED_DATA_PATH = \"data/processed/dayTime_NAM_spatial_5locations_dayahead_features_processed.csv\"\n",
    "df = load_data(PROCESSED_DATA_PATH, date_col=\"measurement_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fbd69e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'ghi', 'dni', 'dhi', 'solar_zenith', 'time_gap_hours',\n",
       "       'time_gap_norm', 'day_boundary_flag', 'hour_progression',\n",
       "       'absolute_hour', 'GHI_cs', 'DNI_cs', 'CSI_ghi', 'CSI_dni',\n",
       "       'season_flag', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos',\n",
       "       'issue_time', 'target_time', 'horizon', 'nam_ghi', 'nam_dni', 'nam_cc',\n",
       "       'nam_target_time', '80_dwsw', 'valtime', '80_cloud_cover', '56_dwsw',\n",
       "       '56_cloud_cover', '20_dwsw', '20_cloud_cover', '88_dwsw',\n",
       "       '88_cloud_cover'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91c68df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ROLLING ORIGIN EVALUATION\n",
      "============================================================\n",
      "Total folds: 35\n",
      "Frequency: MS\n",
      "Data range: 2014-01-03 to 2016-12-31\n",
      "\n",
      "Fold Summary:\n",
      "  Fold 1: Train [2014-01-03 to 2014-02-01] (297 records) → Val [2014-02-02 to 2014-02-28] (186 records)\n",
      "  Fold 2: Train [2014-01-03 to 2014-03-01] (494 records) → Val [2014-03-02 to 2014-03-31] (319 records)\n",
      "  Fold 3: Train [2014-01-03 to 2014-04-01] (835 records) → Val [2014-04-02 to 2014-04-30] (308 records)\n",
      "  Fold 4: Train [2014-01-03 to 2014-05-01] (1,165 records) → Val [2014-05-02 to 2014-05-31] (319 records)\n",
      "  Fold 5: Train [2014-01-03 to 2014-06-01] (1,506 records) → Val [2014-06-02 to 2014-06-30] (308 records)\n",
      "  Fold 6: Train [2014-01-03 to 2014-07-01] (1,836 records) → Val [2014-07-02 to 2014-07-31] (319 records)\n",
      "  Fold 7: Train [2014-01-03 to 2014-08-01] (2,177 records) → Val [2014-08-02 to 2014-08-31] (319 records)\n",
      "  Fold 8: Train [2014-01-03 to 2014-09-01] (2,518 records) → Val [2014-09-02 to 2014-09-30] (308 records)\n",
      "  Fold 9: Train [2014-01-03 to 2014-10-01] (2,848 records) → Val [2014-10-02 to 2014-10-31] (319 records)\n",
      "  Fold 10: Train [2014-01-03 to 2014-11-01] (3,189 records) → Val [2014-11-02 to 2014-11-30] (308 records)\n",
      "  Fold 11: Train [2014-01-03 to 2014-12-01] (3,519 records) → Val [2014-12-02 to 2014-12-31] (319 records)\n",
      "  Fold 12: Train [2014-01-03 to 2015-01-01] (3,860 records) → Val [2015-01-02 to 2015-01-31] (319 records)\n",
      "  Fold 13: Train [2014-01-03 to 2015-02-01] (4,201 records) → Val [2015-02-02 to 2015-02-28] (286 records)\n",
      "  Fold 14: Train [2014-01-03 to 2015-03-01] (4,509 records) → Val [2015-03-02 to 2015-03-31] (319 records)\n",
      "  Fold 15: Train [2014-01-03 to 2015-04-01] (4,850 records) → Val [2015-04-02 to 2015-04-30] (308 records)\n",
      "  Fold 16: Train [2014-01-03 to 2015-05-01] (5,180 records) → Val [2015-05-02 to 2015-05-31] (319 records)\n",
      "  Fold 17: Train [2014-01-03 to 2015-06-01] (5,521 records) → Val [2015-06-02 to 2015-06-30] (308 records)\n",
      "  Fold 18: Train [2014-01-03 to 2015-07-01] (5,851 records) → Val [2015-07-02 to 2015-07-31] (319 records)\n",
      "  Fold 19: Train [2014-01-03 to 2015-08-01] (6,192 records) → Val [2015-08-02 to 2015-08-31] (308 records)\n",
      "  Fold 20: Train [2014-01-03 to 2015-09-01] (6,522 records) → Val [2015-09-02 to 2015-09-30] (308 records)\n",
      "  Fold 21: Train [2014-01-03 to 2015-10-01] (6,852 records) → Val [2015-10-02 to 2015-10-31] (319 records)\n",
      "  Fold 22: Train [2014-01-03 to 2015-11-01] (7,193 records) → Val [2015-11-02 to 2015-11-30] (308 records)\n",
      "  Fold 23: Train [2014-01-03 to 2015-12-01] (7,523 records) → Val [2015-12-02 to 2015-12-31] (319 records)\n",
      "  Fold 24: Train [2014-01-03 to 2016-01-01] (7,854 records) → Val [2016-01-02 to 2016-01-31] (220 records)\n",
      "  Fold 25: Train [2014-01-03 to 2016-02-01] (8,095 records) → Val [2016-02-02 to 2016-02-29] (231 records)\n",
      "  Fold 26: Train [2014-01-03 to 2016-03-01] (8,348 records) → Val [2016-03-02 to 2016-03-31] (319 records)\n",
      "  Fold 27: Train [2014-01-03 to 2016-04-01] (8,689 records) → Val [2016-04-02 to 2016-04-30] (308 records)\n",
      "  Fold 28: Train [2014-01-03 to 2016-05-01] (9,019 records) → Val [2016-05-02 to 2016-05-31] (319 records)\n",
      "  Fold 29: Train [2014-01-03 to 2016-06-01] (9,360 records) → Val [2016-06-02 to 2016-06-30] (308 records)\n",
      "  Fold 30: Train [2014-01-03 to 2016-07-01] (9,690 records) → Val [2016-07-02 to 2016-07-31] (297 records)\n",
      "  Fold 31: Train [2014-01-03 to 2016-08-01] (10,009 records) → Val [2016-08-02 to 2016-08-31] (319 records)\n",
      "  Fold 32: Train [2014-01-03 to 2016-09-01] (10,350 records) → Val [2016-09-02 to 2016-09-30] (308 records)\n",
      "  Fold 33: Train [2014-01-03 to 2016-10-01] (10,680 records) → Val [2016-10-02 to 2016-10-31] (319 records)\n",
      "  Fold 34: Train [2014-01-03 to 2016-11-01] (11,021 records) → Val [2016-11-02 to 2016-11-30] (308 records)\n",
      "  Fold 35: Train [2014-01-03 to 2016-12-01] (11,351 records) → Val [2016-12-02 to 2016-12-31] (208 records)\n",
      "============================================================\n",
      "\n",
      "Split metadata saved to 'splits/exp-003/' directory\n"
     ]
    }
   ],
   "source": [
    "# 2. Rolling Origin Split (More Suitable in TimeSeries) like K-Folds\n",
    "from src.data_preparation import rolling_origin_evaluation,save_splits_info\n",
    "\n",
    "rollingSplits_df_5locations = rolling_origin_evaluation(df=df, start_train = '2014-01-2',\n",
    "    end_train = '2016-12-31')\n",
    "save_splits_info({}, rollingSplits_df_5locations, experiment_name=\"exp-003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "513156ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does NAM records have a 1-hour lag ahead from measurements? Yes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_noIndex = df.reset_index()\n",
    "df_noIndex['nam_target_time'] = pd.to_datetime(df_noIndex['nam_target_time'])\n",
    "df_noIndex['measurement_time'] = pd.to_datetime(df_noIndex['measurement_time'])\n",
    "df_noIndex['time_diff'] = (df_noIndex['nam_target_time'] - df_noIndex['measurement_time']).dt.total_seconds() / 3600  # Calculate the time difference in hours\n",
    "    \n",
    "# Check if the time difference is exactly 1 hour\n",
    "is_nam_lag_correct = np.allclose(df_noIndex['time_diff'], 1)  # All time differences should be 1 hour\n",
    "print(f\"Does NAM records have a 1-hour lag ahead from measurements? {'Yes' if is_nam_lag_correct else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "349153c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do we have NaN values in features? Yes\n",
      "--- Features With NaN Values ---\n",
      "The following features have NaN values, with the total count for each:\n",
      "CSI_dni    788\n",
      "CSI_ghi    639\n",
      "dtype: int64\n",
      "\n",
      "========================================\n",
      "\n",
      "--- Distribution of NaN-Value Records by Hour ---\n",
      "Distribution of records (rows) containing at least one NaN, by hour:\n",
      "measurement_time\n",
      "2     449\n",
      "14    313\n",
      "15     26\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total number of rows with at least one NaN: 788\n"
     ]
    }
   ],
   "source": [
    "# 3. Check if there are zero values in any of the features\n",
    "zero_values_in_features = (df[['ghi', 'dni', 'solar_zenith', 'GHI_cs', 'DNI_cs', 'CSI_ghi', \n",
    "                                   'CSI_dni',\n",
    "                                   'nam_ghi', 'nam_dni', 'nam_cc']] == 0).sum().sum()\n",
    "print(f\"Do we have NaN values in features? {'Yes' if zero_values_in_features > 0 else 'No'}\")\n",
    "\n",
    "\n",
    "features_to_check = ['ghi', 'dni', 'solar_zenith', 'GHI_cs', 'DNI_cs', 'CSI_ghi', \n",
    "                                   'CSI_dni',\n",
    "                                   'nam_ghi', 'nam_dni', 'nam_cc']\n",
    "\n",
    "# 2. Calculate the number of NaN values for each feature\n",
    "# We use .isna() instead of == 0\n",
    "nans_per_feature = df[features_to_check].isna().sum()\n",
    "\n",
    "# 3. Filter to get only features that actually have NaN values\n",
    "features_with_nans = nans_per_feature[nans_per_feature > 0]\n",
    "\n",
    "# 4. Report the findings for which features have NaNs\n",
    "if features_with_nans.empty:\n",
    "    print(\"No NaN (missing) values found in any of the specified features.\")\n",
    "else:\n",
    "    print(\"--- Features With NaN Values ---\")\n",
    "    print(\"The following features have NaN values, with the total count for each:\")\n",
    "    # Sort for clearer output\n",
    "    print(features_with_nans.sort_values(ascending=False))\n",
    "    print(\"\\n\" + \"=\"*40 + \"\\n\")\n",
    "\n",
    "    # 5. Analyze the distribution of hours for rows containing NaNs\n",
    "    print(\"--- Distribution of NaN-Value Records by Hour ---\")\n",
    "    \n",
    "    # Create a boolean mask for rows that contain *at least one* NaN\n",
    "    # in the specified columns\n",
    "    rows_with_any_nan = df[features_to_check].isna().any(axis=1)\n",
    "    \n",
    "    if rows_with_any_nan.sum() > 0:\n",
    "        # Get the index for these rows\n",
    "        nan_rows_index = df.index[rows_with_any_nan]\n",
    "        \n",
    "        # Extract the hour from the DatetimeIndex and get the value counts\n",
    "        hour_distribution = nan_rows_index.hour.value_counts().sort_index()\n",
    "        \n",
    "        print(\"Distribution of records (rows) containing at least one NaN, by hour:\")\n",
    "        print(hour_distribution)\n",
    "        \n",
    "        # Optional: Print total number of affected rows\n",
    "        print(f\"\\nTotal number of rows with at least one NaN: {rows_with_any_nan.sum()}\")\n",
    "    else:\n",
    "        # This case shouldn't be hit if features_with_nans was not empty,\n",
    "        # but it's good practice to include.\n",
    "        print(\"No rows found with NaN values (this is unexpected, check logic).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b082d6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1694326/931910069.py:1: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df_interpolated = df.interpolate(method='linear')\n"
     ]
    }
   ],
   "source": [
    "df_interpolated = df.interpolate(method='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b859051c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do we have NaN values in features? Yes\n",
      "--- Features With NaN Values ---\n",
      "The following features have NaN values, with the total count for each:\n",
      "CSI_ghi    1\n",
      "CSI_dni    1\n",
      "dtype: int64\n",
      "\n",
      "========================================\n",
      "\n",
      "--- Distribution of NaN-Value Records by Hour ---\n",
      "Distribution of records (rows) containing at least one NaN, by hour:\n",
      "measurement_time\n",
      "14    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total number of rows with at least one NaN: 1\n"
     ]
    }
   ],
   "source": [
    "# 3. Check if there are zero values in any of the features\n",
    "zero_values_in_features = (df_interpolated[['ghi', 'dni', 'solar_zenith', 'GHI_cs', 'DNI_cs', 'CSI_ghi', \n",
    "                                   'CSI_dni',\n",
    "                                   'nam_ghi', 'nam_dni', 'nam_cc']] == 0).sum().sum()\n",
    "print(f\"Do we have NaN values in features? {'Yes' if zero_values_in_features > 0 else 'No'}\")\n",
    "\n",
    "\n",
    "features_to_check = ['ghi', 'dni', 'solar_zenith', 'GHI_cs', 'DNI_cs', 'CSI_ghi', \n",
    "                                   'CSI_dni',\n",
    "                                   'nam_ghi', 'nam_dni', 'nam_cc']\n",
    "\n",
    "# 2. Calculate the number of NaN values for each feature\n",
    "# We use .isna() instead of == 0\n",
    "nans_per_feature = df_interpolated[features_to_check].isna().sum()\n",
    "\n",
    "# 3. Filter to get only features that actually have NaN values\n",
    "features_with_nans = nans_per_feature[nans_per_feature > 0]\n",
    "\n",
    "# 4. Report the findings for which features have NaNs\n",
    "if features_with_nans.empty:\n",
    "    print(\"No NaN (missing) values found in any of the specified features.\")\n",
    "else:\n",
    "    print(\"--- Features With NaN Values ---\")\n",
    "    print(\"The following features have NaN values, with the total count for each:\")\n",
    "    # Sort for clearer output\n",
    "    print(features_with_nans.sort_values(ascending=False))\n",
    "    print(\"\\n\" + \"=\"*40 + \"\\n\")\n",
    "\n",
    "    # 5. Analyze the distribution of hours for rows containing NaNs\n",
    "    print(\"--- Distribution of NaN-Value Records by Hour ---\")\n",
    "    \n",
    "    # Create a boolean mask for rows that contain *at least one* NaN\n",
    "    # in the specified columns\n",
    "    rows_with_any_nan = df_interpolated[features_to_check].isna().any(axis=1)\n",
    "    \n",
    "    if rows_with_any_nan.sum() > 0:\n",
    "        # Get the index for these rows\n",
    "        nan_rows_index = df_interpolated.index[rows_with_any_nan]\n",
    "        \n",
    "        # Extract the hour from the DatetimeIndex and get the value counts\n",
    "        hour_distribution = nan_rows_index.hour.value_counts().sort_index()\n",
    "        \n",
    "        print(\"Distribution of records (rows) containing at least one NaN, by hour:\")\n",
    "        print(hour_distribution)\n",
    "        \n",
    "        # Optional: Print total number of affected rows\n",
    "        print(f\"\\nTotal number of rows with at least one NaN: {rows_with_any_nan.sum()}\")\n",
    "    else:\n",
    "        # This case shouldn't be hit if features_with_nans was not empty,\n",
    "        # but it's good practice to include.\n",
    "        print(\"No rows found with NaN values (this is unexpected, check logic).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88ecccde",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_interpolated.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efacfc8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'ghi', 'dni', 'dhi', 'solar_zenith', 'time_gap_hours',\n",
       "       'time_gap_norm', 'day_boundary_flag', 'hour_progression',\n",
       "       'absolute_hour', 'GHI_cs', 'DNI_cs', 'CSI_ghi', 'CSI_dni',\n",
       "       'season_flag', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos',\n",
       "       'issue_time', 'target_time', 'horizon', 'nam_ghi', 'nam_dni', 'nam_cc',\n",
       "       'nam_target_time', '80_dwsw', 'valtime', '80_cloud_cover', '56_dwsw',\n",
       "       '56_cloud_cover', '20_dwsw', '20_cloud_cover', '88_dwsw',\n",
       "       '88_cloud_cover'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_interpolated.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7904cd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_phase3 = df_interpolated[['CSI_ghi', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos',\n",
    "    'nam_ghi', 'nam_dni', 'nam_cc',\n",
    "        '80_dwsw', '80_cloud_cover', '56_dwsw',\n",
    "       '56_cloud_cover', '20_dwsw', '20_cloud_cover', '88_dwsw',\n",
    "       '88_cloud_cover']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c5bb7ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CSI_ghi</th>\n",
       "      <th>hour_sin</th>\n",
       "      <th>hour_cos</th>\n",
       "      <th>month_sin</th>\n",
       "      <th>month_cos</th>\n",
       "      <th>nam_ghi</th>\n",
       "      <th>nam_dni</th>\n",
       "      <th>nam_cc</th>\n",
       "      <th>80_dwsw</th>\n",
       "      <th>80_cloud_cover</th>\n",
       "      <th>56_dwsw</th>\n",
       "      <th>56_cloud_cover</th>\n",
       "      <th>20_dwsw</th>\n",
       "      <th>20_cloud_cover</th>\n",
       "      <th>88_dwsw</th>\n",
       "      <th>88_cloud_cover</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>measurement_time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-01-03 15:00:00+00:00</th>\n",
       "      <td>1.200000</td>\n",
       "      <td>-0.707107</td>\n",
       "      <td>-7.071068e-01</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>99.625</td>\n",
       "      <td>497.069277</td>\n",
       "      <td>2.0</td>\n",
       "      <td>99.625</td>\n",
       "      <td>2.0</td>\n",
       "      <td>97.375</td>\n",
       "      <td>2.0</td>\n",
       "      <td>95.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>102.000</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-03 16:00:00+00:00</th>\n",
       "      <td>1.200000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-5.000000e-01</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>274.750</td>\n",
       "      <td>760.162689</td>\n",
       "      <td>0.0</td>\n",
       "      <td>274.750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>273.375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>274.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>279.000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-03 17:00:00+00:00</th>\n",
       "      <td>1.200000</td>\n",
       "      <td>-0.965926</td>\n",
       "      <td>-2.588190e-01</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>416.375</td>\n",
       "      <td>871.248807</td>\n",
       "      <td>2.0</td>\n",
       "      <td>416.375</td>\n",
       "      <td>2.0</td>\n",
       "      <td>411.500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>417.500</td>\n",
       "      <td>4.0</td>\n",
       "      <td>410.375</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-03 18:00:00+00:00</th>\n",
       "      <td>1.200000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.836970e-16</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>510.875</td>\n",
       "      <td>919.980698</td>\n",
       "      <td>8.0</td>\n",
       "      <td>510.875</td>\n",
       "      <td>8.0</td>\n",
       "      <td>508.125</td>\n",
       "      <td>4.0</td>\n",
       "      <td>509.875</td>\n",
       "      <td>12.0</td>\n",
       "      <td>508.750</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-03 19:00:00+00:00</th>\n",
       "      <td>1.126874</td>\n",
       "      <td>-0.965926</td>\n",
       "      <td>2.588190e-01</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>547.375</td>\n",
       "      <td>935.728503</td>\n",
       "      <td>20.0</td>\n",
       "      <td>547.375</td>\n",
       "      <td>20.0</td>\n",
       "      <td>545.375</td>\n",
       "      <td>18.0</td>\n",
       "      <td>546.750</td>\n",
       "      <td>22.0</td>\n",
       "      <td>546.000</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            CSI_ghi  hour_sin      hour_cos  month_sin  \\\n",
       "measurement_time                                                         \n",
       "2014-01-03 15:00:00+00:00  1.200000 -0.707107 -7.071068e-01        0.5   \n",
       "2014-01-03 16:00:00+00:00  1.200000 -0.866025 -5.000000e-01        0.5   \n",
       "2014-01-03 17:00:00+00:00  1.200000 -0.965926 -2.588190e-01        0.5   \n",
       "2014-01-03 18:00:00+00:00  1.200000 -1.000000 -1.836970e-16        0.5   \n",
       "2014-01-03 19:00:00+00:00  1.126874 -0.965926  2.588190e-01        0.5   \n",
       "\n",
       "                           month_cos  nam_ghi     nam_dni  nam_cc  80_dwsw  \\\n",
       "measurement_time                                                             \n",
       "2014-01-03 15:00:00+00:00   0.866025   99.625  497.069277     2.0   99.625   \n",
       "2014-01-03 16:00:00+00:00   0.866025  274.750  760.162689     0.0  274.750   \n",
       "2014-01-03 17:00:00+00:00   0.866025  416.375  871.248807     2.0  416.375   \n",
       "2014-01-03 18:00:00+00:00   0.866025  510.875  919.980698     8.0  510.875   \n",
       "2014-01-03 19:00:00+00:00   0.866025  547.375  935.728503    20.0  547.375   \n",
       "\n",
       "                           80_cloud_cover  56_dwsw  56_cloud_cover  20_dwsw  \\\n",
       "measurement_time                                                              \n",
       "2014-01-03 15:00:00+00:00             2.0   97.375             2.0   95.000   \n",
       "2014-01-03 16:00:00+00:00             0.0  273.375             0.0  274.125   \n",
       "2014-01-03 17:00:00+00:00             2.0  411.500             0.0  417.500   \n",
       "2014-01-03 18:00:00+00:00             8.0  508.125             4.0  509.875   \n",
       "2014-01-03 19:00:00+00:00            20.0  545.375            18.0  546.750   \n",
       "\n",
       "                           20_cloud_cover  88_dwsw  88_cloud_cover  \n",
       "measurement_time                                                    \n",
       "2014-01-03 15:00:00+00:00             0.0  102.000             6.0  \n",
       "2014-01-03 16:00:00+00:00             0.0  279.000             2.0  \n",
       "2014-01-03 17:00:00+00:00             4.0  410.375             0.0  \n",
       "2014-01-03 18:00:00+00:00            12.0  508.750             2.0  \n",
       "2014-01-03 19:00:00+00:00            22.0  546.000            16.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_phase3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b85e67c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.utils:Saved arrays to data/phas3_data_*.npy\n",
      "INFO:src.utils:X shape: (1056, 7, 11, 15), Y shape: (1056, 1, 11)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 2: Building model arrays (X, Y) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.pipeline:Loading data...\n",
      "INFO:src.utils:Loaded arrays from data/phas3_data_*.npy\n",
      "INFO:src.utils:X shape: (1056, 7, 11, 15), Y shape: (1056, 1, 11)\n",
      "INFO:src.pipeline:Building FIXED-GRID reference frame (K_bins is None)...\n",
      "WARNING:src.pipeline:Failed to construct reference dataframe for NAM comparison: unsupported operand type(s) for -: 'str' and 'int'\n",
      "INFO:src.utils:Loaded 35 folds from exp-003/exp-003rolling_origin_splits.json\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 1 ===\n",
      "INFO:src.pipeline:Train samples: 21, Val samples: 18\n",
      "INFO:src.pipeline:Model parameters: 251,531\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.048390, Val Loss: 0.080466, Train MAE: 0.222599, Val MAE: 0.313158, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.032025, Val Loss: 0.061464, Train MAE: 0.179580, Val MAE: 0.289278, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.026566, Val Loss: 0.061203, Train MAE: 0.152212, Val MAE: 0.273124, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.021560, Val Loss: 0.089690, Train MAE: 0.133791, Val MAE: 0.317289, LR: 0.001000\n",
      "INFO:src.engine:Epoch [50/50] - Train Loss: 0.018941, Val Loss: 0.070745, Train MAE: 0.141067, Val MAE: 0.294176, LR: 0.000500\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 1.\n",
      "INFO:src.pipeline:Fold 1 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.141709\n",
      "INFO:src.pipeline:  rmse: 0.376442\n",
      "INFO:src.pipeline:  mae: 0.294176\n",
      "INFO:src.pipeline:  r2: 0.143852\n",
      "INFO:src.pipeline:  mape: 68.729362\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 2 ===\n",
      "INFO:src.pipeline:Train samples: 40, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 251,531\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.050087, Val Loss: 0.064349, Train MAE: 0.245055, Val MAE: 0.325290, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.027376, Val Loss: 0.056567, Train MAE: 0.166803, Val MAE: 0.297149, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 24\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 2.\n",
      "INFO:src.pipeline:Fold 2 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.189303\n",
      "INFO:src.pipeline:  rmse: 0.435090\n",
      "INFO:src.pipeline:  mae: 0.381988\n",
      "INFO:src.pipeline:  r2: -0.701853\n",
      "INFO:src.pipeline:  mape: 59.862755\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 3 ===\n",
      "INFO:src.pipeline:Train samples: 71, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 251,531\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.043147, Val Loss: 0.028591, Train MAE: 0.229222, Val MAE: 0.196501, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.037269, Val Loss: 0.026207, Train MAE: 0.196465, Val MAE: 0.172200, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.027142, Val Loss: 0.022451, Train MAE: 0.165013, Val MAE: 0.150626, LR: 0.000500\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.025728, Val Loss: 0.023518, Train MAE: 0.160141, Val MAE: 0.156707, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 49\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 3.\n",
      "INFO:src.pipeline:Fold 3 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.047856\n",
      "INFO:src.pipeline:  rmse: 0.218761\n",
      "INFO:src.pipeline:  mae: 0.159614\n",
      "INFO:src.pipeline:  r2: 0.423269\n",
      "INFO:src.pipeline:  mape: 33.589500\n",
      "INFO:src.pipeline:\n",
      "=== Experiment Summary ===\n",
      "INFO:src.pipeline:Average Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.126289 ± 0.058766\n",
      "INFO:src.pipeline:  rmse: 0.343431 ± 0.091349\n",
      "INFO:src.pipeline:  mae: 0.278593 ± 0.091450\n",
      "INFO:src.pipeline:  r2: -0.044911 ± 0.478329\n",
      "INFO:src.pipeline:  mape: 54.060539 ± 14.920942\n"
     ]
    }
   ],
   "source": [
    "from src.preprocessing import KBinConfig, process_splits_to_kbins, build_model_arrays, to_fixedgrid_multiindex\n",
    "import pandas as pd\n",
    "from src.utils import DataManager\n",
    "from src.pipeline import SolarForecastingPipeline\n",
    "\n",
    "# --- Block 1: Define variables ---\n",
    "USE_KBINS = False\n",
    "k_bins = 60\n",
    "TARGET_COL = \"CSI_ghi\"  # <-- FIX 1: Define Target FIRST\n",
    "history_days = 7\n",
    "horizon_days = 1\n",
    "\n",
    "# This list now correctly excludes the target\n",
    "feature_cols = [c for c in df_phase3.columns.tolist() if c != TARGET_COL]\n",
    "\n",
    "# --- Block 2: Build model arrays (Your code was correct here) ---\n",
    "if USE_KBINS:\n",
    "        print(\"\\n--- Step 2: Converting full dataset to K-Bins format ---\")\n",
    "        # ... (rest of your K-Bins code) ...\n",
    "else:\n",
    "    print(\"\\n--- Step 2: Building model arrays (X, Y) ---\")\n",
    "    fixed_df = to_fixedgrid_multiindex(df_phase3, timestamp_col=\"measurement_time\", expected_T=None)  # or set T\n",
    "    \n",
    "    # You correctly passed the filtered 'feature_cols' here, fixing the data leak!\n",
    "    ph3_X, ph3_Y, ph3_labels_list = build_model_arrays(\n",
    "        fixed_df,\n",
    "        feature_cols=feature_cols,  # <-- This is correct!\n",
    "        target_col=TARGET_COL,\n",
    "        history_days=history_days,\n",
    "        horizon_days=horizon_days,\n",
    "    )\n",
    "\n",
    "# --- Block 3: Save arrays ---\n",
    "data_manager = DataManager()\n",
    "data_manager.save_arrays(\n",
    "    ph3_X, ph3_Y,\n",
    "    pd.DataFrame(index=pd.to_datetime(ph3_labels_list, utc=True)),\n",
    "    filename_prefix='phas3_data',  # <-- You are saving as 'phas2_data'\n",
    "    feature_cols=feature_cols,\n",
    "    target_col=TARGET_COL,\n",
    "    metadata={\n",
    "        \"input_csv\": \"data/processed/dayTime_NAM_spatial_5locations_dayahead_features_processed.csv\",\n",
    "        \"timestamp_col\": \"measurement_time\",\n",
    "        \"feature_set\": feature_cols,\n",
    "        \"history_days\": 7,\n",
    "        \"horizon_days\": 1,\n",
    "        \"k_bins\": None,\n",
    "    }\n",
    ")\n",
    "\n",
    "# --- Block 4: Configure and Run ---\n",
    "LSTM_CONFIG = {\n",
    "    \"experiment_name\": \"LSTM_phase3_exp02\",\n",
    "    \"model_type\": \"LSTM\",\n",
    "    \"model_config\": {\n",
    "        \"hidden_size\": 64,\n",
    "        \"num_layers\": 2,\n",
    "        \"dropout\": 0.25,\n",
    "        \"bidirectional\": True,\n",
    "    },\n",
    "    \"data_prefix\": \"phas3_data\",  # <-- FIX 2: Match the filename_prefix\n",
    "    \"splits_file\": \"exp-003/exp-003rolling_origin_splits.json\",\n",
    "    \"feature_cols\": feature_cols,\n",
    "    \"feature_selection\": feature_cols, # <-- This is correct!\n",
    "    \"target_col\": TARGET_COL,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_epochs\": 50,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"loss_function\": \"Huber\",  # <-- This setting needs the fix below\n",
    "    \"early_stopping_patience\": 20,\n",
    "    \"max_folds\": 3,\n",
    "}\n",
    "\n",
    "# This will now work after you apply the fixes below\n",
    "pipeline = SolarForecastingPipeline(LSTM_CONFIG)\n",
    "phas3_fold_results, summary = pipeline.run()\n",
    "\n",
    "# To get the model from the LAST fold:\n",
    "phas3_last_model = phas3_fold_results[-1]['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a97f96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.utils:Saved arrays to data/phas4_data_*.npy\n",
      "INFO:src.utils:X shape: (1056, 7, 11, 6), Y shape: (1056, 1, 11)\n",
      "INFO:src.pipeline:Loading data...\n",
      "INFO:src.utils:Loaded arrays from data/phas4_data_*.npy\n",
      "INFO:src.utils:X shape: (1056, 7, 11, 6), Y shape: (1056, 1, 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:src.pipeline:Failed to construct reference dataframe for NAM comparison: could not convert string to float: '2014-01-03 14:00:00+00:00'\n",
      "INFO:src.utils:Loaded 35 folds from exp-003/exp-003rolling_origin_splits.json\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 1 ===\n",
      "INFO:src.pipeline:Train samples: 21, Val samples: 18\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 2: Building model arrays (X, Y) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.051697, Val Loss: 0.087358, Train MAE: 0.227353, Val MAE: 0.320367, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.044961, Val Loss: 0.076549, Train MAE: 0.212394, Val MAE: 0.301476, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.024929, Val Loss: 0.077833, Train MAE: 0.152070, Val MAE: 0.294396, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.023551, Val Loss: 0.081485, Train MAE: 0.134832, Val MAE: 0.297576, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 45\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 1.\n",
      "INFO:src.pipeline:Fold 1 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.161004\n",
      "INFO:src.pipeline:  rmse: 0.401253\n",
      "INFO:src.pipeline:  mae: 0.295937\n",
      "INFO:src.pipeline:  r2: 0.027275\n",
      "INFO:src.pipeline:  mape: 78.607788\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 2 ===\n",
      "INFO:src.pipeline:Train samples: 40, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.049543, Val Loss: 0.065256, Train MAE: 0.252557, Val MAE: 0.325191, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.042641, Val Loss: 0.045536, Train MAE: 0.220476, Val MAE: 0.267640, LR: 0.000500\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.046473, Val Loss: 0.056028, Train MAE: 0.227792, Val MAE: 0.300442, LR: 0.000500\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.028526, Val Loss: 0.041509, Train MAE: 0.154594, Val MAE: 0.248156, LR: 0.000250\n",
      "INFO:src.engine:Early stopping at epoch 46\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 2.\n",
      "INFO:src.pipeline:Fold 2 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.101845\n",
      "INFO:src.pipeline:  rmse: 0.319132\n",
      "INFO:src.pipeline:  mae: 0.284106\n",
      "INFO:src.pipeline:  r2: 0.084405\n",
      "INFO:src.pipeline:  mape: 65.261124\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 3 ===\n",
      "INFO:src.pipeline:Train samples: 71, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.053629, Val Loss: 0.025698, Train MAE: 0.257343, Val MAE: 0.158472, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.032691, Val Loss: 0.031230, Train MAE: 0.177133, Val MAE: 0.149641, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.031981, Val Loss: 0.023046, Train MAE: 0.178528, Val MAE: 0.127980, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.035552, Val Loss: 0.021529, Train MAE: 0.191864, Val MAE: 0.135020, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 48\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 3.\n",
      "INFO:src.pipeline:Fold 3 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.043600\n",
      "INFO:src.pipeline:  rmse: 0.208806\n",
      "INFO:src.pipeline:  mae: 0.121423\n",
      "INFO:src.pipeline:  r2: 0.474566\n",
      "INFO:src.pipeline:  mape: 33.536869\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 4 ===\n",
      "INFO:src.pipeline:Train samples: 101, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.036342, Val Loss: 0.011919, Train MAE: 0.199115, Val MAE: 0.107658, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.028691, Val Loss: 0.012329, Train MAE: 0.177563, Val MAE: 0.094597, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.030949, Val Loss: 0.012048, Train MAE: 0.173703, Val MAE: 0.100862, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.025279, Val Loss: 0.011656, Train MAE: 0.151898, Val MAE: 0.099173, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 44\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 4.\n",
      "INFO:src.pipeline:Fold 4 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.023625\n",
      "INFO:src.pipeline:  rmse: 0.153705\n",
      "INFO:src.pipeline:  mae: 0.106405\n",
      "INFO:src.pipeline:  r2: 0.493144\n",
      "INFO:src.pipeline:  mape: 15.734255\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 5 ===\n",
      "INFO:src.pipeline:Train samples: 132, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.031278, Val Loss: 0.005768, Train MAE: 0.181544, Val MAE: 0.071452, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.043119, Val Loss: 0.007236, Train MAE: 0.199604, Val MAE: 0.094568, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.022582, Val Loss: 0.004885, Train MAE: 0.148767, Val MAE: 0.055250, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 32\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 5.\n",
      "INFO:src.pipeline:Fold 5 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.010741\n",
      "INFO:src.pipeline:  rmse: 0.103638\n",
      "INFO:src.pipeline:  mae: 0.065713\n",
      "INFO:src.pipeline:  r2: 0.597576\n",
      "INFO:src.pipeline:  mape: 8.141768\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 6 ===\n",
      "INFO:src.pipeline:Train samples: 162, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.024610, Val Loss: 0.012791, Train MAE: 0.149149, Val MAE: 0.095847, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.020657, Val Loss: 0.014148, Train MAE: 0.135303, Val MAE: 0.087318, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 26\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 6.\n",
      "INFO:src.pipeline:Fold 6 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.029220\n",
      "INFO:src.pipeline:  rmse: 0.170939\n",
      "INFO:src.pipeline:  mae: 0.086687\n",
      "INFO:src.pipeline:  r2: 0.267065\n",
      "INFO:src.pipeline:  mape: 17.265215\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 7 ===\n",
      "INFO:src.pipeline:Train samples: 193, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.020646, Val Loss: 0.023925, Train MAE: 0.133516, Val MAE: 0.123788, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.018602, Val Loss: 0.023175, Train MAE: 0.118500, Val MAE: 0.111214, LR: 0.000500\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.024201, Val Loss: 0.022005, Train MAE: 0.145641, Val MAE: 0.114229, LR: 0.000500\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.022725, Val Loss: 0.021406, Train MAE: 0.145836, Val MAE: 0.109784, LR: 0.000500\n",
      "INFO:src.engine:Epoch [50/50] - Train Loss: 0.019765, Val Loss: 0.020219, Train MAE: 0.136289, Val MAE: 0.121249, LR: 0.000500\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 7.\n",
      "INFO:src.pipeline:Fold 7 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.040437\n",
      "INFO:src.pipeline:  rmse: 0.201090\n",
      "INFO:src.pipeline:  mae: 0.121249\n",
      "INFO:src.pipeline:  r2: 0.430613\n",
      "INFO:src.pipeline:  mape: 31.897800\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 8 ===\n",
      "INFO:src.pipeline:Train samples: 224, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.021490, Val Loss: 0.026256, Train MAE: 0.135738, Val MAE: 0.134270, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.018836, Val Loss: 0.026033, Train MAE: 0.122723, Val MAE: 0.113387, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 23\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 8.\n",
      "INFO:src.pipeline:Fold 8 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.051649\n",
      "INFO:src.pipeline:  rmse: 0.227263\n",
      "INFO:src.pipeline:  mae: 0.111550\n",
      "INFO:src.pipeline:  r2: 0.227525\n",
      "INFO:src.pipeline:  mape: 31.877350\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 9 ===\n",
      "INFO:src.pipeline:Train samples: 254, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.022433, Val Loss: 0.020467, Train MAE: 0.140273, Val MAE: 0.139614, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.021594, Val Loss: 0.023301, Train MAE: 0.126856, Val MAE: 0.185459, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.017051, Val Loss: 0.016487, Train MAE: 0.117584, Val MAE: 0.145969, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.014329, Val Loss: 0.017930, Train MAE: 0.105559, Val MAE: 0.154312, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 47\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 9.\n",
      "INFO:src.pipeline:Fold 9 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.037507\n",
      "INFO:src.pipeline:  rmse: 0.193667\n",
      "INFO:src.pipeline:  mae: 0.161545\n",
      "INFO:src.pipeline:  r2: 0.211292\n",
      "INFO:src.pipeline:  mape: 21.366646\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 10 ===\n",
      "INFO:src.pipeline:Train samples: 285, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.021016, Val Loss: 0.041618, Train MAE: 0.134526, Val MAE: 0.183992, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.017528, Val Loss: 0.037540, Train MAE: 0.121849, Val MAE: 0.192569, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.015386, Val Loss: 0.039251, Train MAE: 0.112750, Val MAE: 0.183382, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 32\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 10.\n",
      "INFO:src.pipeline:Fold 10 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.077057\n",
      "INFO:src.pipeline:  rmse: 0.277591\n",
      "INFO:src.pipeline:  mae: 0.184923\n",
      "INFO:src.pipeline:  r2: 0.307627\n",
      "INFO:src.pipeline:  mape: 59.012234\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 11 ===\n",
      "INFO:src.pipeline:Train samples: 315, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.023235, Val Loss: 0.046103, Train MAE: 0.142308, Val MAE: 0.245083, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.018433, Val Loss: 0.053574, Train MAE: 0.124943, Val MAE: 0.246271, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 25\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 11.\n",
      "INFO:src.pipeline:Fold 11 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.117912\n",
      "INFO:src.pipeline:  rmse: 0.343384\n",
      "INFO:src.pipeline:  mae: 0.252153\n",
      "INFO:src.pipeline:  r2: 0.172615\n",
      "INFO:src.pipeline:  mape: 88.300545\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 12 ===\n",
      "INFO:src.pipeline:Train samples: 346, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.025902, Val Loss: 0.037375, Train MAE: 0.155823, Val MAE: 0.211431, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.021295, Val Loss: 0.043704, Train MAE: 0.137489, Val MAE: 0.205821, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 26\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 12.\n",
      "INFO:src.pipeline:Fold 12 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.088723\n",
      "INFO:src.pipeline:  rmse: 0.297865\n",
      "INFO:src.pipeline:  mae: 0.203376\n",
      "INFO:src.pipeline:  r2: 0.026719\n",
      "INFO:src.pipeline:  mape: 46.589241\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 13 ===\n",
      "INFO:src.pipeline:Train samples: 377, Val samples: 26\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.025978, Val Loss: 0.036930, Train MAE: 0.154948, Val MAE: 0.213589, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.023506, Val Loss: 0.036699, Train MAE: 0.145102, Val MAE: 0.206079, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.021263, Val Loss: 0.038685, Train MAE: 0.135402, Val MAE: 0.200521, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 34\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 13.\n",
      "INFO:src.pipeline:Fold 13 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.083406\n",
      "INFO:src.pipeline:  rmse: 0.288801\n",
      "INFO:src.pipeline:  mae: 0.206141\n",
      "INFO:src.pipeline:  r2: 0.288351\n",
      "INFO:src.pipeline:  mape: 68.734001\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 14 ===\n",
      "INFO:src.pipeline:Train samples: 405, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.026300, Val Loss: 0.024623, Train MAE: 0.158884, Val MAE: 0.161754, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.024650, Val Loss: 0.030321, Train MAE: 0.149785, Val MAE: 0.176814, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.021957, Val Loss: 0.032102, Train MAE: 0.141114, Val MAE: 0.175110, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 30\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 14.\n",
      "INFO:src.pipeline:Fold 14 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.064203\n",
      "INFO:src.pipeline:  rmse: 0.253383\n",
      "INFO:src.pipeline:  mae: 0.175110\n",
      "INFO:src.pipeline:  r2: 0.192162\n",
      "INFO:src.pipeline:  mape: 28.988800\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 15 ===\n",
      "INFO:src.pipeline:Train samples: 436, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.025755, Val Loss: 0.019721, Train MAE: 0.155893, Val MAE: 0.129985, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.023669, Val Loss: 0.020706, Train MAE: 0.145085, Val MAE: 0.126720, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.021596, Val Loss: 0.020982, Train MAE: 0.137599, Val MAE: 0.121036, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 38\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 15.\n",
      "INFO:src.pipeline:Fold 15 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.038845\n",
      "INFO:src.pipeline:  rmse: 0.197092\n",
      "INFO:src.pipeline:  mae: 0.115606\n",
      "INFO:src.pipeline:  r2: 0.538708\n",
      "INFO:src.pipeline:  mape: 31.901279\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 16 ===\n",
      "INFO:src.pipeline:Train samples: 466, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.026031, Val Loss: 0.019296, Train MAE: 0.157283, Val MAE: 0.113416, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.022464, Val Loss: 0.018958, Train MAE: 0.141219, Val MAE: 0.114566, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 25\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 16.\n",
      "INFO:src.pipeline:Fold 16 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.037115\n",
      "INFO:src.pipeline:  rmse: 0.192654\n",
      "INFO:src.pipeline:  mae: 0.117598\n",
      "INFO:src.pipeline:  r2: 0.336571\n",
      "INFO:src.pipeline:  mape: 41.817493\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 17 ===\n",
      "INFO:src.pipeline:Train samples: 497, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.025696, Val Loss: 0.018056, Train MAE: 0.153827, Val MAE: 0.097955, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.022375, Val Loss: 0.018085, Train MAE: 0.142316, Val MAE: 0.104165, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 24\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 17.\n",
      "INFO:src.pipeline:Fold 17 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.036015\n",
      "INFO:src.pipeline:  rmse: 0.189776\n",
      "INFO:src.pipeline:  mae: 0.109041\n",
      "INFO:src.pipeline:  r2: 0.386378\n",
      "INFO:src.pipeline:  mape: 26.224894\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 18 ===\n",
      "INFO:src.pipeline:Train samples: 527, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.024988, Val Loss: 0.014959, Train MAE: 0.154037, Val MAE: 0.130499, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.021922, Val Loss: 0.012865, Train MAE: 0.142295, Val MAE: 0.109029, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.020638, Val Loss: 0.013391, Train MAE: 0.138876, Val MAE: 0.091342, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 37\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 18.\n",
      "INFO:src.pipeline:Fold 18 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.028200\n",
      "INFO:src.pipeline:  rmse: 0.167928\n",
      "INFO:src.pipeline:  mae: 0.122653\n",
      "INFO:src.pipeline:  r2: 0.347829\n",
      "INFO:src.pipeline:  mape: 19.578266\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 19 ===\n",
      "INFO:src.pipeline:Train samples: 558, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.025098, Val Loss: 0.025595, Train MAE: 0.155186, Val MAE: 0.127670, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.022708, Val Loss: 0.026428, Train MAE: 0.144254, Val MAE: 0.139375, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.020392, Val Loss: 0.029477, Train MAE: 0.134667, Val MAE: 0.132748, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 35\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 19.\n",
      "INFO:src.pipeline:Fold 19 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.053554\n",
      "INFO:src.pipeline:  rmse: 0.231418\n",
      "INFO:src.pipeline:  mae: 0.107282\n",
      "INFO:src.pipeline:  r2: 0.321015\n",
      "INFO:src.pipeline:  mape: 11.408997\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 20 ===\n",
      "INFO:src.pipeline:Train samples: 589, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.024307, Val Loss: 0.018209, Train MAE: 0.149095, Val MAE: 0.135877, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.022938, Val Loss: 0.017063, Train MAE: 0.141518, Val MAE: 0.132465, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.020451, Val Loss: 0.017932, Train MAE: 0.130738, Val MAE: 0.123219, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 36\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 20.\n",
      "INFO:src.pipeline:Fold 20 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.043560\n",
      "INFO:src.pipeline:  rmse: 0.208711\n",
      "INFO:src.pipeline:  mae: 0.146932\n",
      "INFO:src.pipeline:  r2: 0.337031\n",
      "INFO:src.pipeline:  mape: 25.877800\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 21 ===\n",
      "INFO:src.pipeline:Train samples: 619, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.023521, Val Loss: 0.018728, Train MAE: 0.147029, Val MAE: 0.132867, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.022755, Val Loss: 0.016854, Train MAE: 0.142127, Val MAE: 0.133175, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.019992, Val Loss: 0.016697, Train MAE: 0.132262, Val MAE: 0.123199, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.017442, Val Loss: 0.017434, Train MAE: 0.122750, Val MAE: 0.121912, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 43\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 21.\n",
      "INFO:src.pipeline:Fold 21 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.047102\n",
      "INFO:src.pipeline:  rmse: 0.217030\n",
      "INFO:src.pipeline:  mae: 0.137486\n",
      "INFO:src.pipeline:  r2: 0.023994\n",
      "INFO:src.pipeline:  mape: 19.676910\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 22 ===\n",
      "INFO:src.pipeline:Train samples: 650, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.025240, Val Loss: 0.035450, Train MAE: 0.148716, Val MAE: 0.200312, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.021886, Val Loss: 0.035945, Train MAE: 0.140069, Val MAE: 0.170391, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.020044, Val Loss: 0.039812, Train MAE: 0.133410, Val MAE: 0.182858, LR: 0.001000\n",
      "INFO:src.engine:Early stopping at epoch 39\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 22.\n",
      "INFO:src.pipeline:Fold 22 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.079428\n",
      "INFO:src.pipeline:  rmse: 0.281830\n",
      "INFO:src.pipeline:  mae: 0.193277\n",
      "INFO:src.pipeline:  r2: 0.054252\n",
      "INFO:src.pipeline:  mape: 48.903648\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 23 ===\n",
      "INFO:src.pipeline:Train samples: 680, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.025731, Val Loss: 0.044269, Train MAE: 0.156311, Val MAE: 0.234123, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.022448, Val Loss: 0.043102, Train MAE: 0.141852, Val MAE: 0.234187, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 26\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 23.\n",
      "INFO:src.pipeline:Fold 23 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.098198\n",
      "INFO:src.pipeline:  rmse: 0.313365\n",
      "INFO:src.pipeline:  mae: 0.242602\n",
      "INFO:src.pipeline:  r2: 0.130230\n",
      "INFO:src.pipeline:  mape: 63.711494\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 24 ===\n",
      "INFO:src.pipeline:Train samples: 711, Val samples: 21\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.025549, Val Loss: 0.061818, Train MAE: 0.159307, Val MAE: 0.284757, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.023442, Val Loss: 0.064684, Train MAE: 0.147908, Val MAE: 0.277631, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 24\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 24.\n",
      "INFO:src.pipeline:Fold 24 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.125638\n",
      "INFO:src.pipeline:  rmse: 0.354454\n",
      "INFO:src.pipeline:  mae: 0.272778\n",
      "INFO:src.pipeline:  r2: 0.185251\n",
      "INFO:src.pipeline:  mape: 87.684196\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 25 ===\n",
      "INFO:src.pipeline:Train samples: 734, Val samples: 23\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.026933, Val Loss: 0.052002, Train MAE: 0.162428, Val MAE: 0.209671, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.025067, Val Loss: 0.056742, Train MAE: 0.150539, Val MAE: 0.230183, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 23\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 25.\n",
      "INFO:src.pipeline:Fold 25 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.113314\n",
      "INFO:src.pipeline:  rmse: 0.336622\n",
      "INFO:src.pipeline:  mae: 0.231334\n",
      "INFO:src.pipeline:  r2: 0.061978\n",
      "INFO:src.pipeline:  mape: 20.752678\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 26 ===\n",
      "INFO:src.pipeline:Train samples: 759, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.027646, Val Loss: 0.040843, Train MAE: 0.160730, Val MAE: 0.216293, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.026732, Val Loss: 0.037792, Train MAE: 0.155479, Val MAE: 0.206226, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.023590, Val Loss: 0.052009, Train MAE: 0.145209, Val MAE: 0.234818, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.019117, Val Loss: 0.054727, Train MAE: 0.128630, Val MAE: 0.228009, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 40\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 26.\n",
      "INFO:src.pipeline:Fold 26 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.109454\n",
      "INFO:src.pipeline:  rmse: 0.330838\n",
      "INFO:src.pipeline:  mae: 0.228009\n",
      "INFO:src.pipeline:  r2: 0.229650\n",
      "INFO:src.pipeline:  mape: 133.067581\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 27 ===\n",
      "INFO:src.pipeline:Train samples: 790, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.028779, Val Loss: 0.034747, Train MAE: 0.168027, Val MAE: 0.158063, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.026724, Val Loss: 0.028783, Train MAE: 0.157654, Val MAE: 0.165977, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.023839, Val Loss: 0.029462, Train MAE: 0.145874, Val MAE: 0.152161, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 38\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 27.\n",
      "INFO:src.pipeline:Fold 27 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.062848\n",
      "INFO:src.pipeline:  rmse: 0.250695\n",
      "INFO:src.pipeline:  mae: 0.166412\n",
      "INFO:src.pipeline:  r2: 0.420505\n",
      "INFO:src.pipeline:  mape: 56.794231\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 28 ===\n",
      "INFO:src.pipeline:Train samples: 820, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.028450, Val Loss: 0.024342, Train MAE: 0.166562, Val MAE: 0.138788, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.026977, Val Loss: 0.021258, Train MAE: 0.159112, Val MAE: 0.134474, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.023964, Val Loss: 0.028896, Train MAE: 0.144906, Val MAE: 0.152281, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 38\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 28.\n",
      "INFO:src.pipeline:Fold 28 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.062902\n",
      "INFO:src.pipeline:  rmse: 0.250802\n",
      "INFO:src.pipeline:  mae: 0.159929\n",
      "INFO:src.pipeline:  r2: 0.241806\n",
      "INFO:src.pipeline:  mape: 42.360706\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 29 ===\n",
      "INFO:src.pipeline:Train samples: 851, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.027306, Val Loss: 0.008428, Train MAE: 0.162724, Val MAE: 0.095889, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.026377, Val Loss: 0.008847, Train MAE: 0.154214, Val MAE: 0.110162, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.024110, Val Loss: 0.009409, Train MAE: 0.146722, Val MAE: 0.110830, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 32\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 29.\n",
      "INFO:src.pipeline:Fold 29 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.017750\n",
      "INFO:src.pipeline:  rmse: 0.133231\n",
      "INFO:src.pipeline:  mae: 0.093265\n",
      "INFO:src.pipeline:  r2: 0.567696\n",
      "INFO:src.pipeline:  mape: 12.190336\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 30 ===\n",
      "INFO:src.pipeline:Train samples: 881, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.027379, Val Loss: 0.020375, Train MAE: 0.161499, Val MAE: 0.096838, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.025430, Val Loss: 0.020227, Train MAE: 0.153577, Val MAE: 0.094071, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.023048, Val Loss: 0.020289, Train MAE: 0.144109, Val MAE: 0.099981, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.021182, Val Loss: 0.020102, Train MAE: 0.138240, Val MAE: 0.089413, LR: 0.001000\n",
      "INFO:src.engine:Epoch [50/50] - Train Loss: 0.014287, Val Loss: 0.020407, Train MAE: 0.112134, Val MAE: 0.071468, LR: 0.000500\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 30.\n",
      "INFO:src.pipeline:Fold 30 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.041133\n",
      "INFO:src.pipeline:  rmse: 0.202812\n",
      "INFO:src.pipeline:  mae: 0.071468\n",
      "INFO:src.pipeline:  r2: 0.283906\n",
      "INFO:src.pipeline:  mape: 3.725259\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 31 ===\n",
      "INFO:src.pipeline:Train samples: 911, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.027241, Val Loss: 0.004731, Train MAE: 0.157798, Val MAE: 0.064420, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.025299, Val Loss: 0.003976, Train MAE: 0.151908, Val MAE: 0.040780, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.024220, Val Loss: 0.004649, Train MAE: 0.146260, Val MAE: 0.062394, LR: 0.001000\n",
      "INFO:src.engine:Early stopping at epoch 39\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 31.\n",
      "INFO:src.pipeline:Fold 31 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.008273\n",
      "INFO:src.pipeline:  rmse: 0.090953\n",
      "INFO:src.pipeline:  mae: 0.049101\n",
      "INFO:src.pipeline:  r2: 0.787008\n",
      "INFO:src.pipeline:  mape: 6.733099\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 32 ===\n",
      "INFO:src.pipeline:Train samples: 942, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.025762, Val Loss: 0.007503, Train MAE: 0.153349, Val MAE: 0.072169, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.025328, Val Loss: 0.006744, Train MAE: 0.148937, Val MAE: 0.065085, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.022814, Val Loss: 0.005989, Train MAE: 0.137797, Val MAE: 0.057619, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.019439, Val Loss: 0.005523, Train MAE: 0.127130, Val MAE: 0.044307, LR: 0.000500\n",
      "INFO:src.engine:Epoch [50/50] - Train Loss: 0.016106, Val Loss: 0.005049, Train MAE: 0.115874, Val MAE: 0.047503, LR: 0.000500\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 32.\n",
      "INFO:src.pipeline:Fold 32 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.010099\n",
      "INFO:src.pipeline:  rmse: 0.100493\n",
      "INFO:src.pipeline:  mae: 0.047503\n",
      "INFO:src.pipeline:  r2: 0.744573\n",
      "INFO:src.pipeline:  mape: 6.831277\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 33 ===\n",
      "INFO:src.pipeline:Train samples: 972, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.025383, Val Loss: 0.048333, Train MAE: 0.151313, Val MAE: 0.210315, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.024246, Val Loss: 0.043372, Train MAE: 0.142927, Val MAE: 0.219479, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.022970, Val Loss: 0.050011, Train MAE: 0.137914, Val MAE: 0.207201, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.020043, Val Loss: 0.059244, Train MAE: 0.126987, Val MAE: 0.208531, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 40\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 33.\n",
      "INFO:src.pipeline:Fold 33 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.118512\n",
      "INFO:src.pipeline:  rmse: 0.344255\n",
      "INFO:src.pipeline:  mae: 0.208531\n",
      "INFO:src.pipeline:  r2: 0.089055\n",
      "INFO:src.pipeline:  mape: 100.944374\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 34 ===\n",
      "INFO:src.pipeline:Train samples: 1003, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.025444, Val Loss: 0.033160, Train MAE: 0.151195, Val MAE: 0.193632, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.024006, Val Loss: 0.032679, Train MAE: 0.146429, Val MAE: 0.196598, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.022674, Val Loss: 0.034618, Train MAE: 0.137188, Val MAE: 0.200304, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.020294, Val Loss: 0.039926, Train MAE: 0.128247, Val MAE: 0.215727, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 46\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 34.\n",
      "INFO:src.pipeline:Fold 34 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.099326\n",
      "INFO:src.pipeline:  rmse: 0.315160\n",
      "INFO:src.pipeline:  mae: 0.232802\n",
      "INFO:src.pipeline:  r2: 0.017104\n",
      "INFO:src.pipeline:  mape: 43.812309\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 35 ===\n",
      "INFO:src.pipeline:Train samples: 1033, Val samples: 21\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.027288, Val Loss: 0.078180, Train MAE: 0.155568, Val MAE: 0.282623, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.024558, Val Loss: 0.078398, Train MAE: 0.147611, Val MAE: 0.281962, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.021986, Val Loss: 0.082337, Train MAE: 0.138717, Val MAE: 0.330081, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 37\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 35.\n",
      "INFO:src.pipeline:Fold 35 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.176798\n",
      "INFO:src.pipeline:  rmse: 0.420474\n",
      "INFO:src.pipeline:  mae: 0.339765\n",
      "INFO:src.pipeline:  r2: -0.108294\n",
      "INFO:src.pipeline:  mape: 53.972595\n",
      "INFO:src.pipeline:\n",
      "=== Experiment Summary ===\n",
      "INFO:src.pipeline:Average Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.066828 ± 0.041981\n",
      "INFO:src.pipeline:  rmse: 0.244889 ± 0.082812\n",
      "INFO:src.pipeline:  mae: 0.164848 ± 0.073236\n",
      "INFO:src.pipeline:  r2: 0.279919 ± 0.208601\n",
      "INFO:src.pipeline:  mape: 41.522373 ± 29.729323\n"
     ]
    }
   ],
   "source": [
    "from src.preprocessing import KBinConfig, process_splits_to_kbins, build_model_arrays, to_fixedgrid_multiindex\n",
    "import pandas as pd\n",
    "from src.utils import DataManager\n",
    "from src.pipeline import SolarForecastingPipeline\n",
    "\n",
    "\n",
    "df_phase4 = df_interpolated[['CSI_ghi', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos',\n",
    "    'nam_ghi', 'nam_cc']]\n",
    "\n",
    "\n",
    "# --- Block 1: Define variables ---\n",
    "USE_KBINS = False\n",
    "k_bins = 60\n",
    "TARGET_COL = \"CSI_ghi\"  # <-- FIX 1: Define Target FIRST\n",
    "history_days = 7\n",
    "horizon_days = 1\n",
    "\n",
    "# This list now correctly excludes the target\n",
    "feature_cols = [c for c in df_phase4.columns.tolist() if c != TARGET_COL]\n",
    "\n",
    "# --- Block 2: Build model arrays (Your code was correct here) ---\n",
    "if USE_KBINS:\n",
    "        print(\"\\n--- Step 2: Converting full dataset to K-Bins format ---\")\n",
    "        # ... (rest of your K-Bins code) ...\n",
    "else:\n",
    "    print(\"\\n--- Step 2: Building model arrays (X, Y) ---\")\n",
    "    fixed_df = to_fixedgrid_multiindex(df_phase4, timestamp_col=\"measurement_time\", expected_T=None)  # or set T\n",
    "    \n",
    "    # You correctly passed the filtered 'feature_cols' here, fixing the data leak!\n",
    "    ph4_X, ph4_Y, ph4_labels_list = build_model_arrays(\n",
    "        fixed_df,\n",
    "        feature_cols=feature_cols,  # <-- This is correct!\n",
    "        target_col=TARGET_COL,\n",
    "        history_days=history_days,\n",
    "        horizon_days=horizon_days,\n",
    "    )\n",
    "\n",
    "# --- Block 3: Save arrays ---\n",
    "data_manager = DataManager()\n",
    "data_manager.save_arrays(\n",
    "    ph4_X, ph4_Y,\n",
    "    pd.DataFrame(index=pd.to_datetime(ph4_labels_list, utc=True)),\n",
    "    filename_prefix='phas4_data',  # <-- You are saving as 'phas2_data'\n",
    "    feature_cols=feature_cols,\n",
    "    target_col=TARGET_COL,\n",
    "    metadata={\n",
    "        \"input_csv\": \"data/processed/dayTime_NAM_spatial_5locations_dayahead_features_processed.csv\",\n",
    "        \"timestamp_col\": \"measurement_time\",\n",
    "        \"feature_set\": feature_cols,\n",
    "        \"history_days\": 7,\n",
    "        \"horizon_days\": 1,\n",
    "        \"k_bins\": None,\n",
    "    }\n",
    ")\n",
    "\n",
    "# --- Block 4: Configure and Run ---\n",
    "LSTM_CONFIG = {\n",
    "    \"experiment_name\": \"LSTM_phase4_exp02\",\n",
    "    \"model_type\": \"LSTM\",\n",
    "    \"model_config\": {\n",
    "        \"hidden_size\": 64,\n",
    "        \"num_layers\": 2,\n",
    "        \"dropout\": 0.25,\n",
    "        \"bidirectional\": True,\n",
    "    },\n",
    "    \"data_prefix\": \"phas4_data\",  # <-- FIX 2: Match the filename_prefix\n",
    "    \"splits_file\": \"exp-003/exp-003rolling_origin_splits.json\",\n",
    "    \"feature_cols\": feature_cols,\n",
    "    \"feature_selection\": feature_cols, # <-- This is correct!\n",
    "    \"target_col\": TARGET_COL,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_epochs\": 50,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"loss_function\": \"Huber\",  # <-- This setting needs the fix below\n",
    "    \"early_stopping_patience\": 20,\n",
    "    \"max_folds\": 35,\n",
    "}\n",
    "\n",
    "# This will now work after you apply the fixes below\n",
    "pipeline = SolarForecastingPipeline(LSTM_CONFIG)\n",
    "fold_results, summary = pipeline.run()\n",
    "\n",
    "# To get the model from the LAST fold:\n",
    "last_model = fold_results[-1]['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da4bc29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.utils:Saved arrays to data/phas5_data_*.npy\n",
      "INFO:src.utils:X shape: (1056, 7, 11, 9), Y shape: (1056, 1, 11)\n",
      "INFO:src.pipeline:Loading data...\n",
      "INFO:src.utils:Loaded arrays from data/phas5_data_*.npy\n",
      "INFO:src.utils:X shape: (1056, 7, 11, 9), Y shape: (1056, 1, 11)\n",
      "WARNING:src.pipeline:Failed to construct reference dataframe for NAM comparison: could not convert string to float: '2014-01-03 14:00:00+00:00'\n",
      "INFO:src.utils:Loaded 35 folds from exp-003/exp-003rolling_origin_splits.json\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 1 ===\n",
      "INFO:src.pipeline:Train samples: 21, Val samples: 18\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 2: Building model arrays (X, Y) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.049739, Val Loss: 0.088166, Train MAE: 0.221810, Val MAE: 0.325361, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.025353, Val Loss: 0.074557, Train MAE: 0.148397, Val MAE: 0.294710, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.022186, Val Loss: 0.080162, Train MAE: 0.141149, Val MAE: 0.305926, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 38\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 1.\n",
      "INFO:src.pipeline:Fold 1 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.167472\n",
      "INFO:src.pipeline:  rmse: 0.409233\n",
      "INFO:src.pipeline:  mae: 0.301700\n",
      "INFO:src.pipeline:  r2: -0.011800\n",
      "INFO:src.pipeline:  mape: 70.875938\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 2 ===\n",
      "INFO:src.pipeline:Train samples: 40, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.055114, Val Loss: 0.061075, Train MAE: 0.258907, Val MAE: 0.291307, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.033499, Val Loss: 0.059160, Train MAE: 0.195160, Val MAE: 0.313426, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.026593, Val Loss: 0.048143, Train MAE: 0.169916, Val MAE: 0.273401, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.030522, Val Loss: 0.068122, Train MAE: 0.179059, Val MAE: 0.298328, LR: 0.001000\n",
      "INFO:src.engine:Epoch [50/50] - Train Loss: 0.019333, Val Loss: 0.063462, Train MAE: 0.139043, Val MAE: 0.300646, LR: 0.000500\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 2.\n",
      "INFO:src.pipeline:Fold 2 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.126925\n",
      "INFO:src.pipeline:  rmse: 0.356265\n",
      "INFO:src.pipeline:  mae: 0.300646\n",
      "INFO:src.pipeline:  r2: -0.141062\n",
      "INFO:src.pipeline:  mape: 66.571030\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 3 ===\n",
      "INFO:src.pipeline:Train samples: 71, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.048149, Val Loss: 0.024014, Train MAE: 0.232928, Val MAE: 0.147684, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.038094, Val Loss: 0.025987, Train MAE: 0.199845, Val MAE: 0.162763, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.036815, Val Loss: 0.024543, Train MAE: 0.190239, Val MAE: 0.138014, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 35\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 3.\n",
      "INFO:src.pipeline:Fold 3 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.051933\n",
      "INFO:src.pipeline:  rmse: 0.227889\n",
      "INFO:src.pipeline:  mae: 0.147245\n",
      "INFO:src.pipeline:  r2: 0.374139\n",
      "INFO:src.pipeline:  mape: 37.968739\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 4 ===\n",
      "INFO:src.pipeline:Train samples: 101, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.041340, Val Loss: 0.012185, Train MAE: 0.213012, Val MAE: 0.118589, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.034761, Val Loss: 0.017536, Train MAE: 0.182338, Val MAE: 0.151904, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.027251, Val Loss: 0.013135, Train MAE: 0.164802, Val MAE: 0.106954, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 30\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 4.\n",
      "INFO:src.pipeline:Fold 4 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.026269\n",
      "INFO:src.pipeline:  rmse: 0.162078\n",
      "INFO:src.pipeline:  mae: 0.106954\n",
      "INFO:src.pipeline:  r2: 0.436417\n",
      "INFO:src.pipeline:  mape: 15.510724\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 5 ===\n",
      "INFO:src.pipeline:Train samples: 132, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.032011, Val Loss: 0.005425, Train MAE: 0.171786, Val MAE: 0.071881, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.028441, Val Loss: 0.007665, Train MAE: 0.166018, Val MAE: 0.087199, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.022992, Val Loss: 0.005190, Train MAE: 0.145422, Val MAE: 0.064185, LR: 0.000500\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.021471, Val Loss: 0.007628, Train MAE: 0.148713, Val MAE: 0.095986, LR: 0.000500\n",
      "INFO:src.engine:Epoch [50/50] - Train Loss: 0.024071, Val Loss: 0.004941, Train MAE: 0.150342, Val MAE: 0.061780, LR: 0.000250\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 5.\n",
      "INFO:src.pipeline:Fold 5 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.009882\n",
      "INFO:src.pipeline:  rmse: 0.099408\n",
      "INFO:src.pipeline:  mae: 0.061780\n",
      "INFO:src.pipeline:  r2: 0.629755\n",
      "INFO:src.pipeline:  mape: 7.719372\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 6 ===\n",
      "INFO:src.pipeline:Train samples: 162, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.024543, Val Loss: 0.012756, Train MAE: 0.144394, Val MAE: 0.089856, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.019932, Val Loss: 0.013583, Train MAE: 0.123598, Val MAE: 0.081918, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 27\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 6.\n",
      "INFO:src.pipeline:Fold 6 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.027370\n",
      "INFO:src.pipeline:  rmse: 0.165438\n",
      "INFO:src.pipeline:  mae: 0.083476\n",
      "INFO:src.pipeline:  r2: 0.313480\n",
      "INFO:src.pipeline:  mape: 16.761002\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 7 ===\n",
      "INFO:src.pipeline:Train samples: 193, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.024784, Val Loss: 0.023211, Train MAE: 0.153931, Val MAE: 0.119144, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.021530, Val Loss: 0.023971, Train MAE: 0.139480, Val MAE: 0.125381, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 22\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 7.\n",
      "INFO:src.pipeline:Fold 7 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.048810\n",
      "INFO:src.pipeline:  rmse: 0.220929\n",
      "INFO:src.pipeline:  mae: 0.125643\n",
      "INFO:src.pipeline:  r2: 0.312723\n",
      "INFO:src.pipeline:  mape: 35.053028\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 8 ===\n",
      "INFO:src.pipeline:Train samples: 224, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.023238, Val Loss: 0.024657, Train MAE: 0.144381, Val MAE: 0.123621, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.018565, Val Loss: 0.025388, Train MAE: 0.124855, Val MAE: 0.114486, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 25\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 8.\n",
      "INFO:src.pipeline:Fold 8 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.051172\n",
      "INFO:src.pipeline:  rmse: 0.226213\n",
      "INFO:src.pipeline:  mae: 0.122225\n",
      "INFO:src.pipeline:  r2: 0.234647\n",
      "INFO:src.pipeline:  mape: 30.586529\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 9 ===\n",
      "INFO:src.pipeline:Train samples: 254, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.022170, Val Loss: 0.019227, Train MAE: 0.138999, Val MAE: 0.138570, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.018453, Val Loss: 0.015843, Train MAE: 0.122052, Val MAE: 0.128152, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.015057, Val Loss: 0.015702, Train MAE: 0.112460, Val MAE: 0.117966, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.013170, Val Loss: 0.022639, Train MAE: 0.107431, Val MAE: 0.156920, LR: 0.001000\n",
      "INFO:src.engine:Epoch [50/50] - Train Loss: 0.010256, Val Loss: 0.018894, Train MAE: 0.091944, Val MAE: 0.125287, LR: 0.001000\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 9.\n",
      "INFO:src.pipeline:Fold 9 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.037787\n",
      "INFO:src.pipeline:  rmse: 0.194389\n",
      "INFO:src.pipeline:  mae: 0.125287\n",
      "INFO:src.pipeline:  r2: 0.205400\n",
      "INFO:src.pipeline:  mape: 18.512054\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 10 ===\n",
      "INFO:src.pipeline:Train samples: 285, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.020911, Val Loss: 0.035733, Train MAE: 0.134251, Val MAE: 0.192622, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.017886, Val Loss: 0.034550, Train MAE: 0.120741, Val MAE: 0.180675, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.013540, Val Loss: 0.036096, Train MAE: 0.107384, Val MAE: 0.169945, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 34\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 10.\n",
      "INFO:src.pipeline:Fold 10 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.079962\n",
      "INFO:src.pipeline:  rmse: 0.282776\n",
      "INFO:src.pipeline:  mae: 0.171494\n",
      "INFO:src.pipeline:  r2: 0.281518\n",
      "INFO:src.pipeline:  mape: 59.414673\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 11 ===\n",
      "INFO:src.pipeline:Train samples: 315, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.022112, Val Loss: 0.062412, Train MAE: 0.139714, Val MAE: 0.279819, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.018422, Val Loss: 0.066737, Train MAE: 0.119876, Val MAE: 0.278156, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 24\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 11.\n",
      "INFO:src.pipeline:Fold 11 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.162995\n",
      "INFO:src.pipeline:  rmse: 0.403726\n",
      "INFO:src.pipeline:  mae: 0.300785\n",
      "INFO:src.pipeline:  r2: -0.143724\n",
      "INFO:src.pipeline:  mape: 145.011246\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 12 ===\n",
      "INFO:src.pipeline:Train samples: 346, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.026062, Val Loss: 0.035817, Train MAE: 0.160097, Val MAE: 0.198975, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.021081, Val Loss: 0.041863, Train MAE: 0.134085, Val MAE: 0.198278, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 25\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 12.\n",
      "INFO:src.pipeline:Fold 12 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.092260\n",
      "INFO:src.pipeline:  rmse: 0.303744\n",
      "INFO:src.pipeline:  mae: 0.208193\n",
      "INFO:src.pipeline:  r2: -0.012083\n",
      "INFO:src.pipeline:  mape: 40.277283\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 13 ===\n",
      "INFO:src.pipeline:Train samples: 377, Val samples: 26\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.025071, Val Loss: 0.038043, Train MAE: 0.151148, Val MAE: 0.203937, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.020696, Val Loss: 0.044347, Train MAE: 0.135808, Val MAE: 0.221291, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.017169, Val Loss: 0.053981, Train MAE: 0.121402, Val MAE: 0.245385, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 33\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 13.\n",
      "INFO:src.pipeline:Fold 13 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.115924\n",
      "INFO:src.pipeline:  rmse: 0.340476\n",
      "INFO:src.pipeline:  mae: 0.254056\n",
      "INFO:src.pipeline:  r2: 0.010899\n",
      "INFO:src.pipeline:  mape: 86.344238\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 14 ===\n",
      "INFO:src.pipeline:Train samples: 405, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.027117, Val Loss: 0.031416, Train MAE: 0.164126, Val MAE: 0.194538, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.021599, Val Loss: 0.032403, Train MAE: 0.140746, Val MAE: 0.175428, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.017112, Val Loss: 0.035667, Train MAE: 0.126409, Val MAE: 0.184144, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 33\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 14.\n",
      "INFO:src.pipeline:Fold 14 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.084125\n",
      "INFO:src.pipeline:  rmse: 0.290043\n",
      "INFO:src.pipeline:  mae: 0.194729\n",
      "INFO:src.pipeline:  r2: -0.058510\n",
      "INFO:src.pipeline:  mape: 28.850922\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 15 ===\n",
      "INFO:src.pipeline:Train samples: 436, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.025066, Val Loss: 0.022932, Train MAE: 0.155259, Val MAE: 0.119922, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.021058, Val Loss: 0.021219, Train MAE: 0.139992, Val MAE: 0.112089, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.019038, Val Loss: 0.023027, Train MAE: 0.131557, Val MAE: 0.111390, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.013739, Val Loss: 0.024297, Train MAE: 0.107168, Val MAE: 0.112210, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 46\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 15.\n",
      "INFO:src.pipeline:Fold 15 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.049185\n",
      "INFO:src.pipeline:  rmse: 0.221776\n",
      "INFO:src.pipeline:  mae: 0.113555\n",
      "INFO:src.pipeline:  r2: 0.415924\n",
      "INFO:src.pipeline:  mape: 36.367908\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 16 ===\n",
      "INFO:src.pipeline:Train samples: 466, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.025440, Val Loss: 0.018988, Train MAE: 0.157199, Val MAE: 0.125256, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.021348, Val Loss: 0.018732, Train MAE: 0.137328, Val MAE: 0.128543, LR: 0.001000\n",
      "INFO:src.engine:Early stopping at epoch 29\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 16.\n",
      "INFO:src.pipeline:Fold 16 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.041313\n",
      "INFO:src.pipeline:  rmse: 0.203256\n",
      "INFO:src.pipeline:  mae: 0.118935\n",
      "INFO:src.pipeline:  r2: 0.261540\n",
      "INFO:src.pipeline:  mape: 46.266155\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 17 ===\n",
      "INFO:src.pipeline:Train samples: 497, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.024693, Val Loss: 0.018129, Train MAE: 0.150920, Val MAE: 0.099087, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.020585, Val Loss: 0.017854, Train MAE: 0.137614, Val MAE: 0.108257, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 25\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 17.\n",
      "INFO:src.pipeline:Fold 17 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.035963\n",
      "INFO:src.pipeline:  rmse: 0.189638\n",
      "INFO:src.pipeline:  mae: 0.107473\n",
      "INFO:src.pipeline:  r2: 0.387269\n",
      "INFO:src.pipeline:  mape: 26.542166\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 18 ===\n",
      "INFO:src.pipeline:Train samples: 527, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.023795, Val Loss: 0.013677, Train MAE: 0.151435, Val MAE: 0.096766, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.020869, Val Loss: 0.014007, Train MAE: 0.139290, Val MAE: 0.115967, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.016759, Val Loss: 0.021018, Train MAE: 0.119996, Val MAE: 0.141515, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.012789, Val Loss: 0.022286, Train MAE: 0.105171, Val MAE: 0.125612, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 41\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 18.\n",
      "INFO:src.pipeline:Fold 18 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.041552\n",
      "INFO:src.pipeline:  rmse: 0.203844\n",
      "INFO:src.pipeline:  mae: 0.123599\n",
      "INFO:src.pipeline:  r2: 0.039030\n",
      "INFO:src.pipeline:  mape: 19.685352\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 19 ===\n",
      "INFO:src.pipeline:Train samples: 558, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.023898, Val Loss: 0.026374, Train MAE: 0.149526, Val MAE: 0.125620, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.020730, Val Loss: 0.026389, Train MAE: 0.137913, Val MAE: 0.121267, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 25\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 19.\n",
      "INFO:src.pipeline:Fold 19 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.055387\n",
      "INFO:src.pipeline:  rmse: 0.235343\n",
      "INFO:src.pipeline:  mae: 0.117671\n",
      "INFO:src.pipeline:  r2: 0.297785\n",
      "INFO:src.pipeline:  mape: 12.635724\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 20 ===\n",
      "INFO:src.pipeline:Train samples: 589, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.023054, Val Loss: 0.018885, Train MAE: 0.143848, Val MAE: 0.138909, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.020041, Val Loss: 0.020721, Train MAE: 0.129904, Val MAE: 0.154245, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 27\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 20.\n",
      "INFO:src.pipeline:Fold 20 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.048630\n",
      "INFO:src.pipeline:  rmse: 0.220521\n",
      "INFO:src.pipeline:  mae: 0.152749\n",
      "INFO:src.pipeline:  r2: 0.259879\n",
      "INFO:src.pipeline:  mape: 25.736025\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 21 ===\n",
      "INFO:src.pipeline:Train samples: 619, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.023482, Val Loss: 0.018673, Train MAE: 0.148472, Val MAE: 0.128078, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.019580, Val Loss: 0.022576, Train MAE: 0.129625, Val MAE: 0.149954, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 28\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 21.\n",
      "INFO:src.pipeline:Fold 21 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.041040\n",
      "INFO:src.pipeline:  rmse: 0.202582\n",
      "INFO:src.pipeline:  mae: 0.121127\n",
      "INFO:src.pipeline:  r2: 0.149610\n",
      "INFO:src.pipeline:  mape: 18.595581\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 22 ===\n",
      "INFO:src.pipeline:Train samples: 650, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.022920, Val Loss: 0.038257, Train MAE: 0.144673, Val MAE: 0.186678, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.020057, Val Loss: 0.035428, Train MAE: 0.135215, Val MAE: 0.140749, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.016928, Val Loss: 0.040013, Train MAE: 0.121133, Val MAE: 0.177582, LR: 0.001000\n",
      "INFO:src.engine:Early stopping at epoch 39\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 22.\n",
      "INFO:src.pipeline:Fold 22 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.077485\n",
      "INFO:src.pipeline:  rmse: 0.278362\n",
      "INFO:src.pipeline:  mae: 0.174660\n",
      "INFO:src.pipeline:  r2: 0.077387\n",
      "INFO:src.pipeline:  mape: 48.758240\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 23 ===\n",
      "INFO:src.pipeline:Train samples: 680, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.024596, Val Loss: 0.050256, Train MAE: 0.147254, Val MAE: 0.243004, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.020791, Val Loss: 0.050167, Train MAE: 0.135936, Val MAE: 0.239642, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.016149, Val Loss: 0.048597, Train MAE: 0.117114, Val MAE: 0.228210, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 31\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 23.\n",
      "INFO:src.pipeline:Fold 23 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.106905\n",
      "INFO:src.pipeline:  rmse: 0.326964\n",
      "INFO:src.pipeline:  mae: 0.240022\n",
      "INFO:src.pipeline:  r2: 0.053106\n",
      "INFO:src.pipeline:  mape: 83.487206\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 24 ===\n",
      "INFO:src.pipeline:Train samples: 711, Val samples: 21\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.025610, Val Loss: 0.066743, Train MAE: 0.155347, Val MAE: 0.283975, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.022021, Val Loss: 0.065962, Train MAE: 0.140452, Val MAE: 0.275898, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 22\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 24.\n",
      "INFO:src.pipeline:Fold 24 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.139415\n",
      "INFO:src.pipeline:  rmse: 0.373384\n",
      "INFO:src.pipeline:  mae: 0.278496\n",
      "INFO:src.pipeline:  r2: 0.095904\n",
      "INFO:src.pipeline:  mape: 83.882530\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 25 ===\n",
      "INFO:src.pipeline:Train samples: 734, Val samples: 23\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.026429, Val Loss: 0.053366, Train MAE: 0.158225, Val MAE: 0.227215, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.023021, Val Loss: 0.062348, Train MAE: 0.142145, Val MAE: 0.240848, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 23\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 25.\n",
      "INFO:src.pipeline:Fold 25 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.140074\n",
      "INFO:src.pipeline:  rmse: 0.374265\n",
      "INFO:src.pipeline:  mae: 0.259929\n",
      "INFO:src.pipeline:  r2: -0.159540\n",
      "INFO:src.pipeline:  mape: 24.705645\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 26 ===\n",
      "INFO:src.pipeline:Train samples: 759, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.027090, Val Loss: 0.039704, Train MAE: 0.159943, Val MAE: 0.218598, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.023477, Val Loss: 0.042245, Train MAE: 0.146917, Val MAE: 0.201930, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.018832, Val Loss: 0.037276, Train MAE: 0.127910, Val MAE: 0.203150, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.012980, Val Loss: 0.043894, Train MAE: 0.109903, Val MAE: 0.208270, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 44\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 26.\n",
      "INFO:src.pipeline:Fold 26 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.090278\n",
      "INFO:src.pipeline:  rmse: 0.300462\n",
      "INFO:src.pipeline:  mae: 0.208887\n",
      "INFO:src.pipeline:  r2: 0.364613\n",
      "INFO:src.pipeline:  mape: 116.385391\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 27 ===\n",
      "INFO:src.pipeline:Train samples: 790, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.027537, Val Loss: 0.031697, Train MAE: 0.162990, Val MAE: 0.163306, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.025005, Val Loss: 0.033719, Train MAE: 0.151787, Val MAE: 0.169065, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.019546, Val Loss: 0.035290, Train MAE: 0.132061, Val MAE: 0.169893, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 31\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 27.\n",
      "INFO:src.pipeline:Fold 27 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.075868\n",
      "INFO:src.pipeline:  rmse: 0.275441\n",
      "INFO:src.pipeline:  mae: 0.184951\n",
      "INFO:src.pipeline:  r2: 0.300455\n",
      "INFO:src.pipeline:  mape: 58.884914\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 28 ===\n",
      "INFO:src.pipeline:Train samples: 820, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.028162, Val Loss: 0.022614, Train MAE: 0.167958, Val MAE: 0.144288, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.025330, Val Loss: 0.021524, Train MAE: 0.152232, Val MAE: 0.134927, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.021057, Val Loss: 0.029954, Train MAE: 0.136602, Val MAE: 0.151922, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.015160, Val Loss: 0.034983, Train MAE: 0.116106, Val MAE: 0.163015, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 42\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 28.\n",
      "INFO:src.pipeline:Fold 28 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.068335\n",
      "INFO:src.pipeline:  rmse: 0.261409\n",
      "INFO:src.pipeline:  mae: 0.165170\n",
      "INFO:src.pipeline:  r2: 0.176319\n",
      "INFO:src.pipeline:  mape: 45.339687\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 29 ===\n",
      "INFO:src.pipeline:Train samples: 851, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.026917, Val Loss: 0.008809, Train MAE: 0.160681, Val MAE: 0.105148, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.023459, Val Loss: 0.007609, Train MAE: 0.146850, Val MAE: 0.089162, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.018754, Val Loss: 0.008308, Train MAE: 0.130391, Val MAE: 0.101885, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.014711, Val Loss: 0.007193, Train MAE: 0.113867, Val MAE: 0.075597, LR: 0.001000\n",
      "INFO:src.engine:Epoch [50/50] - Train Loss: 0.009984, Val Loss: 0.007337, Train MAE: 0.094417, Val MAE: 0.072147, LR: 0.000500\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 29.\n",
      "INFO:src.pipeline:Fold 29 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.014674\n",
      "INFO:src.pipeline:  rmse: 0.121136\n",
      "INFO:src.pipeline:  mae: 0.072147\n",
      "INFO:src.pipeline:  r2: 0.642620\n",
      "INFO:src.pipeline:  mape: 10.886256\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 30 ===\n",
      "INFO:src.pipeline:Train samples: 881, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.027031, Val Loss: 0.020538, Train MAE: 0.161018, Val MAE: 0.102005, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.022975, Val Loss: 0.020714, Train MAE: 0.144655, Val MAE: 0.083796, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.017573, Val Loss: 0.020244, Train MAE: 0.125764, Val MAE: 0.083106, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 38\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 30.\n",
      "INFO:src.pipeline:Fold 30 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.040924\n",
      "INFO:src.pipeline:  rmse: 0.202296\n",
      "INFO:src.pipeline:  mae: 0.084907\n",
      "INFO:src.pipeline:  r2: 0.287544\n",
      "INFO:src.pipeline:  mape: 5.047194\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 31 ===\n",
      "INFO:src.pipeline:Train samples: 911, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.026799, Val Loss: 0.004248, Train MAE: 0.158686, Val MAE: 0.042042, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.024198, Val Loss: 0.004084, Train MAE: 0.144086, Val MAE: 0.050532, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.018119, Val Loss: 0.004454, Train MAE: 0.123523, Val MAE: 0.053664, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.013398, Val Loss: 0.003947, Train MAE: 0.108609, Val MAE: 0.044633, LR: 0.000500\n",
      "INFO:src.engine:Epoch [50/50] - Train Loss: 0.011809, Val Loss: 0.003892, Train MAE: 0.099917, Val MAE: 0.043993, LR: 0.000500\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 31.\n",
      "INFO:src.pipeline:Fold 31 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.007784\n",
      "INFO:src.pipeline:  rmse: 0.088227\n",
      "INFO:src.pipeline:  mae: 0.043993\n",
      "INFO:src.pipeline:  r2: 0.799588\n",
      "INFO:src.pipeline:  mape: 6.177649\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 32 ===\n",
      "INFO:src.pipeline:Train samples: 942, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.025809, Val Loss: 0.010248, Train MAE: 0.152514, Val MAE: 0.060743, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.023822, Val Loss: 0.010146, Train MAE: 0.144593, Val MAE: 0.087108, LR: 0.001000\n",
      "INFO:src.engine:Early stopping at epoch 29\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 32.\n",
      "INFO:src.pipeline:Fold 32 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.021876\n",
      "INFO:src.pipeline:  rmse: 0.147905\n",
      "INFO:src.pipeline:  mae: 0.075413\n",
      "INFO:src.pipeline:  r2: 0.446691\n",
      "INFO:src.pipeline:  mape: 10.177034\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 33 ===\n",
      "INFO:src.pipeline:Train samples: 972, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.024814, Val Loss: 0.051907, Train MAE: 0.146493, Val MAE: 0.228969, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.021571, Val Loss: 0.056872, Train MAE: 0.132775, Val MAE: 0.234370, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 27\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 33.\n",
      "INFO:src.pipeline:Fold 33 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.132702\n",
      "INFO:src.pipeline:  rmse: 0.364283\n",
      "INFO:src.pipeline:  mae: 0.231969\n",
      "INFO:src.pipeline:  r2: -0.020023\n",
      "INFO:src.pipeline:  mape: 107.300316\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 34 ===\n",
      "INFO:src.pipeline:Train samples: 1003, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.026175, Val Loss: 0.036864, Train MAE: 0.155132, Val MAE: 0.217704, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.023404, Val Loss: 0.039472, Train MAE: 0.142540, Val MAE: 0.186458, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.018804, Val Loss: 0.051317, Train MAE: 0.124569, Val MAE: 0.238508, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 31\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 34.\n",
      "INFO:src.pipeline:Fold 34 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.116114\n",
      "INFO:src.pipeline:  rmse: 0.340756\n",
      "INFO:src.pipeline:  mae: 0.252805\n",
      "INFO:src.pipeline:  r2: -0.149029\n",
      "INFO:src.pipeline:  mape: 69.719948\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 35 ===\n",
      "INFO:src.pipeline:Train samples: 1033, Val samples: 21\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.026418, Val Loss: 0.084932, Train MAE: 0.154451, Val MAE: 0.309301, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.023207, Val Loss: 0.094938, Train MAE: 0.142987, Val MAE: 0.326713, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 21\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 35.\n",
      "INFO:src.pipeline:Fold 35 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.190529\n",
      "INFO:src.pipeline:  rmse: 0.436497\n",
      "INFO:src.pipeline:  mae: 0.343321\n",
      "INFO:src.pipeline:  r2: -0.194370\n",
      "INFO:src.pipeline:  mape: 63.040531\n",
      "INFO:src.pipeline:\n",
      "=== Experiment Summary ===\n",
      "INFO:src.pipeline:Average Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.074826 ± 0.047808\n",
      "INFO:src.pipeline:  rmse: 0.258599 ± 0.089179\n",
      "INFO:src.pipeline:  mae: 0.170743 ± 0.078700\n",
      "INFO:src.pipeline:  r2: 0.198974 ± 0.240384\n",
      "INFO:src.pipeline:  mape: 45.116521 ± 33.649637\n"
     ]
    }
   ],
   "source": [
    "from src.preprocessing import KBinConfig, process_splits_to_kbins, build_model_arrays, to_fixedgrid_multiindex\n",
    "import pandas as pd\n",
    "from src.utils import DataManager\n",
    "from src.pipeline import SolarForecastingPipeline\n",
    "\n",
    "\n",
    "df_phase5 = df_interpolated[['CSI_ghi', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos',\n",
    "     'nam_cc',\n",
    "        '80_cloud_cover',\n",
    "       '56_cloud_cover', '20_cloud_cover',\n",
    "       '88_cloud_cover']]\n",
    "\n",
    "\n",
    "# --- Block 1: Define variables ---\n",
    "USE_KBINS = False\n",
    "k_bins = 60\n",
    "TARGET_COL = \"CSI_ghi\"  # <-- FIX 1: Define Target FIRST\n",
    "history_days = 7\n",
    "horizon_days = 1\n",
    "\n",
    "# This list now correctly excludes the target\n",
    "feature_cols = [c for c in df_phase5.columns.tolist() if c != TARGET_COL]\n",
    "\n",
    "# --- Block 2: Build model arrays (Your code was correct here) ---\n",
    "if USE_KBINS:\n",
    "        print(\"\\n--- Step 2: Converting full dataset to K-Bins format ---\")\n",
    "        # ... (rest of your K-Bins code) ...\n",
    "else:\n",
    "    print(\"\\n--- Step 2: Building model arrays (X, Y) ---\")\n",
    "    fixed_df = to_fixedgrid_multiindex(df_phase5, timestamp_col=\"measurement_time\", expected_T=None)  # or set T\n",
    "    \n",
    "    # You correctly passed the filtered 'feature_cols' here, fixing the data leak!\n",
    "    ph5_X, ph5_Y, ph5_labels_list = build_model_arrays(\n",
    "        fixed_df,\n",
    "        feature_cols=feature_cols,  # <-- This is correct!\n",
    "        target_col=TARGET_COL,\n",
    "        history_days=history_days,\n",
    "        horizon_days=horizon_days,\n",
    "    )\n",
    "\n",
    "# --- Block 3: Save arrays ---\n",
    "data_manager = DataManager()\n",
    "data_manager.save_arrays(\n",
    "    ph5_X, ph5_Y,\n",
    "    pd.DataFrame(index=pd.to_datetime(ph5_labels_list, utc=True)),\n",
    "    filename_prefix='phas5_data',  # <-- You are saving as 'phas2_data'\n",
    "    feature_cols=feature_cols,\n",
    "    target_col=TARGET_COL,\n",
    "    metadata={\n",
    "        \"input_csv\": \"data/processed/dayTime_NAM_spatial_5locations_dayahead_features_processed.csv\",\n",
    "        \"timestamp_col\": \"measurement_time\",\n",
    "        \"feature_set\": feature_cols,\n",
    "        \"history_days\": 7,\n",
    "        \"horizon_days\": 1,\n",
    "        \"k_bins\": None,\n",
    "    }\n",
    ")\n",
    "\n",
    "# --- Block 4: Configure and Run ---\n",
    "LSTM_CONFIG = {\n",
    "    \"experiment_name\": \"LSTM_phase5_exp02\",\n",
    "    \"model_type\": \"LSTM\",\n",
    "    \"model_config\": {\n",
    "        \"hidden_size\": 64,\n",
    "        \"num_layers\": 2,\n",
    "        \"dropout\": 0.25,\n",
    "        \"bidirectional\": True,\n",
    "    },\n",
    "    \"data_prefix\": \"phas5_data\",  # <-- FIX 2: Match the filename_prefix\n",
    "    \"splits_file\": \"exp-003/exp-003rolling_origin_splits.json\",\n",
    "    \"feature_cols\": feature_cols,\n",
    "    \"feature_selection\": feature_cols, # <-- This is correct!\n",
    "    \"target_col\": TARGET_COL,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_epochs\": 50,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"loss_function\": \"Huber\",  # <-- This setting needs the fix below\n",
    "    \"early_stopping_patience\": 20,\n",
    "    \"max_folds\": 35,\n",
    "}\n",
    "\n",
    "# This will now work after you apply the fixes below\n",
    "pipeline = SolarForecastingPipeline(LSTM_CONFIG)\n",
    "phas5_fold_results, summary = pipeline.run()\n",
    "\n",
    "# To get the model from the LAST fold:\n",
    "phas5_fold_results = phas3_fold_results[-1]['model']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a65f76",
   "metadata": {},
   "source": [
    "Inconsistent Results Across Folds: When adding the spatial-temporal features, the model shows considerable variation in performance across the folds. Some folds perform worse (e.g., Fold 3: RMSE 0.2124 to 0.466), while others perform better (e.g., Fold 31: RMSE 0.0918). This suggests that the added features may be contributing noise or irrelevant complexity, which the model is struggling to handle uniformly across different subsets of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc804f53",
   "metadata": {},
   "source": [
    "The higher variance in metrics across folds, especially in terms of RMSE and MAE, indicates that the model is not stable with the added features. This suggests that while the added features may hold some information, their inclusion doesn’t consistently improve predictive accuracy across all training and validation splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1978bd01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'history': {'train_loss': [0.06003684177994728,\n",
       "    0.059081628918647766,\n",
       "    0.05886007845401764,\n",
       "    0.05694744363427162,\n",
       "    0.0553533211350441,\n",
       "    0.053611330687999725,\n",
       "    0.05284370481967926,\n",
       "    0.052881281822919846,\n",
       "    0.05291888490319252,\n",
       "    0.05297527462244034,\n",
       "    0.04997491464018822,\n",
       "    0.053134702146053314,\n",
       "    0.04904967173933983,\n",
       "    0.0478777177631855,\n",
       "    0.047167930752038956,\n",
       "    0.046798259019851685,\n",
       "    0.04694787412881851,\n",
       "    0.04371234029531479,\n",
       "    0.044653840363025665,\n",
       "    0.0406917929649353,\n",
       "    0.04292237013578415,\n",
       "    0.03734507039189339,\n",
       "    0.03371132165193558,\n",
       "    0.030639274045825005,\n",
       "    0.03304049372673035,\n",
       "    0.029997609555721283,\n",
       "    0.02925606444478035,\n",
       "    0.02697451412677765,\n",
       "    0.02603142336010933,\n",
       "    0.026743434369564056,\n",
       "    0.024257680401206017,\n",
       "    0.02271345816552639,\n",
       "    0.02418750710785389,\n",
       "    0.028082501143217087,\n",
       "    0.02268541231751442,\n",
       "    0.018612494692206383,\n",
       "    0.022291239351034164,\n",
       "    0.023941392078995705,\n",
       "    0.020655987784266472,\n",
       "    0.020717311650514603,\n",
       "    0.02025049552321434,\n",
       "    0.017783040180802345,\n",
       "    0.017522064968943596,\n",
       "    0.016095982864499092],\n",
       "   'val_loss': [0.09866582602262497,\n",
       "    0.09783302992582321,\n",
       "    0.09704198688268661,\n",
       "    0.09622205793857574,\n",
       "    0.09525701403617859,\n",
       "    0.09400943666696548,\n",
       "    0.09247295558452606,\n",
       "    0.09080058336257935,\n",
       "    0.09004609286785126,\n",
       "    0.08985322713851929,\n",
       "    0.08958656340837479,\n",
       "    0.08893394470214844,\n",
       "    0.0876365453004837,\n",
       "    0.08585827052593231,\n",
       "    0.08383392542600632,\n",
       "    0.08183281123638153,\n",
       "    0.07995512336492538,\n",
       "    0.07813270390033722,\n",
       "    0.07615675032138824,\n",
       "    0.07389504462480545,\n",
       "    0.07243393361568451,\n",
       "    0.0716053918004036,\n",
       "    0.07131533324718475,\n",
       "    0.0707559660077095,\n",
       "    0.07136493921279907,\n",
       "    0.07293448597192764,\n",
       "    0.07492406666278839,\n",
       "    0.07554207742214203,\n",
       "    0.07624770700931549,\n",
       "    0.0768333226442337,\n",
       "    0.07742251455783844,\n",
       "    0.07785854488611221,\n",
       "    0.0777415931224823,\n",
       "    0.07727716118097305,\n",
       "    0.07747605443000793,\n",
       "    0.0775834396481514,\n",
       "    0.07723426073789597,\n",
       "    0.076830193400383,\n",
       "    0.07651925086975098,\n",
       "    0.07616252452135086,\n",
       "    0.07594573497772217,\n",
       "    0.07584536075592041,\n",
       "    0.07581766694784164,\n",
       "    0.07584173232316971],\n",
       "   'train_mae': [0.25549373030662537,\n",
       "    0.2526325583457947,\n",
       "    0.2500351369380951,\n",
       "    0.24755486845970154,\n",
       "    0.24418604373931885,\n",
       "    0.2399939000606537,\n",
       "    0.23473094403743744,\n",
       "    0.2322060614824295,\n",
       "    0.22859464585781097,\n",
       "    0.22573912143707275,\n",
       "    0.21825933456420898,\n",
       "    0.21635816991329193,\n",
       "    0.20472830533981323,\n",
       "    0.20761503279209137,\n",
       "    0.2087285965681076,\n",
       "    0.207489475607872,\n",
       "    0.21232958137989044,\n",
       "    0.2051735371351242,\n",
       "    0.20266088843345642,\n",
       "    0.1884879469871521,\n",
       "    0.19794081151485443,\n",
       "    0.1818520724773407,\n",
       "    0.17304964363574982,\n",
       "    0.16448339819908142,\n",
       "    0.17378948628902435,\n",
       "    0.16322527825832367,\n",
       "    0.15957869589328766,\n",
       "    0.14493748545646667,\n",
       "    0.14037805795669556,\n",
       "    0.155052050948143,\n",
       "    0.1530487984418869,\n",
       "    0.13856366276741028,\n",
       "    0.13783572614192963,\n",
       "    0.1517500877380371,\n",
       "    0.12785358726978302,\n",
       "    0.12316443771123886,\n",
       "    0.14382076263427734,\n",
       "    0.15341205894947052,\n",
       "    0.1411556452512741,\n",
       "    0.14014917612075806,\n",
       "    0.13562089204788208,\n",
       "    0.13312149047851562,\n",
       "    0.12745091319084167,\n",
       "    0.12759055197238922],\n",
       "   'val_mae': [0.34586161375045776,\n",
       "    0.3436753749847412,\n",
       "    0.341677188873291,\n",
       "    0.3396679162979126,\n",
       "    0.3373381793498993,\n",
       "    0.33454081416130066,\n",
       "    0.33103838562965393,\n",
       "    0.32730862498283386,\n",
       "    0.32510969042778015,\n",
       "    0.32424503564834595,\n",
       "    0.32335856556892395,\n",
       "    0.32256025075912476,\n",
       "    0.32122644782066345,\n",
       "    0.3197256028652191,\n",
       "    0.3179199993610382,\n",
       "    0.3158930242061615,\n",
       "    0.31352531909942627,\n",
       "    0.31072312593460083,\n",
       "    0.3075943887233734,\n",
       "    0.3034736216068268,\n",
       "    0.3003136217594147,\n",
       "    0.2984689474105835,\n",
       "    0.2979101240634918,\n",
       "    0.29738444089889526,\n",
       "    0.298634797334671,\n",
       "    0.3008671700954437,\n",
       "    0.303356409072876,\n",
       "    0.30548664927482605,\n",
       "    0.30866101384162903,\n",
       "    0.31142929196357727,\n",
       "    0.31362059712409973,\n",
       "    0.3148425817489624,\n",
       "    0.3154776990413666,\n",
       "    0.3161035478115082,\n",
       "    0.31795090436935425,\n",
       "    0.318742573261261,\n",
       "    0.3192324638366699,\n",
       "    0.3193384110927582,\n",
       "    0.3192938566207886,\n",
       "    0.31937769055366516,\n",
       "    0.3196190595626831,\n",
       "    0.319476842880249,\n",
       "    0.3195214569568634,\n",
       "    0.31963685154914856],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005]},\n",
       "  'metrics': {'mse': 0.15170642733573914,\n",
       "   'rmse': 0.3894950928262629,\n",
       "   'mae': 0.31963688135147095,\n",
       "   'r2': 0.08344894647598267,\n",
       "   'mape': 81.203369140625},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.06416858918964863,\n",
       "    0.07969227805733681,\n",
       "    0.0666227675974369,\n",
       "    0.08474689908325672,\n",
       "    0.0561613105237484,\n",
       "    0.056837914511561394,\n",
       "    0.06758140213787556,\n",
       "    0.08036716282367706,\n",
       "    0.062167927622795105,\n",
       "    0.0512021966278553,\n",
       "    0.05090458132326603,\n",
       "    0.053224340081214905,\n",
       "    0.04525951109826565,\n",
       "    0.055910708382725716,\n",
       "    0.053750403225421906,\n",
       "    0.0373664814978838,\n",
       "    0.03630437422543764,\n",
       "    0.03898312710225582,\n",
       "    0.040067607536911964,\n",
       "    0.046912044286727905,\n",
       "    0.03607586771249771,\n",
       "    0.04244311526417732,\n",
       "    0.023997285403311253,\n",
       "    0.028372149914503098,\n",
       "    0.026159613393247128,\n",
       "    0.042973555624485016,\n",
       "    0.02707718964666128,\n",
       "    0.03270685952156782,\n",
       "    0.03344408329576254,\n",
       "    0.025278820656239986,\n",
       "    0.02585465833544731,\n",
       "    0.026773061137646437,\n",
       "    0.021030688658356667,\n",
       "    0.028014271520078182,\n",
       "    0.02305351523682475,\n",
       "    0.02964208461344242,\n",
       "    0.026332699693739414,\n",
       "    0.02940695732831955,\n",
       "    0.03351951204240322,\n",
       "    0.0296437107026577],\n",
       "   'val_loss': [0.05770126357674599,\n",
       "    0.05472533404827118,\n",
       "    0.05219636857509613,\n",
       "    0.04983317852020264,\n",
       "    0.04911147058010101,\n",
       "    0.050200074911117554,\n",
       "    0.051478222012519836,\n",
       "    0.05427861958742142,\n",
       "    0.06124148145318031,\n",
       "    0.0632871761918068,\n",
       "    0.05804600939154625,\n",
       "    0.05443418025970459,\n",
       "    0.05093630030751228,\n",
       "    0.04634124040603638,\n",
       "    0.043015312403440475,\n",
       "    0.0418689101934433,\n",
       "    0.04119621589779854,\n",
       "    0.04024923965334892,\n",
       "    0.039197470992803574,\n",
       "    0.03861423209309578,\n",
       "    0.04135281220078468,\n",
       "    0.04236840829253197,\n",
       "    0.040372297167778015,\n",
       "    0.041555255651474,\n",
       "    0.04504774138331413,\n",
       "    0.043831419199705124,\n",
       "    0.05255390331149101,\n",
       "    0.07479143142700195,\n",
       "    0.0599331371486187,\n",
       "    0.0424746572971344,\n",
       "    0.039890535175800323,\n",
       "    0.039595093578100204,\n",
       "    0.039421506226062775,\n",
       "    0.04037174955010414,\n",
       "    0.04292965307831764,\n",
       "    0.04598085954785347,\n",
       "    0.048347704112529755,\n",
       "    0.05019117146730423,\n",
       "    0.051353584975004196,\n",
       "    0.0519736222922802],\n",
       "   'train_mae': [0.2923143729567528,\n",
       "    0.32878510653972626,\n",
       "    0.2953139841556549,\n",
       "    0.3252017945051193,\n",
       "    0.2705688774585724,\n",
       "    0.27762259542942047,\n",
       "    0.29423779249191284,\n",
       "    0.3110828995704651,\n",
       "    0.284056693315506,\n",
       "    0.2576928362250328,\n",
       "    0.2559037059545517,\n",
       "    0.2578763887286186,\n",
       "    0.2428547963500023,\n",
       "    0.2577321529388428,\n",
       "    0.2614152431488037,\n",
       "    0.20079955458641052,\n",
       "    0.1972206011414528,\n",
       "    0.19835620373487473,\n",
       "    0.20157355070114136,\n",
       "    0.2081657275557518,\n",
       "    0.1843796968460083,\n",
       "    0.2048756405711174,\n",
       "    0.15231547504663467,\n",
       "    0.15501072257757187,\n",
       "    0.15913739055395126,\n",
       "    0.19811926037073135,\n",
       "    0.16657983511686325,\n",
       "    0.18853173404932022,\n",
       "    0.19190648943185806,\n",
       "    0.1504637636244297,\n",
       "    0.16090521216392517,\n",
       "    0.1536230742931366,\n",
       "    0.1414262279868126,\n",
       "    0.15680257976055145,\n",
       "    0.1500435173511505,\n",
       "    0.16520658135414124,\n",
       "    0.15802084654569626,\n",
       "    0.16667181998491287,\n",
       "    0.16843216121196747,\n",
       "    0.17044758796691895],\n",
       "   'val_mae': [0.2868775725364685,\n",
       "    0.2806684076786041,\n",
       "    0.27602115273475647,\n",
       "    0.2720817029476166,\n",
       "    0.2736457288265228,\n",
       "    0.27736896276474,\n",
       "    0.2802140414714813,\n",
       "    0.28911304473876953,\n",
       "    0.3062623143196106,\n",
       "    0.31196656823158264,\n",
       "    0.3022357225418091,\n",
       "    0.2944808900356293,\n",
       "    0.28495097160339355,\n",
       "    0.2685377597808838,\n",
       "    0.25251254439353943,\n",
       "    0.2461490035057068,\n",
       "    0.24153216183185577,\n",
       "    0.2397625893354416,\n",
       "    0.2358567714691162,\n",
       "    0.22994962334632874,\n",
       "    0.2504742443561554,\n",
       "    0.2543186545372009,\n",
       "    0.2363959103822708,\n",
       "    0.21877549588680267,\n",
       "    0.21122314035892487,\n",
       "    0.2264237403869629,\n",
       "    0.2958929240703583,\n",
       "    0.35477927327156067,\n",
       "    0.31692442297935486,\n",
       "    0.24909375607967377,\n",
       "    0.21901075541973114,\n",
       "    0.21699871122837067,\n",
       "    0.2215651273727417,\n",
       "    0.23429571092128754,\n",
       "    0.2496785670518875,\n",
       "    0.2638782262802124,\n",
       "    0.2735165059566498,\n",
       "    0.28047996759414673,\n",
       "    0.2846301198005676,\n",
       "    0.28704410791397095],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005]},\n",
       "  'metrics': {'mse': 0.1039472445845604,\n",
       "   'rmse': 0.32240850575715335,\n",
       "   'mae': 0.28704413771629333,\n",
       "   'r2': 0.06550604104995728,\n",
       "   'mape': 72.9692611694336},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.06369752809405327,\n",
       "    0.07744146262605985,\n",
       "    0.06402349720398585,\n",
       "    0.06087345505754153,\n",
       "    0.06169188519318899,\n",
       "    0.0520206056535244,\n",
       "    0.05693471555908521,\n",
       "    0.049217390517393746,\n",
       "    0.05393077557285627,\n",
       "    0.04715893045067787,\n",
       "    0.04276937618851662,\n",
       "    0.044055952380100884,\n",
       "    0.039159788439671196,\n",
       "    0.049592221776644387,\n",
       "    0.040695808827877045,\n",
       "    0.04207648212711016,\n",
       "    0.03720266496141752,\n",
       "    0.040908707305788994,\n",
       "    0.032485129311680794,\n",
       "    0.038348520174622536,\n",
       "    0.031610346088806786,\n",
       "    0.03224086823562781,\n",
       "    0.04427427612245083,\n",
       "    0.03330689171950022,\n",
       "    0.040789367631077766,\n",
       "    0.03371818115313848,\n",
       "    0.03509928286075592,\n",
       "    0.03758615938325723,\n",
       "    0.03248435507218043,\n",
       "    0.038603742296497025,\n",
       "    0.028535325701038044,\n",
       "    0.02575154198954503,\n",
       "    0.028357392797867458,\n",
       "    0.026571953979631264,\n",
       "    0.033586585273345314,\n",
       "    0.03049057846268018,\n",
       "    0.026669050256411236,\n",
       "    0.030614705756306648,\n",
       "    0.0325519305964311,\n",
       "    0.032307639718055725,\n",
       "    0.025204307089249294,\n",
       "    0.03362979739904404,\n",
       "    0.032335774352153145,\n",
       "    0.024072018762429554,\n",
       "    0.031030116602778435,\n",
       "    0.02153863540540139,\n",
       "    0.026032570128639538,\n",
       "    0.02774816316862901,\n",
       "    0.024901308119297028,\n",
       "    0.02275252901017666],\n",
       "   'val_loss': [0.03658197447657585,\n",
       "    0.03158765658736229,\n",
       "    0.034342531114816666,\n",
       "    0.03203785791993141,\n",
       "    0.03183828666806221,\n",
       "    0.032375551760196686,\n",
       "    0.03192136809229851,\n",
       "    0.03172764927148819,\n",
       "    0.03001077100634575,\n",
       "    0.027561431750655174,\n",
       "    0.025750601664185524,\n",
       "    0.025331886485219002,\n",
       "    0.024942705407738686,\n",
       "    0.024731682613492012,\n",
       "    0.02464938350021839,\n",
       "    0.02447652444243431,\n",
       "    0.025317270308732986,\n",
       "    0.026787275448441505,\n",
       "    0.02890566736459732,\n",
       "    0.028956886380910873,\n",
       "    0.026645086705684662,\n",
       "    0.026170717552304268,\n",
       "    0.025701096281409264,\n",
       "    0.025867247954010963,\n",
       "    0.024634521454572678,\n",
       "    0.022573895752429962,\n",
       "    0.02245994098484516,\n",
       "    0.023611988872289658,\n",
       "    0.023552898317575455,\n",
       "    0.02218475006520748,\n",
       "    0.02161099761724472,\n",
       "    0.023157592862844467,\n",
       "    0.025325192138552666,\n",
       "    0.02208220399916172,\n",
       "    0.02142888866364956,\n",
       "    0.022196335718035698,\n",
       "    0.022562937811017036,\n",
       "    0.02206822670996189,\n",
       "    0.0230450090020895,\n",
       "    0.02481280453503132,\n",
       "    0.02329591102898121,\n",
       "    0.021408403292298317,\n",
       "    0.021750353276729584,\n",
       "    0.021424051374197006,\n",
       "    0.022249091416597366,\n",
       "    0.024741575121879578,\n",
       "    0.021469630300998688,\n",
       "    0.021841678768396378,\n",
       "    0.02159024216234684,\n",
       "    0.020742444321513176],\n",
       "   'train_mae': [0.29284321268399555,\n",
       "    0.3257165352503459,\n",
       "    0.2936607201894124,\n",
       "    0.28242552280426025,\n",
       "    0.2896217455466588,\n",
       "    0.27002479135990143,\n",
       "    0.27663464347521466,\n",
       "    0.261147677898407,\n",
       "    0.2631320208311081,\n",
       "    0.2484229157368342,\n",
       "    0.21950483322143555,\n",
       "    0.22756234804789224,\n",
       "    0.20533939202626547,\n",
       "    0.2370487799247106,\n",
       "    0.21122084061304727,\n",
       "    0.2127391199270884,\n",
       "    0.20174129803975424,\n",
       "    0.2231883406639099,\n",
       "    0.190581684311231,\n",
       "    0.19188280403614044,\n",
       "    0.1815310368935267,\n",
       "    0.17870967586835226,\n",
       "    0.20798506836096445,\n",
       "    0.19359743098417917,\n",
       "    0.21632833778858185,\n",
       "    0.20602128406365713,\n",
       "    0.20531010627746582,\n",
       "    0.20720531046390533,\n",
       "    0.18099225064118704,\n",
       "    0.18554587165514627,\n",
       "    0.17121302088101706,\n",
       "    0.17147532105445862,\n",
       "    0.17069107294082642,\n",
       "    0.16752955317497253,\n",
       "    0.19098603228727976,\n",
       "    0.18174557387828827,\n",
       "    0.1637940208117167,\n",
       "    0.17351782321929932,\n",
       "    0.17335829635461172,\n",
       "    0.17731441060702005,\n",
       "    0.15921339889367422,\n",
       "    0.185105433066686,\n",
       "    0.18286069730917612,\n",
       "    0.16860567529996237,\n",
       "    0.1810975025097529,\n",
       "    0.1521578406294187,\n",
       "    0.15647739171981812,\n",
       "    0.16324839492638907,\n",
       "    0.16559998691082,\n",
       "    0.15965413053830466],\n",
       "   'val_mae': [0.22556151449680328,\n",
       "    0.21659345924854279,\n",
       "    0.21892203390598297,\n",
       "    0.2183350920677185,\n",
       "    0.21460139751434326,\n",
       "    0.21381635963916779,\n",
       "    0.21210820972919464,\n",
       "    0.2103773057460785,\n",
       "    0.19946211576461792,\n",
       "    0.18344904482364655,\n",
       "    0.1675863415002823,\n",
       "    0.16048943996429443,\n",
       "    0.16049543023109436,\n",
       "    0.16205178201198578,\n",
       "    0.1640491932630539,\n",
       "    0.15514861047267914,\n",
       "    0.14847257733345032,\n",
       "    0.14587244391441345,\n",
       "    0.1468270868062973,\n",
       "    0.1449684351682663,\n",
       "    0.13804256916046143,\n",
       "    0.1351955085992813,\n",
       "    0.13592199981212616,\n",
       "    0.1374754011631012,\n",
       "    0.13852323591709137,\n",
       "    0.1502571851015091,\n",
       "    0.1412895917892456,\n",
       "    0.13108915090560913,\n",
       "    0.1265152245759964,\n",
       "    0.12678927183151245,\n",
       "    0.12856318056583405,\n",
       "    0.13104578852653503,\n",
       "    0.14329028129577637,\n",
       "    0.1351822167634964,\n",
       "    0.13179734349250793,\n",
       "    0.1311136782169342,\n",
       "    0.12792153656482697,\n",
       "    0.13000443577766418,\n",
       "    0.12454812973737717,\n",
       "    0.12644726037979126,\n",
       "    0.12560854852199554,\n",
       "    0.13124923408031464,\n",
       "    0.14816677570343018,\n",
       "    0.14270104467868805,\n",
       "    0.12542642652988434,\n",
       "    0.1329643875360489,\n",
       "    0.12511835992336273,\n",
       "    0.14825381338596344,\n",
       "    0.1508161425590515,\n",
       "    0.14345651865005493],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001]},\n",
       "  'metrics': {'mse': 0.04148488491773605,\n",
       "   'rmse': 0.20367838598569082,\n",
       "   'mae': 0.14345653355121613,\n",
       "   'r2': 0.5000544786453247,\n",
       "   'mape': 32.4563102722168},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.059449538588523865,\n",
       "    0.05592306237667799,\n",
       "    0.049204918555915356,\n",
       "    0.06195576209574938,\n",
       "    0.04628328653052449,\n",
       "    0.04565663356333971,\n",
       "    0.05956124234944582,\n",
       "    0.049106160178780556,\n",
       "    0.044541649520397186,\n",
       "    0.03720840159803629,\n",
       "    0.043214403092861176,\n",
       "    0.039069199468940496,\n",
       "    0.046270107850432396,\n",
       "    0.03438613237813115,\n",
       "    0.03390065999701619,\n",
       "    0.03277271194383502,\n",
       "    0.03391501773148775,\n",
       "    0.04108652286231518,\n",
       "    0.029396147467195988,\n",
       "    0.036815958097577095,\n",
       "    0.030052953865379095,\n",
       "    0.03865288011729717,\n",
       "    0.03414418688043952,\n",
       "    0.03371778782457113,\n",
       "    0.03072591871023178,\n",
       "    0.027943206019699574,\n",
       "    0.039553483948111534,\n",
       "    0.025028546922840178,\n",
       "    0.03303006058558822,\n",
       "    0.023791005136445165,\n",
       "    0.02555384999141097,\n",
       "    0.02374418661929667,\n",
       "    0.026594122406095266,\n",
       "    0.02713884925469756,\n",
       "    0.029941340442746878,\n",
       "    0.02660667710006237,\n",
       "    0.024367161560803652,\n",
       "    0.030938105657696724,\n",
       "    0.026379082817584276,\n",
       "    0.020598046714439988,\n",
       "    0.03128363145515323,\n",
       "    0.021829779259860516,\n",
       "    0.022730405442416668,\n",
       "    0.021648470778018236,\n",
       "    0.02186083677224815,\n",
       "    0.02974611846730113,\n",
       "    0.02725644549354911,\n",
       "    0.019280157051980495],\n",
       "   'val_loss': [0.028762081637978554,\n",
       "    0.021517137065529823,\n",
       "    0.01701037585735321,\n",
       "    0.013860098086297512,\n",
       "    0.013066306710243225,\n",
       "    0.013556581921875477,\n",
       "    0.012590749189257622,\n",
       "    0.013600409962236881,\n",
       "    0.013387339189648628,\n",
       "    0.012800730764865875,\n",
       "    0.012905116192996502,\n",
       "    0.012520377524197102,\n",
       "    0.012427154928445816,\n",
       "    0.012674045749008656,\n",
       "    0.01245596818625927,\n",
       "    0.014778878539800644,\n",
       "    0.013240219093859196,\n",
       "    0.011489255353808403,\n",
       "    0.011899655684828758,\n",
       "    0.012757360935211182,\n",
       "    0.013590658083558083,\n",
       "    0.013596979901194572,\n",
       "    0.012865173630416393,\n",
       "    0.01264698151499033,\n",
       "    0.01323927752673626,\n",
       "    0.01743648760020733,\n",
       "    0.01477180514484644,\n",
       "    0.011425362899899483,\n",
       "    0.01167573407292366,\n",
       "    0.013239290565252304,\n",
       "    0.013185335323214531,\n",
       "    0.012107187882065773,\n",
       "    0.011690299957990646,\n",
       "    0.01148148812353611,\n",
       "    0.012583880685269833,\n",
       "    0.012978426180779934,\n",
       "    0.01197776384651661,\n",
       "    0.012342650443315506,\n",
       "    0.012612476013600826,\n",
       "    0.0128336101770401,\n",
       "    0.012924103997647762,\n",
       "    0.012361099012196064,\n",
       "    0.011863704770803452,\n",
       "    0.011650207452476025,\n",
       "    0.01162785105407238,\n",
       "    0.011767682619392872,\n",
       "    0.01184246689081192,\n",
       "    0.011773125268518925],\n",
       "   'train_mae': [0.2836090438067913,\n",
       "    0.2682575024664402,\n",
       "    0.23655571788549423,\n",
       "    0.25452711433172226,\n",
       "    0.2304752878844738,\n",
       "    0.23690535500645638,\n",
       "    0.26782649382948875,\n",
       "    0.24938275292515755,\n",
       "    0.23736726492643356,\n",
       "    0.2151612527668476,\n",
       "    0.22514765337109566,\n",
       "    0.20745060220360756,\n",
       "    0.228132463991642,\n",
       "    0.1970439925789833,\n",
       "    0.19886307045817375,\n",
       "    0.18855875357985497,\n",
       "    0.18170371279120445,\n",
       "    0.20442988350987434,\n",
       "    0.1771251279860735,\n",
       "    0.1975906826555729,\n",
       "    0.17658031359314919,\n",
       "    0.2016509287059307,\n",
       "    0.19230296090245247,\n",
       "    0.18964328989386559,\n",
       "    0.18715588003396988,\n",
       "    0.16972566395998,\n",
       "    0.1874077506363392,\n",
       "    0.15626361593604088,\n",
       "    0.18298041820526123,\n",
       "    0.15743579156696796,\n",
       "    0.15736696869134903,\n",
       "    0.15096681006252766,\n",
       "    0.15657583065330982,\n",
       "    0.16592856124043465,\n",
       "    0.17750653252005577,\n",
       "    0.16155505180358887,\n",
       "    0.15857774019241333,\n",
       "    0.18557018786668777,\n",
       "    0.16692334413528442,\n",
       "    0.14154494367539883,\n",
       "    0.1753922961652279,\n",
       "    0.14327950030565262,\n",
       "    0.14879266917705536,\n",
       "    0.15074463561177254,\n",
       "    0.14842211082577705,\n",
       "    0.17158479616045952,\n",
       "    0.1612382009625435,\n",
       "    0.13620220683515072],\n",
       "   'val_mae': [0.20458854734897614,\n",
       "    0.17072492837905884,\n",
       "    0.1456683874130249,\n",
       "    0.13438740372657776,\n",
       "    0.13133788108825684,\n",
       "    0.11739270389080048,\n",
       "    0.11804713308811188,\n",
       "    0.13848839700222015,\n",
       "    0.13696745038032532,\n",
       "    0.12580034136772156,\n",
       "    0.12191906571388245,\n",
       "    0.12288019061088562,\n",
       "    0.1222316324710846,\n",
       "    0.12392482906579971,\n",
       "    0.1105552688241005,\n",
       "    0.10971628874540329,\n",
       "    0.09789790213108063,\n",
       "    0.10223153978586197,\n",
       "    0.11225534975528717,\n",
       "    0.1013694480061531,\n",
       "    0.10407175868749619,\n",
       "    0.10474871844053268,\n",
       "    0.09925907850265503,\n",
       "    0.0983097106218338,\n",
       "    0.09886735677719116,\n",
       "    0.1203121766448021,\n",
       "    0.10463157296180725,\n",
       "    0.09805313497781754,\n",
       "    0.09997053444385529,\n",
       "    0.1034439206123352,\n",
       "    0.10023029893636703,\n",
       "    0.09602318704128265,\n",
       "    0.09870877861976624,\n",
       "    0.09823241829872131,\n",
       "    0.09308484941720963,\n",
       "    0.09532754868268967,\n",
       "    0.09543114900588989,\n",
       "    0.09769652783870697,\n",
       "    0.09508077055215836,\n",
       "    0.0954386368393898,\n",
       "    0.09695100784301758,\n",
       "    0.09770234674215317,\n",
       "    0.10222040861845016,\n",
       "    0.10368473827838898,\n",
       "    0.1026291698217392,\n",
       "    0.09960047900676727,\n",
       "    0.0973365306854248,\n",
       "    0.09770087897777557],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005]},\n",
       "  'metrics': {'mse': 0.023546252399683,\n",
       "   'rmse': 0.15344788170477622,\n",
       "   'mae': 0.09770087897777557,\n",
       "   'r2': 0.49483931064605713,\n",
       "   'mape': 15.185403823852539},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.07135388702154159,\n",
       "    0.043080438673496244,\n",
       "    0.04837194085121155,\n",
       "    0.04164638966321945,\n",
       "    0.03737262785434723,\n",
       "    0.03313495814800262,\n",
       "    0.035105335712432864,\n",
       "    0.033657876402139665,\n",
       "    0.02941645383834839,\n",
       "    0.030484236776828766,\n",
       "    0.03769058063626289,\n",
       "    0.03509870879352093,\n",
       "    0.02960713244974613,\n",
       "    0.024707432836294174,\n",
       "    0.028605109453201293,\n",
       "    0.02477349191904068,\n",
       "    0.02435844987630844,\n",
       "    0.03568428047001362,\n",
       "    0.03038078360259533,\n",
       "    0.024673630855977534,\n",
       "    0.02294714320451021,\n",
       "    0.02240015994757414,\n",
       "    0.026925412565469743,\n",
       "    0.023258959874510765,\n",
       "    0.030570983327925204,\n",
       "    0.02191358953714371,\n",
       "    0.03253067508339882,\n",
       "    0.020236767642199994,\n",
       "    0.028971989825367927,\n",
       "    0.02652488201856613,\n",
       "    0.02142808437347412],\n",
       "   'val_loss': [0.02070511505007744,\n",
       "    0.009761308319866657,\n",
       "    0.007712225429713726,\n",
       "    0.011938348412513733,\n",
       "    0.008701246231794357,\n",
       "    0.007563871331512928,\n",
       "    0.006598574575036764,\n",
       "    0.005600316915661097,\n",
       "    0.004506865981966257,\n",
       "    0.004247534554451704,\n",
       "    0.004208649508655071,\n",
       "    0.004236605484038591,\n",
       "    0.006192823406308889,\n",
       "    0.007027068641036749,\n",
       "    0.006640160456299782,\n",
       "    0.006483091507107019,\n",
       "    0.00563153438270092,\n",
       "    0.005901264026761055,\n",
       "    0.006343197543174028,\n",
       "    0.007060423959046602,\n",
       "    0.007142567075788975,\n",
       "    0.007495209574699402,\n",
       "    0.0073035042732954025,\n",
       "    0.006587557960301638,\n",
       "    0.006301607470959425,\n",
       "    0.007849466055631638,\n",
       "    0.009083791635930538,\n",
       "    0.0101095549762249,\n",
       "    0.00920788198709488,\n",
       "    0.00849237572401762,\n",
       "    0.007706610951572657],\n",
       "   'train_mae': [0.29457409381866456,\n",
       "    0.22843575477600098,\n",
       "    0.22819347083568572,\n",
       "    0.21703635454177855,\n",
       "    0.20836269855499268,\n",
       "    0.192076613008976,\n",
       "    0.196844619512558,\n",
       "    0.19321051239967346,\n",
       "    0.1689379870891571,\n",
       "    0.17683686316013336,\n",
       "    0.19058477282524108,\n",
       "    0.19323773980140685,\n",
       "    0.1779786765575409,\n",
       "    0.16128090769052505,\n",
       "    0.1643500506877899,\n",
       "    0.15700165331363677,\n",
       "    0.155220228433609,\n",
       "    0.19096459746360778,\n",
       "    0.18213672637939454,\n",
       "    0.15375147461891175,\n",
       "    0.14036829322576522,\n",
       "    0.14553515017032623,\n",
       "    0.1618417978286743,\n",
       "    0.1503885954618454,\n",
       "    0.17557332068681716,\n",
       "    0.14758467972278594,\n",
       "    0.17347404956817628,\n",
       "    0.13960521221160888,\n",
       "    0.16794603765010835,\n",
       "    0.15990722477436065,\n",
       "    0.1459841638803482],\n",
       "   'val_mae': [0.17023949325084686,\n",
       "    0.12481971830129623,\n",
       "    0.09935018420219421,\n",
       "    0.12020227313041687,\n",
       "    0.10328323394060135,\n",
       "    0.09253934025764465,\n",
       "    0.0842590183019638,\n",
       "    0.07770327478647232,\n",
       "    0.06320023536682129,\n",
       "    0.0593978576362133,\n",
       "    0.055783096700906754,\n",
       "    0.057755470275878906,\n",
       "    0.08504373580217361,\n",
       "    0.09202205389738083,\n",
       "    0.08897943049669266,\n",
       "    0.08976702392101288,\n",
       "    0.07981012761592865,\n",
       "    0.07891759276390076,\n",
       "    0.07953036576509476,\n",
       "    0.08919014781713486,\n",
       "    0.09367232769727707,\n",
       "    0.09687765687704086,\n",
       "    0.09466961771249771,\n",
       "    0.08838149160146713,\n",
       "    0.08696042001247406,\n",
       "    0.10224323719739914,\n",
       "    0.11116289347410202,\n",
       "    0.11797480285167694,\n",
       "    0.11263938993215561,\n",
       "    0.10904314368963242,\n",
       "    0.1037384644150734],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005]},\n",
       "  'metrics': {'mse': 0.015413228422403336,\n",
       "   'rmse': 0.12415002385180333,\n",
       "   'mae': 0.10373847186565399,\n",
       "   'r2': 0.42251622676849365,\n",
       "   'mape': 11.34990119934082},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.05029337015002966,\n",
       "    0.041088311622540154,\n",
       "    0.03906923703228434,\n",
       "    0.03591785083214442,\n",
       "    0.033933792573710285,\n",
       "    0.030366020742803812,\n",
       "    0.029016330372542143,\n",
       "    0.02451170371690144,\n",
       "    0.027060828792552154,\n",
       "    0.025626929787298042,\n",
       "    0.041285169310867786,\n",
       "    0.022510228135312598,\n",
       "    0.027396071857462328,\n",
       "    0.022222593193873763,\n",
       "    0.023385499138385057,\n",
       "    0.02191212571536501,\n",
       "    0.019602412668367226,\n",
       "    0.03360413387417793,\n",
       "    0.026399787049740553,\n",
       "    0.028477434068918228,\n",
       "    0.026834289853771526,\n",
       "    0.020992691861465573],\n",
       "   'val_loss': [0.013552584685385227,\n",
       "    0.011409711092710495,\n",
       "    0.013458539731800556,\n",
       "    0.01340384129434824,\n",
       "    0.012934517115354538,\n",
       "    0.012364485301077366,\n",
       "    0.012228517793118954,\n",
       "    0.01289291586726904,\n",
       "    0.01321202702820301,\n",
       "    0.012869531288743019,\n",
       "    0.01301333587616682,\n",
       "    0.014065993018448353,\n",
       "    0.014817863702774048,\n",
       "    0.014570368453860283,\n",
       "    0.014639909379184246,\n",
       "    0.01451636478304863,\n",
       "    0.014059516601264477,\n",
       "    0.013957555405795574,\n",
       "    0.013506240211427212,\n",
       "    0.013134420849382877,\n",
       "    0.013115284964442253,\n",
       "    0.012857040390372276],\n",
       "   'train_mae': [0.22250283261140189,\n",
       "    0.19564673552910486,\n",
       "    0.20100344717502594,\n",
       "    0.19043264041344324,\n",
       "    0.18175635735193887,\n",
       "    0.16447948043545088,\n",
       "    0.16641776636242867,\n",
       "    0.14934757351875305,\n",
       "    0.1588182101647059,\n",
       "    0.14328554521004358,\n",
       "    0.18795057634512582,\n",
       "    0.14331458633144697,\n",
       "    0.15573247273763022,\n",
       "    0.1334308876345555,\n",
       "    0.14077004541953406,\n",
       "    0.13694950938224792,\n",
       "    0.13180489713946977,\n",
       "    0.16789804647366205,\n",
       "    0.15056627492109934,\n",
       "    0.16280976310372353,\n",
       "    0.15372870365778604,\n",
       "    0.1302614938467741],\n",
       "   'val_mae': [0.11038123071193695,\n",
       "    0.0955624058842659,\n",
       "    0.1040067970752716,\n",
       "    0.10255621373653412,\n",
       "    0.09377245604991913,\n",
       "    0.0898808091878891,\n",
       "    0.09166929870843887,\n",
       "    0.08584042638540268,\n",
       "    0.08368729799985886,\n",
       "    0.08532336354255676,\n",
       "    0.08590258657932281,\n",
       "    0.0866442322731018,\n",
       "    0.0910111665725708,\n",
       "    0.08949930220842361,\n",
       "    0.08847586810588837,\n",
       "    0.08705876022577286,\n",
       "    0.0838785395026207,\n",
       "    0.08337079733610153,\n",
       "    0.08245775103569031,\n",
       "    0.0840788334608078,\n",
       "    0.0858263298869133,\n",
       "    0.08462657779455185],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005]},\n",
       "  'metrics': {'mse': 0.025714082643389702,\n",
       "   'rmse': 0.16035611196143945,\n",
       "   'mae': 0.08462658524513245,\n",
       "   'r2': 0.35501039028167725,\n",
       "   'mape': 16.588106155395508},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.0416385257350547,\n",
       "    0.031271579010146,\n",
       "    0.028223866862910136,\n",
       "    0.028618344238826206,\n",
       "    0.027900444343686104,\n",
       "    0.02901338626231466,\n",
       "    0.025404494388827255,\n",
       "    0.05803338864019939,\n",
       "    0.02464078020836626,\n",
       "    0.022326839888202294,\n",
       "    0.02292730945295521,\n",
       "    0.02129382493772677,\n",
       "    0.020105548302776048,\n",
       "    0.021155616800699915,\n",
       "    0.018930740787514617,\n",
       "    0.02195428271910974,\n",
       "    0.019251408454562937,\n",
       "    0.02986728958785534,\n",
       "    0.022546017542481422,\n",
       "    0.02470284208123173,\n",
       "    0.022785498893686702,\n",
       "    0.020183016479547535,\n",
       "    0.05800637882202864,\n",
       "    0.025546359164374217,\n",
       "    0.023235842552302138,\n",
       "    0.021739221843225614,\n",
       "    0.020634602954877273,\n",
       "    0.019548344558903148,\n",
       "    0.018388241595987762,\n",
       "    0.03254211467823812,\n",
       "    0.028980708015816554,\n",
       "    0.019323434348085096,\n",
       "    0.01904015110007354,\n",
       "    0.025494664242225035,\n",
       "    0.02004704757460526,\n",
       "    0.01792595475646002,\n",
       "    0.016404835457381393,\n",
       "    0.0263753260618874,\n",
       "    0.01902322717277067,\n",
       "    0.03282347082027367,\n",
       "    0.019874051745448793,\n",
       "    0.016847179803465093,\n",
       "    0.01629907132259437,\n",
       "    0.017262703173660805,\n",
       "    0.017494678497314453,\n",
       "    0.01679938765508788,\n",
       "    0.017640360897140845,\n",
       "    0.024362641253641674,\n",
       "    0.017356418738407747,\n",
       "    0.01999004344855036],\n",
       "   'val_loss': [0.030682362616062164,\n",
       "    0.023721380159258842,\n",
       "    0.023867914453148842,\n",
       "    0.021626103669404984,\n",
       "    0.023782305419445038,\n",
       "    0.02344362623989582,\n",
       "    0.02246025949716568,\n",
       "    0.023568831384181976,\n",
       "    0.02185744419693947,\n",
       "    0.02120540477335453,\n",
       "    0.019946258515119553,\n",
       "    0.022653263062238693,\n",
       "    0.02089638262987137,\n",
       "    0.020114658400416374,\n",
       "    0.02116105705499649,\n",
       "    0.01947578601539135,\n",
       "    0.020252544432878494,\n",
       "    0.022568965330719948,\n",
       "    0.021878760308027267,\n",
       "    0.02271244302392006,\n",
       "    0.021152708679437637,\n",
       "    0.02060992829501629,\n",
       "    0.021025732159614563,\n",
       "    0.02154911682009697,\n",
       "    0.02232109010219574,\n",
       "    0.02213139273226261,\n",
       "    0.021597839891910553,\n",
       "    0.0208953358232975,\n",
       "    0.02037474885582924,\n",
       "    0.02008003741502762,\n",
       "    0.020255079492926598,\n",
       "    0.020396195352077484,\n",
       "    0.02010601945221424,\n",
       "    0.01897887885570526,\n",
       "    0.018850555643439293,\n",
       "    0.01898597553372383,\n",
       "    0.01903289556503296,\n",
       "    0.01928124949336052,\n",
       "    0.019762001931667328,\n",
       "    0.020011665299534798,\n",
       "    0.022035637870430946,\n",
       "    0.020871970802545547,\n",
       "    0.020346328616142273,\n",
       "    0.019968600943684578,\n",
       "    0.020218342542648315,\n",
       "    0.02032417804002762,\n",
       "    0.020020434632897377,\n",
       "    0.019030313938856125,\n",
       "    0.019118648022413254,\n",
       "    0.019180722534656525],\n",
       "   'train_mae': [0.21955126311097825,\n",
       "    0.17217360969100678,\n",
       "    0.16538700248513902,\n",
       "    0.17084319250924246,\n",
       "    0.15807094957147325,\n",
       "    0.16589793988636561,\n",
       "    0.1549763913665499,\n",
       "    0.2219350657292775,\n",
       "    0.1461691888315337,\n",
       "    0.13904978334903717,\n",
       "    0.13316353357263974,\n",
       "    0.13761029073170253,\n",
       "    0.1268690824508667,\n",
       "    0.1323569608586175,\n",
       "    0.12345790224415916,\n",
       "    0.13983843475580215,\n",
       "    0.1285587442772729,\n",
       "    0.16563529201916286,\n",
       "    0.15603433123656682,\n",
       "    0.15691843841757094,\n",
       "    0.1347235558288438,\n",
       "    0.13015681505203247,\n",
       "    0.21954813706023352,\n",
       "    0.15855562686920166,\n",
       "    0.1330935891185488,\n",
       "    0.13331400070871627,\n",
       "    0.13264982295887812,\n",
       "    0.1331767322761672,\n",
       "    0.1254322220172201,\n",
       "    0.1698785818048886,\n",
       "    0.16354120416300638,\n",
       "    0.12410654659782137,\n",
       "    0.12466759660414287,\n",
       "    0.14102491842848913,\n",
       "    0.13552577048540115,\n",
       "    0.11894257366657257,\n",
       "    0.10919799442802157,\n",
       "    0.1528807921069009,\n",
       "    0.12052579011235919,\n",
       "    0.17046871462038585,\n",
       "    0.13792387502534048,\n",
       "    0.12150972762278148,\n",
       "    0.11292711592146329,\n",
       "    0.1137382739356586,\n",
       "    0.1240904831460544,\n",
       "    0.11586657698665347,\n",
       "    0.11961821679558073,\n",
       "    0.14694135529654367,\n",
       "    0.11988537119967597,\n",
       "    0.12721607089042664],\n",
       "   'val_mae': [0.18179142475128174,\n",
       "    0.11842630803585052,\n",
       "    0.12391243129968643,\n",
       "    0.11805213242769241,\n",
       "    0.13443179428577423,\n",
       "    0.13068346679210663,\n",
       "    0.11811909824609756,\n",
       "    0.11899346858263016,\n",
       "    0.11892245709896088,\n",
       "    0.11489075422286987,\n",
       "    0.12251487374305725,\n",
       "    0.1278139054775238,\n",
       "    0.11407992243766785,\n",
       "    0.11214554309844971,\n",
       "    0.1269078552722931,\n",
       "    0.11812549829483032,\n",
       "    0.10991718620061874,\n",
       "    0.11998172104358673,\n",
       "    0.13485483825206757,\n",
       "    0.12170293927192688,\n",
       "    0.10937179625034332,\n",
       "    0.10832740366458893,\n",
       "    0.11463536322116852,\n",
       "    0.11649083346128464,\n",
       "    0.11558477580547333,\n",
       "    0.11628270894289017,\n",
       "    0.1229376494884491,\n",
       "    0.11724412441253662,\n",
       "    0.10870325565338135,\n",
       "    0.10499100387096405,\n",
       "    0.10537178069353104,\n",
       "    0.10827753692865372,\n",
       "    0.10920000076293945,\n",
       "    0.11042460054159164,\n",
       "    0.10993263870477676,\n",
       "    0.10907256603240967,\n",
       "    0.10814607888460159,\n",
       "    0.11304415017366409,\n",
       "    0.12374552339315414,\n",
       "    0.13326694071292877,\n",
       "    0.16539524495601654,\n",
       "    0.14543543756008148,\n",
       "    0.12716320157051086,\n",
       "    0.12578997015953064,\n",
       "    0.12148921936750412,\n",
       "    0.12382402271032333,\n",
       "    0.1249794289469719,\n",
       "    0.12695083022117615,\n",
       "    0.1202002763748169,\n",
       "    0.11803218722343445],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.00025,\n",
       "    0.00025,\n",
       "    0.00025,\n",
       "    0.00025]},\n",
       "  'metrics': {'mse': 0.038361452519893646,\n",
       "   'rmse': 0.19586079883400262,\n",
       "   'mae': 0.11803219467401505,\n",
       "   'r2': 0.4598422050476074,\n",
       "   'mape': 30.9086856842041},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.04270877795560019,\n",
       "    0.033517751576645036,\n",
       "    0.030387388808386668,\n",
       "    0.029526270925998688,\n",
       "    0.029576638447386876,\n",
       "    0.02764375614268439,\n",
       "    0.026318724267184734,\n",
       "    0.023912581216011728,\n",
       "    0.023630776841725622,\n",
       "    0.022707419469952583,\n",
       "    0.02221181908888476,\n",
       "    0.021841257411454405,\n",
       "    0.021786864580852643,\n",
       "    0.01996926690584847,\n",
       "    0.018526449001261165,\n",
       "    0.019413511401840618,\n",
       "    0.01842974405735731,\n",
       "    0.017872360135827745,\n",
       "    0.018997219258121083,\n",
       "    0.018349966699523584,\n",
       "    0.017671240360609124,\n",
       "    0.017497105124805654,\n",
       "    0.017171221652201245,\n",
       "    0.017074710556438992],\n",
       "   'val_loss': [0.025272391736507416,\n",
       "    0.023326326161623,\n",
       "    0.0221258457750082,\n",
       "    0.022083671763539314,\n",
       "    0.02354823425412178,\n",
       "    0.022803006693720818,\n",
       "    0.023132380098104477,\n",
       "    0.023561028763651848,\n",
       "    0.023963749408721924,\n",
       "    0.024232327938079834,\n",
       "    0.02560274302959442,\n",
       "    0.024046309292316437,\n",
       "    0.025879453867673874,\n",
       "    0.025286760181188583,\n",
       "    0.024617468938231468,\n",
       "    0.02534291334450245,\n",
       "    0.025916049256920815,\n",
       "    0.026517366990447044,\n",
       "    0.02502676285803318,\n",
       "    0.02764023467898369,\n",
       "    0.026258524507284164,\n",
       "    0.02597498893737793,\n",
       "    0.024975724518299103,\n",
       "    0.026260288432240486],\n",
       "   'train_mae': [0.22138425707817078,\n",
       "    0.18299985144819533,\n",
       "    0.17258555335657938,\n",
       "    0.17113059972013747,\n",
       "    0.15896344291312353,\n",
       "    0.16027937403747014,\n",
       "    0.15472248728786195,\n",
       "    0.14705027320555278,\n",
       "    0.14619063585996628,\n",
       "    0.14023954101971217,\n",
       "    0.13567869365215302,\n",
       "    0.1358415803739003,\n",
       "    0.13493014765637262,\n",
       "    0.12951617794377462,\n",
       "    0.12582824592079436,\n",
       "    0.12790508036102569,\n",
       "    0.1279794318335397,\n",
       "    0.11957718538386482,\n",
       "    0.13020654448441096,\n",
       "    0.12543174411569322,\n",
       "    0.1211141049861908,\n",
       "    0.11963629935468946,\n",
       "    0.12092808846916471,\n",
       "    0.12057467017854963],\n",
       "   'val_mae': [0.164655864238739,\n",
       "    0.1477353721857071,\n",
       "    0.1368298977613449,\n",
       "    0.13500085473060608,\n",
       "    0.13878507912158966,\n",
       "    0.1374613493680954,\n",
       "    0.13160866498947144,\n",
       "    0.12721611559391022,\n",
       "    0.12616847455501556,\n",
       "    0.12447106838226318,\n",
       "    0.11428958177566528,\n",
       "    0.11923002451658249,\n",
       "    0.11386420577764511,\n",
       "    0.11352625489234924,\n",
       "    0.11315121501684189,\n",
       "    0.11239007115364075,\n",
       "    0.11526911705732346,\n",
       "    0.11490115523338318,\n",
       "    0.11551795154809952,\n",
       "    0.11812757700681686,\n",
       "    0.11282467842102051,\n",
       "    0.11157319694757462,\n",
       "    0.11655442416667938,\n",
       "    0.11151337623596191],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005]},\n",
       "  'metrics': {'mse': 0.0525997094810009,\n",
       "   'rmse': 0.22934626546120365,\n",
       "   'mae': 0.11151337623596191,\n",
       "   'r2': 0.2132994532585144,\n",
       "   'mape': 32.239017486572266},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.03864515922032297,\n",
       "    0.03190914331935346,\n",
       "    0.02924246317707002,\n",
       "    0.027740336256101727,\n",
       "    0.026892150519415736,\n",
       "    0.024685578187927604,\n",
       "    0.023524619755335152,\n",
       "    0.022249935660511255,\n",
       "    0.023545901873148978,\n",
       "    0.024121661204844713,\n",
       "    0.021998095326125622,\n",
       "    0.021571992547251284,\n",
       "    0.021054717246443033,\n",
       "    0.019859326537698507,\n",
       "    0.019768660073168576,\n",
       "    0.01993094850331545,\n",
       "    0.019502533948980272,\n",
       "    0.01852256420534104,\n",
       "    0.018293629749678075,\n",
       "    0.016980398097075522,\n",
       "    0.018142284243367612,\n",
       "    0.01721034455113113,\n",
       "    0.01690431172028184,\n",
       "    0.017547446535900235,\n",
       "    0.015908570610918105,\n",
       "    0.016467008739709854,\n",
       "    0.017090806737542152,\n",
       "    0.017055147036444396,\n",
       "    0.014774955809116364,\n",
       "    0.014837190276011825,\n",
       "    0.014593475498259068,\n",
       "    0.01461887953337282,\n",
       "    0.014058477245271206,\n",
       "    0.014045506832189858,\n",
       "    0.014230378903448582],\n",
       "   'val_loss': [0.018001077696681023,\n",
       "    0.01846068911254406,\n",
       "    0.021621348336338997,\n",
       "    0.019866373389959335,\n",
       "    0.02259218879044056,\n",
       "    0.022278131917119026,\n",
       "    0.021745529025793076,\n",
       "    0.029840726405382156,\n",
       "    0.016679368913173676,\n",
       "    0.018277045339345932,\n",
       "    0.018530772998929024,\n",
       "    0.01670694537460804,\n",
       "    0.018276238813996315,\n",
       "    0.021246856078505516,\n",
       "    0.015107749029994011,\n",
       "    0.01586887799203396,\n",
       "    0.016336655244231224,\n",
       "    0.015841621905565262,\n",
       "    0.01718093827366829,\n",
       "    0.01591646485030651,\n",
       "    0.021013259887695312,\n",
       "    0.016220448538661003,\n",
       "    0.018919434398412704,\n",
       "    0.016662422567605972,\n",
       "    0.016658352687954903,\n",
       "    0.01768021285533905,\n",
       "    0.021257635205984116,\n",
       "    0.016869885846972466,\n",
       "    0.016574222594499588,\n",
       "    0.018144119530916214,\n",
       "    0.01753668300807476,\n",
       "    0.016856376081705093,\n",
       "    0.01825024001300335,\n",
       "    0.017990559339523315,\n",
       "    0.018094096332788467],\n",
       "   'train_mae': [0.20170041173696518,\n",
       "    0.1685820184648037,\n",
       "    0.1595391072332859,\n",
       "    0.16455244459211826,\n",
       "    0.15354913286864758,\n",
       "    0.14997893571853638,\n",
       "    0.14338912535458803,\n",
       "    0.1419717948883772,\n",
       "    0.14031732454895973,\n",
       "    0.13762136083096266,\n",
       "    0.1331595443189144,\n",
       "    0.13870742078870535,\n",
       "    0.12725705094635487,\n",
       "    0.1313912896439433,\n",
       "    0.12663518637418747,\n",
       "    0.12643131520599127,\n",
       "    0.12414853926748037,\n",
       "    0.12349934224039316,\n",
       "    0.12029544822871685,\n",
       "    0.1172936549410224,\n",
       "    0.12344790529459715,\n",
       "    0.1152519378811121,\n",
       "    0.11579952668398619,\n",
       "    0.12084723357111216,\n",
       "    0.11136860772967339,\n",
       "    0.11493076663464308,\n",
       "    0.1128716403618455,\n",
       "    0.11527664959430695,\n",
       "    0.10584517568349838,\n",
       "    0.10958588868379593,\n",
       "    0.10762736294418573,\n",
       "    0.10675875004380941,\n",
       "    0.10474570281803608,\n",
       "    0.10435789823532104,\n",
       "    0.10728456266224384],\n",
       "   'val_mae': [0.1475667655467987,\n",
       "    0.14705698192119598,\n",
       "    0.1765611171722412,\n",
       "    0.1633833646774292,\n",
       "    0.17867209017276764,\n",
       "    0.17522293329238892,\n",
       "    0.1545058786869049,\n",
       "    0.19994333386421204,\n",
       "    0.11645790189504623,\n",
       "    0.14088599383831024,\n",
       "    0.1357566863298416,\n",
       "    0.12806828320026398,\n",
       "    0.1407461315393448,\n",
       "    0.1489640176296234,\n",
       "    0.11251349002122879,\n",
       "    0.1248915046453476,\n",
       "    0.13118189573287964,\n",
       "    0.12896661460399628,\n",
       "    0.13040821254253387,\n",
       "    0.13290396332740784,\n",
       "    0.16145548224449158,\n",
       "    0.13561633229255676,\n",
       "    0.15652920305728912,\n",
       "    0.14140956103801727,\n",
       "    0.14168469607830048,\n",
       "    0.1532379686832428,\n",
       "    0.17420895397663116,\n",
       "    0.13812272250652313,\n",
       "    0.1374206840991974,\n",
       "    0.15605485439300537,\n",
       "    0.14951257407665253,\n",
       "    0.14248929917812347,\n",
       "    0.1529494673013687,\n",
       "    0.150239497423172,\n",
       "    0.1500348001718521],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005]},\n",
       "  'metrics': {'mse': 0.03618819639086723,\n",
       "   'rmse': 0.19023195417927882,\n",
       "   'mae': 0.1500348150730133,\n",
       "   'r2': 0.23902446031570435,\n",
       "   'mape': 20.815839767456055},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.037017624411318034,\n",
       "    0.030788312355677288,\n",
       "    0.027881159550613828,\n",
       "    0.027625987099276647,\n",
       "    0.025141369758380785,\n",
       "    0.024265615166061454,\n",
       "    0.022919190116226673,\n",
       "    0.020820628437731,\n",
       "    0.022551532834768295,\n",
       "    0.021541491150856018,\n",
       "    0.021206856498287782,\n",
       "    0.02014706304503812,\n",
       "    0.0201025502756238,\n",
       "    0.019505651874674693,\n",
       "    0.019738112576305866,\n",
       "    0.018798292614519596,\n",
       "    0.018926111567351554,\n",
       "    0.01784791849139664,\n",
       "    0.017441388736996386,\n",
       "    0.01782768385277854,\n",
       "    0.01769734313711524,\n",
       "    0.0196207861105601,\n",
       "    0.0178999530358447,\n",
       "    0.018561311066150665,\n",
       "    0.017678794244097337,\n",
       "    0.015457391324970458,\n",
       "    0.015363596534977356,\n",
       "    0.017230881481534906,\n",
       "    0.016245625706182584,\n",
       "    0.014600679071413146,\n",
       "    0.014466446100009812,\n",
       "    0.014975121969150173,\n",
       "    0.01443696798135837,\n",
       "    0.01380464517407947,\n",
       "    0.014148024428221915,\n",
       "    0.013773647343946828,\n",
       "    0.013304155733850267],\n",
       "   'val_loss': [0.04715300723910332,\n",
       "    0.043925460427999496,\n",
       "    0.04353079944849014,\n",
       "    0.04447903856635094,\n",
       "    0.04775123670697212,\n",
       "    0.04504573345184326,\n",
       "    0.042429082095623016,\n",
       "    0.03814583271741867,\n",
       "    0.040908332914114,\n",
       "    0.03915145620703697,\n",
       "    0.042661186307668686,\n",
       "    0.043073851615190506,\n",
       "    0.04426995664834976,\n",
       "    0.03902232274413109,\n",
       "    0.04503755643963814,\n",
       "    0.03955372795462608,\n",
       "    0.036966901272535324,\n",
       "    0.04053390026092529,\n",
       "    0.039524611085653305,\n",
       "    0.04178536683320999,\n",
       "    0.04020622745156288,\n",
       "    0.041645076125860214,\n",
       "    0.040303271263837814,\n",
       "    0.04596887156367302,\n",
       "    0.03929577395319939,\n",
       "    0.04606781527400017,\n",
       "    0.04056544601917267,\n",
       "    0.04404307156801224,\n",
       "    0.0412149503827095,\n",
       "    0.039760854095220566,\n",
       "    0.042945317924022675,\n",
       "    0.04321741685271263,\n",
       "    0.04247331991791725,\n",
       "    0.042093005031347275,\n",
       "    0.04421311616897583,\n",
       "    0.03863860294222832,\n",
       "    0.04282946512103081],\n",
       "   'train_mae': [0.19013947579595777,\n",
       "    0.1725134270058738,\n",
       "    0.15988157358434466,\n",
       "    0.16317755480607352,\n",
       "    0.14859098278813893,\n",
       "    0.14699059310886595,\n",
       "    0.14356483022371927,\n",
       "    0.1315646469593048,\n",
       "    0.13863368746307161,\n",
       "    0.12914128684335285,\n",
       "    0.13436777724160087,\n",
       "    0.1273262600104014,\n",
       "    0.12931341346767214,\n",
       "    0.1284935043917762,\n",
       "    0.12326805790265401,\n",
       "    0.12390188376108806,\n",
       "    0.12140930195649464,\n",
       "    0.12282889667484495,\n",
       "    0.11962070481644736,\n",
       "    0.11819910920328563,\n",
       "    0.11921014802323447,\n",
       "    0.12310575279924604,\n",
       "    0.12097570217318004,\n",
       "    0.12295303410953945,\n",
       "    0.1148424314128028,\n",
       "    0.11353285445107354,\n",
       "    0.10983134888940388,\n",
       "    0.11593878848685159,\n",
       "    0.11023962580495411,\n",
       "    0.11014167467753093,\n",
       "    0.10721659246418211,\n",
       "    0.10924031999376085,\n",
       "    0.10552758226792018,\n",
       "    0.10389715101983812,\n",
       "    0.11043038301997715,\n",
       "    0.1020264940129386,\n",
       "    0.10406270954344007],\n",
       "   'val_mae': [0.23102375864982605,\n",
       "    0.24911752343177795,\n",
       "    0.2494538575410843,\n",
       "    0.24191957712173462,\n",
       "    0.2629004716873169,\n",
       "    0.22488680481910706,\n",
       "    0.21332044899463654,\n",
       "    0.20379361510276794,\n",
       "    0.19965702295303345,\n",
       "    0.21910196542739868,\n",
       "    0.19593015313148499,\n",
       "    0.1986512988805771,\n",
       "    0.18894314765930176,\n",
       "    0.1912919133901596,\n",
       "    0.18485285341739655,\n",
       "    0.1895073503255844,\n",
       "    0.1922745406627655,\n",
       "    0.18524816632270813,\n",
       "    0.1918269693851471,\n",
       "    0.18380561470985413,\n",
       "    0.19339899718761444,\n",
       "    0.1827508509159088,\n",
       "    0.1927245557308197,\n",
       "    0.18233975768089294,\n",
       "    0.18749284744262695,\n",
       "    0.18257546424865723,\n",
       "    0.19786497950553894,\n",
       "    0.18057172000408173,\n",
       "    0.18562795221805573,\n",
       "    0.18889081478118896,\n",
       "    0.18602725863456726,\n",
       "    0.1865883320569992,\n",
       "    0.18463489413261414,\n",
       "    0.18610218167304993,\n",
       "    0.18414835631847382,\n",
       "    0.18866008520126343,\n",
       "    0.18544481694698334],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005]},\n",
       "  'metrics': {'mse': 0.08565893024206161,\n",
       "   'rmse': 0.2926754691498104,\n",
       "   'mae': 0.18544480204582214,\n",
       "   'r2': 0.2303316593170166,\n",
       "   'mape': 59.476558685302734},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.04155615530908108,\n",
       "    0.03354899920523167,\n",
       "    0.030496589094400405,\n",
       "    0.028986548073589802,\n",
       "    0.028177323937416076,\n",
       "    0.02706227693706751,\n",
       "    0.025302855856716634,\n",
       "    0.02354701440781355,\n",
       "    0.023568115569651125,\n",
       "    0.022019524592906237,\n",
       "    0.022414102032780648,\n",
       "    0.022405327949672937,\n",
       "    0.022997780237346888,\n",
       "    0.021088263764977456,\n",
       "    0.02146185329183936,\n",
       "    0.02073037959635258,\n",
       "    0.020242841634899377,\n",
       "    0.02070433311164379,\n",
       "    0.01933878008276224,\n",
       "    0.01892990432679653,\n",
       "    0.018183738552033903,\n",
       "    0.01806538589298725,\n",
       "    0.018353823199868204,\n",
       "    0.017459029331803322,\n",
       "    0.01693067271262407,\n",
       "    0.017179363779723643],\n",
       "   'val_loss': [0.07962222397327423,\n",
       "    0.06571713089942932,\n",
       "    0.05500999093055725,\n",
       "    0.049510303884744644,\n",
       "    0.04875360056757927,\n",
       "    0.04525306075811386,\n",
       "    0.04664410278201103,\n",
       "    0.0468178354203701,\n",
       "    0.05180124565958977,\n",
       "    0.049728307873010635,\n",
       "    0.06814102083444595,\n",
       "    0.04857785627245903,\n",
       "    0.04656204581260681,\n",
       "    0.049948107451200485,\n",
       "    0.051183395087718964,\n",
       "    0.04776058346033096,\n",
       "    0.04997219145298004,\n",
       "    0.0493290014564991,\n",
       "    0.0585993155837059,\n",
       "    0.0607304647564888,\n",
       "    0.0545656681060791,\n",
       "    0.06580250710248947,\n",
       "    0.05077257752418518,\n",
       "    0.0516333132982254,\n",
       "    0.050720710307359695,\n",
       "    0.05164497345685959],\n",
       "   'train_mae': [0.2118481755256653,\n",
       "    0.18216368854045867,\n",
       "    0.17620100677013398,\n",
       "    0.16698424220085145,\n",
       "    0.16173501014709474,\n",
       "    0.1581866979598999,\n",
       "    0.15324430167675018,\n",
       "    0.14484361410140992,\n",
       "    0.14894916042685508,\n",
       "    0.1397700384259224,\n",
       "    0.13975942954421045,\n",
       "    0.1441831573843956,\n",
       "    0.14189542979001998,\n",
       "    0.13723716512322426,\n",
       "    0.1316341146826744,\n",
       "    0.1350018747150898,\n",
       "    0.13202813118696213,\n",
       "    0.1311137095093727,\n",
       "    0.12907826527953148,\n",
       "    0.12553366646170616,\n",
       "    0.12429352402687073,\n",
       "    0.12196696251630783,\n",
       "    0.12467396706342697,\n",
       "    0.1213665895164013,\n",
       "    0.11902932301163674,\n",
       "    0.12093752548098564],\n",
       "   'val_mae': [0.3160422444343567,\n",
       "    0.2992556691169739,\n",
       "    0.28334179520606995,\n",
       "    0.2649707496166229,\n",
       "    0.26219531893730164,\n",
       "    0.25067755579948425,\n",
       "    0.24512283504009247,\n",
       "    0.2493991255760193,\n",
       "    0.25294753909111023,\n",
       "    0.2537142336368561,\n",
       "    0.28979673981666565,\n",
       "    0.23535262048244476,\n",
       "    0.23599472641944885,\n",
       "    0.24118991196155548,\n",
       "    0.24510686099529266,\n",
       "    0.233343243598938,\n",
       "    0.23351161181926727,\n",
       "    0.23397032916545868,\n",
       "    0.2448880672454834,\n",
       "    0.2493220418691635,\n",
       "    0.24739715456962585,\n",
       "    0.25953131914138794,\n",
       "    0.2364886999130249,\n",
       "    0.23938576877117157,\n",
       "    0.23774383962154388,\n",
       "    0.23531435430049896],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005]},\n",
       "  'metrics': {'mse': 0.10329541563987732,\n",
       "   'rmse': 0.3213960417302573,\n",
       "   'mae': 0.23531439900398254,\n",
       "   'r2': 0.27518224716186523,\n",
       "   'mape': 84.27233123779297},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.04146492684429342,\n",
       "    0.03275428983298215,\n",
       "    0.030655697157437153,\n",
       "    0.030257673595439304,\n",
       "    0.028707549737935715,\n",
       "    0.02713167294859886,\n",
       "    0.026336478577418762,\n",
       "    0.025838266550139946,\n",
       "    0.026131930168379437,\n",
       "    0.025034984743053264,\n",
       "    0.024420668625018814,\n",
       "    0.024940105324441738,\n",
       "    0.024038596840744667,\n",
       "    0.024057845669713886,\n",
       "    0.02317589885470542,\n",
       "    0.022967240349812942,\n",
       "    0.021614982831207188,\n",
       "    0.02173429939218543,\n",
       "    0.020406675186346878,\n",
       "    0.022229776802388104,\n",
       "    0.021913935938342052,\n",
       "    0.021494124000045387,\n",
       "    0.020082959058609875,\n",
       "    0.0200679422440854],\n",
       "   'val_loss': [0.03782632574439049,\n",
       "    0.03686075285077095,\n",
       "    0.036554694175720215,\n",
       "    0.03590830788016319,\n",
       "    0.03610960766673088,\n",
       "    0.0370447151362896,\n",
       "    0.038217317312955856,\n",
       "    0.038569126278162,\n",
       "    0.038760699331760406,\n",
       "    0.04204777255654335,\n",
       "    0.04025450348854065,\n",
       "    0.04393578693270683,\n",
       "    0.04599808529019356,\n",
       "    0.04207935184240341,\n",
       "    0.04336059093475342,\n",
       "    0.04296884313225746,\n",
       "    0.042791448533535004,\n",
       "    0.04373779147863388,\n",
       "    0.047083862125873566,\n",
       "    0.04410555958747864,\n",
       "    0.045852143317461014,\n",
       "    0.04455355182290077,\n",
       "    0.045622121542692184,\n",
       "    0.04773297533392906],\n",
       "   'train_mae': [0.21164571155201306,\n",
       "    0.1808897460048849,\n",
       "    0.17656592211940073,\n",
       "    0.17094281315803528,\n",
       "    0.1640912280841307,\n",
       "    0.15956373512744904,\n",
       "    0.15701325170018457,\n",
       "    0.1565227061510086,\n",
       "    0.15357061543247916,\n",
       "    0.15278923172842374,\n",
       "    0.15006228197704663,\n",
       "    0.14922902597622437,\n",
       "    0.14580298486081036,\n",
       "    0.15018589726903223,\n",
       "    0.14359856058250775,\n",
       "    0.14004930718378586,\n",
       "    0.1385308097709309,\n",
       "    0.13702636618505826,\n",
       "    0.1323515691540458,\n",
       "    0.1376891170035709,\n",
       "    0.13712101429700851,\n",
       "    0.13870307870886542,\n",
       "    0.13050511343912644,\n",
       "    0.13058268617499957],\n",
       "   'val_mae': [0.21789959073066711,\n",
       "    0.2279863953590393,\n",
       "    0.21280471980571747,\n",
       "    0.21110373735427856,\n",
       "    0.21061472594738007,\n",
       "    0.21163026988506317,\n",
       "    0.21282294392585754,\n",
       "    0.2262965887784958,\n",
       "    0.2118227183818817,\n",
       "    0.21384307742118835,\n",
       "    0.21889247000217438,\n",
       "    0.2151188850402832,\n",
       "    0.22836834192276,\n",
       "    0.21312180161476135,\n",
       "    0.2140464186668396,\n",
       "    0.21437028050422668,\n",
       "    0.21378426253795624,\n",
       "    0.21476487815380096,\n",
       "    0.22018852829933167,\n",
       "    0.215122789144516,\n",
       "    0.21675772964954376,\n",
       "    0.21593116223812103,\n",
       "    0.21940208971500397,\n",
       "    0.22338692843914032],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005]},\n",
       "  'metrics': {'mse': 0.09546594321727753,\n",
       "   'rmse': 0.30897563531333266,\n",
       "   'mae': 0.22338689863681793,\n",
       "   'r2': -0.047246336936950684,\n",
       "   'mape': 45.464603424072266},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.044480322704960905,\n",
       "    0.03467735322192311,\n",
       "    0.03304900873141984,\n",
       "    0.03110291777799527,\n",
       "    0.030274860560894012,\n",
       "    0.02862304278338949,\n",
       "    0.028525784766922396,\n",
       "    0.028809411606440943,\n",
       "    0.027181876978526514,\n",
       "    0.026226361592610676,\n",
       "    0.026474081988756854,\n",
       "    0.026134340713421505,\n",
       "    0.025682813487946987,\n",
       "    0.025957991213848192,\n",
       "    0.026515485253185034,\n",
       "    0.024840160738676786,\n",
       "    0.02441975474357605,\n",
       "    0.02405627832437555,\n",
       "    0.023325876332819462,\n",
       "    0.023051165820409853,\n",
       "    0.0231264173053205,\n",
       "    0.021446972930183012,\n",
       "    0.021914170278857153,\n",
       "    0.022065997822210193,\n",
       "    0.02177543138774733,\n",
       "    0.021660275602092344,\n",
       "    0.022018426408370335,\n",
       "    0.02096132926332454,\n",
       "    0.020973123687629897,\n",
       "    0.021854490352173645],\n",
       "   'val_loss': [0.04785672947764397,\n",
       "    0.04966948553919792,\n",
       "    0.046410635113716125,\n",
       "    0.04816132038831711,\n",
       "    0.04473220184445381,\n",
       "    0.044266264885663986,\n",
       "    0.03890608251094818,\n",
       "    0.039783280342817307,\n",
       "    0.04094170033931732,\n",
       "    0.036254554986953735,\n",
       "    0.04122412949800491,\n",
       "    0.03797825798392296,\n",
       "    0.04123218357563019,\n",
       "    0.039992883801460266,\n",
       "    0.039122577756643295,\n",
       "    0.03707204759120941,\n",
       "    0.03877246752381325,\n",
       "    0.03828226402401924,\n",
       "    0.041256316006183624,\n",
       "    0.03971252590417862,\n",
       "    0.040076617151498795,\n",
       "    0.03892119601368904,\n",
       "    0.041262686252593994,\n",
       "    0.04077811911702156,\n",
       "    0.0421161986887455,\n",
       "    0.04423459619283676,\n",
       "    0.04272780194878578,\n",
       "    0.0431441105902195,\n",
       "    0.04549024999141693,\n",
       "    0.04474114626646042],\n",
       "   'train_mae': [0.23794864863157272,\n",
       "    0.19772044693430266,\n",
       "    0.18508977194627127,\n",
       "    0.1843002326786518,\n",
       "    0.1777776467303435,\n",
       "    0.17098497847716013,\n",
       "    0.1700268772741159,\n",
       "    0.1673111729323864,\n",
       "    0.1662954663236936,\n",
       "    0.1579428923626741,\n",
       "    0.16434619079033533,\n",
       "    0.15788896443943182,\n",
       "    0.16042431133488813,\n",
       "    0.15433862805366516,\n",
       "    0.16046227887272835,\n",
       "    0.15325384897490343,\n",
       "    0.15244991021851698,\n",
       "    0.1527668945491314,\n",
       "    0.14665859378874302,\n",
       "    0.1486564427614212,\n",
       "    0.14699184149503708,\n",
       "    0.1410402531425158,\n",
       "    0.14159292976061502,\n",
       "    0.1458412930369377,\n",
       "    0.13839358215530714,\n",
       "    0.14343168213963509,\n",
       "    0.13955536670982838,\n",
       "    0.14015296225746474,\n",
       "    0.1354903895407915,\n",
       "    0.14279429242014885],\n",
       "   'val_mae': [0.2533683478832245,\n",
       "    0.24543842673301697,\n",
       "    0.24044516682624817,\n",
       "    0.23862126469612122,\n",
       "    0.22480224072933197,\n",
       "    0.22190627455711365,\n",
       "    0.21010954678058624,\n",
       "    0.22119930386543274,\n",
       "    0.20380374789237976,\n",
       "    0.217740997672081,\n",
       "    0.2024487853050232,\n",
       "    0.20905336737632751,\n",
       "    0.20062170922756195,\n",
       "    0.2266622632741928,\n",
       "    0.20619265735149384,\n",
       "    0.20383894443511963,\n",
       "    0.2093055099248886,\n",
       "    0.2042904794216156,\n",
       "    0.20706088840961456,\n",
       "    0.2084040343761444,\n",
       "    0.20915670692920685,\n",
       "    0.20605851709842682,\n",
       "    0.20634648203849792,\n",
       "    0.205079585313797,\n",
       "    0.2117224484682083,\n",
       "    0.20942501723766327,\n",
       "    0.2150012105703354,\n",
       "    0.21218295395374298,\n",
       "    0.21302661299705505,\n",
       "    0.21327155828475952],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005]},\n",
       "  'metrics': {'mse': 0.08948229998350143,\n",
       "   'rmse': 0.2991359222552541,\n",
       "   'mae': 0.21327155828475952,\n",
       "   'r2': 0.23650729656219482,\n",
       "   'mape': 70.45777130126953},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.038815613549489245,\n",
       "    0.03371834382414818,\n",
       "    0.03211524753043285,\n",
       "    0.03017937406324423,\n",
       "    0.028745831563495673,\n",
       "    0.028091654897882387,\n",
       "    0.02719588907292256,\n",
       "    0.027362430611482032,\n",
       "    0.02589168456884531,\n",
       "    0.025482564304883663,\n",
       "    0.025442225572008353,\n",
       "    0.02617691349811279,\n",
       "    0.02547196499430216,\n",
       "    0.024449721408578064,\n",
       "    0.024604020210412834,\n",
       "    0.023587218557412807,\n",
       "    0.023292569515223686,\n",
       "    0.022528298055896394,\n",
       "    0.022473586579928033,\n",
       "    0.022154165861698296,\n",
       "    0.022430888305489834,\n",
       "    0.021957575558469847,\n",
       "    0.0222871398123411,\n",
       "    0.02285166299686982,\n",
       "    0.02201610029889987],\n",
       "   'val_loss': [0.029383067041635513,\n",
       "    0.03007294051349163,\n",
       "    0.029467429965734482,\n",
       "    0.0284255500882864,\n",
       "    0.024372486397624016,\n",
       "    0.026961827650666237,\n",
       "    0.02651030942797661,\n",
       "    0.030473753809928894,\n",
       "    0.031679727137088776,\n",
       "    0.029128190129995346,\n",
       "    0.024434585124254227,\n",
       "    0.032717783004045486,\n",
       "    0.027538998052477837,\n",
       "    0.026199083775281906,\n",
       "    0.02878989651799202,\n",
       "    0.03141758218407631,\n",
       "    0.03127552941441536,\n",
       "    0.03250263258814812,\n",
       "    0.03087250515818596,\n",
       "    0.03383202105760574,\n",
       "    0.0297600906342268,\n",
       "    0.03294415399432182,\n",
       "    0.03308350592851639,\n",
       "    0.0290833730250597,\n",
       "    0.0330314077436924],\n",
       "   'train_mae': [0.20647211372852325,\n",
       "    0.1917298688338353,\n",
       "    0.18292835698677942,\n",
       "    0.17601239451995263,\n",
       "    0.17060916011150068,\n",
       "    0.16669828272782838,\n",
       "    0.16347863238591415,\n",
       "    0.16121239501696366,\n",
       "    0.16021610681827253,\n",
       "    0.1571191537838716,\n",
       "    0.1538772783600367,\n",
       "    0.15709798209942305,\n",
       "    0.1546990596331083,\n",
       "    0.15206494354284728,\n",
       "    0.15008467837021902,\n",
       "    0.14844929369596335,\n",
       "    0.14507204064956078,\n",
       "    0.14289765518445235,\n",
       "    0.13957958897719017,\n",
       "    0.14406218494360262,\n",
       "    0.13836648487127745,\n",
       "    0.142095464353378,\n",
       "    0.14028075967843717,\n",
       "    0.1450373255289518,\n",
       "    0.13796189771248743],\n",
       "   'val_mae': [0.19782137870788574,\n",
       "    0.19768093526363373,\n",
       "    0.19526930153369904,\n",
       "    0.18952952325344086,\n",
       "    0.16388671100139618,\n",
       "    0.1747927963733673,\n",
       "    0.16556516289710999,\n",
       "    0.18810081481933594,\n",
       "    0.1822492480278015,\n",
       "    0.18195290863513947,\n",
       "    0.1686786562204361,\n",
       "    0.1878785938024521,\n",
       "    0.17014987766742706,\n",
       "    0.1645175665616989,\n",
       "    0.1721746176481247,\n",
       "    0.17708022892475128,\n",
       "    0.1758221536874771,\n",
       "    0.17483820021152496,\n",
       "    0.17274217307567596,\n",
       "    0.17861244082450867,\n",
       "    0.16909433901309967,\n",
       "    0.17673853039741516,\n",
       "    0.17836745083332062,\n",
       "    0.16832299530506134,\n",
       "    0.17698541283607483],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005]},\n",
       "  'metrics': {'mse': 0.0660628229379654,\n",
       "   'rmse': 0.2570268914685103,\n",
       "   'mae': 0.17698542773723602,\n",
       "   'r2': 0.16876119375228882,\n",
       "   'mape': 28.949127197265625},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.039503744803369045,\n",
       "    0.03374870786709445,\n",
       "    0.031960752326995134,\n",
       "    0.030429257213004997,\n",
       "    0.03002745977469853,\n",
       "    0.0276863340820585,\n",
       "    0.02808315133942025,\n",
       "    0.026463831375752176,\n",
       "    0.027407640192125524,\n",
       "    0.026470213862402097,\n",
       "    0.026271614379116466,\n",
       "    0.024867061459060227,\n",
       "    0.025685330985912254,\n",
       "    0.02552148187533021,\n",
       "    0.024408334999212196,\n",
       "    0.024763571763677255,\n",
       "    0.024878608994185925,\n",
       "    0.026288368473095552,\n",
       "    0.024308409275753156,\n",
       "    0.024144628351288184,\n",
       "    0.02369466470554471,\n",
       "    0.0225821718174432,\n",
       "    0.022785371568586146,\n",
       "    0.02236285073948758,\n",
       "    0.021144221503553644,\n",
       "    0.021466040518134832,\n",
       "    0.02149165315287454,\n",
       "    0.0214488986613495,\n",
       "    0.022147162218711207,\n",
       "    0.021408582904509137,\n",
       "    0.02066137781366706,\n",
       "    0.01982352856014456,\n",
       "    0.019292361635182585,\n",
       "    0.019674486946314573,\n",
       "    0.020206459199211428,\n",
       "    0.019624588678457906,\n",
       "    0.018778613036764518,\n",
       "    0.02044719025226576,\n",
       "    0.0183742645728801,\n",
       "    0.019544960066143955,\n",
       "    0.018167320572371994,\n",
       "    0.01786440970110042,\n",
       "    0.017889031807758977,\n",
       "    0.016063378724668707,\n",
       "    0.016231626737862825,\n",
       "    0.015230695317898477,\n",
       "    0.015038469739790474,\n",
       "    0.015063550655863114,\n",
       "    0.01398594690752881,\n",
       "    0.01487090315536729],\n",
       "   'val_loss': [0.02406056597828865,\n",
       "    0.024473978206515312,\n",
       "    0.024837184697389603,\n",
       "    0.022671842947602272,\n",
       "    0.022187672555446625,\n",
       "    0.021719614043831825,\n",
       "    0.02152378298342228,\n",
       "    0.02036973647773266,\n",
       "    0.019948666915297508,\n",
       "    0.02098851650953293,\n",
       "    0.02068893238902092,\n",
       "    0.02082170732319355,\n",
       "    0.021945342421531677,\n",
       "    0.020754234865307808,\n",
       "    0.019132792949676514,\n",
       "    0.0201568640768528,\n",
       "    0.019425729289650917,\n",
       "    0.020183511078357697,\n",
       "    0.020000210031867027,\n",
       "    0.019335100427269936,\n",
       "    0.020239761099219322,\n",
       "    0.02038647048175335,\n",
       "    0.01995076797902584,\n",
       "    0.018889639526605606,\n",
       "    0.017720850184559822,\n",
       "    0.018743766471743584,\n",
       "    0.018528442829847336,\n",
       "    0.018407123163342476,\n",
       "    0.017942799255251884,\n",
       "    0.01801244542002678,\n",
       "    0.018516365438699722,\n",
       "    0.018125571310520172,\n",
       "    0.01801116392016411,\n",
       "    0.017665758728981018,\n",
       "    0.01798819564282894,\n",
       "    0.016319405287504196,\n",
       "    0.02065357193350792,\n",
       "    0.019930554553866386,\n",
       "    0.018740640953183174,\n",
       "    0.017503274604678154,\n",
       "    0.017773890867829323,\n",
       "    0.017909284681081772,\n",
       "    0.016228996217250824,\n",
       "    0.016803573817014694,\n",
       "    0.018583575263619423,\n",
       "    0.01577007584273815,\n",
       "    0.01717486046254635,\n",
       "    0.01789226569235325,\n",
       "    0.016346650198101997,\n",
       "    0.015742262825369835],\n",
       "   'train_mae': [0.2112354380743844,\n",
       "    0.19178092692579543,\n",
       "    0.18343211923326766,\n",
       "    0.17704839152949198,\n",
       "    0.1720177818621908,\n",
       "    0.16893962451389857,\n",
       "    0.16450338172061102,\n",
       "    0.1592887102493218,\n",
       "    0.16285797847168787,\n",
       "    0.1600813732615539,\n",
       "    0.15770582588655607,\n",
       "    0.15584092906543187,\n",
       "    0.15421192507658685,\n",
       "    0.15717179753950664,\n",
       "    0.15277968240635736,\n",
       "    0.1537722987788064,\n",
       "    0.1490823525403227,\n",
       "    0.1548362821340561,\n",
       "    0.15069855749607086,\n",
       "    0.14817051216959953,\n",
       "    0.1497224856700216,\n",
       "    0.14414483521665847,\n",
       "    0.1462211890944413,\n",
       "    0.14561612904071808,\n",
       "    0.13805702275463513,\n",
       "    0.13923597655126027,\n",
       "    0.1452283790068967,\n",
       "    0.13708519988826343,\n",
       "    0.13745551077382906,\n",
       "    0.1399319033537592,\n",
       "    0.13821589148470334,\n",
       "    0.13089961665017263,\n",
       "    0.13121129465954645,\n",
       "    0.13334220754248755,\n",
       "    0.1343547904065677,\n",
       "    0.13148843018071993,\n",
       "    0.1314805613032409,\n",
       "    0.1333801890058177,\n",
       "    0.1279454380273819,\n",
       "    0.1297468218420233,\n",
       "    0.13279316574335098,\n",
       "    0.12634448441011564,\n",
       "    0.12450161948800087,\n",
       "    0.12160007283091545,\n",
       "    0.11858870035835675,\n",
       "    0.11396106226103646,\n",
       "    0.11563931618418012,\n",
       "    0.11695766129664012,\n",
       "    0.11121659725904465,\n",
       "    0.11423472953694207],\n",
       "   'val_mae': [0.1491984874010086,\n",
       "    0.15096357464790344,\n",
       "    0.15019671618938446,\n",
       "    0.13609544932842255,\n",
       "    0.14172141253948212,\n",
       "    0.12323333323001862,\n",
       "    0.11720818281173706,\n",
       "    0.1189553514122963,\n",
       "    0.1225561648607254,\n",
       "    0.12275853008031845,\n",
       "    0.12033159285783768,\n",
       "    0.12326736003160477,\n",
       "    0.1316477358341217,\n",
       "    0.1356130838394165,\n",
       "    0.12396591901779175,\n",
       "    0.11927969008684158,\n",
       "    0.11776432394981384,\n",
       "    0.13276061415672302,\n",
       "    0.12055453658103943,\n",
       "    0.11272185295820236,\n",
       "    0.12919235229492188,\n",
       "    0.11098948121070862,\n",
       "    0.11837680637836456,\n",
       "    0.12618803977966309,\n",
       "    0.11306630074977875,\n",
       "    0.11803852766752243,\n",
       "    0.11167926341295242,\n",
       "    0.1140550896525383,\n",
       "    0.11799081414937973,\n",
       "    0.10766606777906418,\n",
       "    0.11424639075994492,\n",
       "    0.10834784060716629,\n",
       "    0.1059076115489006,\n",
       "    0.10578601062297821,\n",
       "    0.1182338148355484,\n",
       "    0.0994759127497673,\n",
       "    0.10804591327905655,\n",
       "    0.12243018299341202,\n",
       "    0.1188206821680069,\n",
       "    0.11577204614877701,\n",
       "    0.09993362426757812,\n",
       "    0.10466627776622772,\n",
       "    0.10800110548734665,\n",
       "    0.10243523865938187,\n",
       "    0.10487868636846542,\n",
       "    0.1116584911942482,\n",
       "    0.106486976146698,\n",
       "    0.10444452613592148,\n",
       "    0.10546988993883133,\n",
       "    0.10018327832221985],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001]},\n",
       "  'metrics': {'mse': 0.03148452565073967,\n",
       "   'rmse': 0.17743879409740043,\n",
       "   'mae': 0.10018330067396164,\n",
       "   'r2': 0.6261155605316162,\n",
       "   'mape': 27.18384552001953},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.03788718692958355,\n",
       "    0.03299435190856457,\n",
       "    0.031149000177780786,\n",
       "    0.029882816473642985,\n",
       "    0.028418725728988646,\n",
       "    0.028143957381447156,\n",
       "    0.027148237203558287,\n",
       "    0.026632648954788844,\n",
       "    0.025632678158581258,\n",
       "    0.025229238097866376,\n",
       "    0.02523862545688947,\n",
       "    0.025646159052848817,\n",
       "    0.02501855914791425,\n",
       "    0.025848521043856938,\n",
       "    0.02436160494883855,\n",
       "    0.024222138958672684,\n",
       "    0.02456200818220774,\n",
       "    0.02306837247063716,\n",
       "    0.02354901737223069,\n",
       "    0.02391948060443004,\n",
       "    0.02369348475088676,\n",
       "    0.022477812630434833,\n",
       "    0.021973878766099613,\n",
       "    0.021731149901946387,\n",
       "    0.022141701231400173,\n",
       "    0.021664408221840858,\n",
       "    0.02224042291442553,\n",
       "    0.021244091416398683,\n",
       "    0.020760743257900078,\n",
       "    0.020609868317842485,\n",
       "    0.019686045621832213,\n",
       "    0.019059362448751928,\n",
       "    0.018227982272704443,\n",
       "    0.01860309299081564,\n",
       "    0.018977081527312596,\n",
       "    0.01792305465787649,\n",
       "    0.0169318376109004,\n",
       "    0.017587703776856263,\n",
       "    0.01683832568426927],\n",
       "   'val_loss': [0.01861860230565071,\n",
       "    0.01868448592722416,\n",
       "    0.018513556569814682,\n",
       "    0.018519314005970955,\n",
       "    0.019442014396190643,\n",
       "    0.01812194660305977,\n",
       "    0.01863871142268181,\n",
       "    0.018580511212348938,\n",
       "    0.017930466681718826,\n",
       "    0.019629361107945442,\n",
       "    0.018338706344366074,\n",
       "    0.018332036212086678,\n",
       "    0.01914915256202221,\n",
       "    0.01821301504969597,\n",
       "    0.019738277420401573,\n",
       "    0.01956423744559288,\n",
       "    0.01942422240972519,\n",
       "    0.021730748936533928,\n",
       "    0.01784539595246315,\n",
       "    0.022387513890862465,\n",
       "    0.01945585012435913,\n",
       "    0.021993154659867287,\n",
       "    0.021227382123470306,\n",
       "    0.020444802939891815,\n",
       "    0.021949876099824905,\n",
       "    0.018702296540141106,\n",
       "    0.022628845646977425,\n",
       "    0.020257001742720604,\n",
       "    0.021070891991257668,\n",
       "    0.020981192588806152,\n",
       "    0.019909285008907318,\n",
       "    0.02147763967514038,\n",
       "    0.021388916298747063,\n",
       "    0.02062183804810047,\n",
       "    0.02037121169269085,\n",
       "    0.021468287333846092,\n",
       "    0.020575663074851036,\n",
       "    0.021309223026037216,\n",
       "    0.019928693771362305],\n",
       "   'train_mae': [0.1997892419497172,\n",
       "    0.18641638159751892,\n",
       "    0.17923707167307537,\n",
       "    0.17793877919514975,\n",
       "    0.17075136601924895,\n",
       "    0.16597330967585247,\n",
       "    0.16329961617787678,\n",
       "    0.1598917216062546,\n",
       "    0.15904712875684102,\n",
       "    0.1543558011452357,\n",
       "    0.15629223187764485,\n",
       "    0.15545303424199422,\n",
       "    0.1533179461956024,\n",
       "    0.15345780849456786,\n",
       "    0.15141624063253403,\n",
       "    0.15077007015546162,\n",
       "    0.1508707270026207,\n",
       "    0.14926083932320278,\n",
       "    0.14259397536516188,\n",
       "    0.15037719160318375,\n",
       "    0.1473596195379893,\n",
       "    0.14384837249914806,\n",
       "    0.14249761352936427,\n",
       "    0.14199222375949225,\n",
       "    0.1406714012225469,\n",
       "    0.13843214760224024,\n",
       "    0.14175057311852773,\n",
       "    0.14021472185850142,\n",
       "    0.1350011726220449,\n",
       "    0.13694615066051483,\n",
       "    0.13031978160142899,\n",
       "    0.13028287291526794,\n",
       "    0.1279768024881681,\n",
       "    0.12662241011857986,\n",
       "    0.12754714637994766,\n",
       "    0.12655096848805744,\n",
       "    0.12161074827114741,\n",
       "    0.12439180463552475,\n",
       "    0.12173836131890615],\n",
       "   'val_mae': [0.12973876297473907,\n",
       "    0.12092845886945724,\n",
       "    0.13405339419841766,\n",
       "    0.11884724348783493,\n",
       "    0.11800631135702133,\n",
       "    0.1279948204755783,\n",
       "    0.1337842047214508,\n",
       "    0.11612144112586975,\n",
       "    0.12242674082517624,\n",
       "    0.12547136843204498,\n",
       "    0.11598239094018936,\n",
       "    0.11960405856370926,\n",
       "    0.11168492585420609,\n",
       "    0.12171564996242523,\n",
       "    0.11392022669315338,\n",
       "    0.11267180740833282,\n",
       "    0.11505342274904251,\n",
       "    0.11444378644227982,\n",
       "    0.12216459214687347,\n",
       "    0.12213850766420364,\n",
       "    0.11405302584171295,\n",
       "    0.11762996762990952,\n",
       "    0.11727966368198395,\n",
       "    0.1134866252541542,\n",
       "    0.11164619028568268,\n",
       "    0.12313845008611679,\n",
       "    0.11583893746137619,\n",
       "    0.11408427357673645,\n",
       "    0.11286032944917679,\n",
       "    0.10998713225126266,\n",
       "    0.11333230137825012,\n",
       "    0.11205635964870453,\n",
       "    0.11016286164522171,\n",
       "    0.1102861762046814,\n",
       "    0.11169734597206116,\n",
       "    0.11071174591779709,\n",
       "    0.11261175572872162,\n",
       "    0.10975321382284164,\n",
       "    0.11425688117742538],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005]},\n",
       "  'metrics': {'mse': 0.03985738754272461,\n",
       "   'rmse': 0.19964315050290257,\n",
       "   'mae': 0.11425688117742538,\n",
       "   'r2': 0.2875586152076721,\n",
       "   'mape': 42.1710205078125},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.04024163063149899,\n",
       "    0.032973867491818964,\n",
       "    0.032324525993317366,\n",
       "    0.030760459834709764,\n",
       "    0.02868863579351455,\n",
       "    0.02712088485714048,\n",
       "    0.027041175053454936,\n",
       "    0.026980345835909247,\n",
       "    0.025160498218610883,\n",
       "    0.025231583509594202,\n",
       "    0.024938883318100125,\n",
       "    0.024894411675632,\n",
       "    0.02377297164639458,\n",
       "    0.024511840718332678,\n",
       "    0.02417356870137155,\n",
       "    0.02364523889264092,\n",
       "    0.02307193074375391,\n",
       "    0.021843655442353338,\n",
       "    0.022386154683772475,\n",
       "    0.021785212156828493,\n",
       "    0.022176679514814168,\n",
       "    0.022253400005865842,\n",
       "    0.0220376817160286,\n",
       "    0.020808027358725667,\n",
       "    0.020057785557582974,\n",
       "    0.02053162467200309,\n",
       "    0.019878890656400472,\n",
       "    0.020026524434797466,\n",
       "    0.019861822540406138,\n",
       "    0.0189423612318933,\n",
       "    0.019133584515657276,\n",
       "    0.01919749309308827,\n",
       "    0.01878793543437496,\n",
       "    0.01873336941935122,\n",
       "    0.01808528701076284,\n",
       "    0.01791389856953174,\n",
       "    0.018205751926871017,\n",
       "    0.018908373778685927,\n",
       "    0.01837400274234824,\n",
       "    0.018844717531464994,\n",
       "    0.018116524443030357,\n",
       "    0.017894736432936043,\n",
       "    0.016200619633309543,\n",
       "    0.018556057359091938,\n",
       "    0.01727775571634993,\n",
       "    0.016649319062707946,\n",
       "    0.015989926061592996,\n",
       "    0.016373621037928388,\n",
       "    0.016450404305942357,\n",
       "    0.01589913497446105],\n",
       "   'val_loss': [0.019012629985809326,\n",
       "    0.017314888536930084,\n",
       "    0.0169313196092844,\n",
       "    0.017368365079164505,\n",
       "    0.0171930193901062,\n",
       "    0.017129959538578987,\n",
       "    0.01777423545718193,\n",
       "    0.01686958596110344,\n",
       "    0.017324138432741165,\n",
       "    0.017043931409716606,\n",
       "    0.017388660460710526,\n",
       "    0.017598891630768776,\n",
       "    0.01810656674206257,\n",
       "    0.017697574570775032,\n",
       "    0.017318814992904663,\n",
       "    0.0172582995146513,\n",
       "    0.017337383702397346,\n",
       "    0.01696442998945713,\n",
       "    0.01737067848443985,\n",
       "    0.017060494050383568,\n",
       "    0.016612835228443146,\n",
       "    0.01794901117682457,\n",
       "    0.01716662384569645,\n",
       "    0.0166705921292305,\n",
       "    0.017060065641999245,\n",
       "    0.01708231307566166,\n",
       "    0.016425805166363716,\n",
       "    0.01726659946143627,\n",
       "    0.01685437001287937,\n",
       "    0.016880447044968605,\n",
       "    0.01664057746529579,\n",
       "    0.016698364168405533,\n",
       "    0.016382068395614624,\n",
       "    0.01679196208715439,\n",
       "    0.016612796112895012,\n",
       "    0.016345739364624023,\n",
       "    0.01679205894470215,\n",
       "    0.016436168923974037,\n",
       "    0.01619366742670536,\n",
       "    0.016319289803504944,\n",
       "    0.01687045767903328,\n",
       "    0.016350312158465385,\n",
       "    0.01635991968214512,\n",
       "    0.016824303194880486,\n",
       "    0.01677483133971691,\n",
       "    0.0169055238366127,\n",
       "    0.01643790490925312,\n",
       "    0.01634334959089756,\n",
       "    0.01629173569381237,\n",
       "    0.016697047278285027],\n",
       "   'train_mae': [0.20853293128311634,\n",
       "    0.18761167861521244,\n",
       "    0.18453054036945105,\n",
       "    0.17815840430557728,\n",
       "    0.17015677317976952,\n",
       "    0.1615707315504551,\n",
       "    0.16349210310727358,\n",
       "    0.15955023607239127,\n",
       "    0.158178448677063,\n",
       "    0.15485851047560573,\n",
       "    0.15419465815648437,\n",
       "    0.15150005836039782,\n",
       "    0.1507259220816195,\n",
       "    0.15088710561394691,\n",
       "    0.15356954373419285,\n",
       "    0.1453448487445712,\n",
       "    0.14789208211004734,\n",
       "    0.13960651168599725,\n",
       "    0.1429528258740902,\n",
       "    0.1409306824207306,\n",
       "    0.14112531766295433,\n",
       "    0.1432769037783146,\n",
       "    0.1399760008789599,\n",
       "    0.14060924435034394,\n",
       "    0.1337904343381524,\n",
       "    0.13277713302522898,\n",
       "    0.13263544254004955,\n",
       "    0.1364810336381197,\n",
       "    0.13075182447209954,\n",
       "    0.13117410149425268,\n",
       "    0.12763337278738618,\n",
       "    0.12933678505942225,\n",
       "    0.12897037528455257,\n",
       "    0.1274336939677596,\n",
       "    0.12655612733215094,\n",
       "    0.12616557767614722,\n",
       "    0.12628456950187683,\n",
       "    0.1317758997902274,\n",
       "    0.12452544458210468,\n",
       "    0.1314782747067511,\n",
       "    0.12652885727584362,\n",
       "    0.12575009418651462,\n",
       "    0.12041064631193876,\n",
       "    0.12789406767114997,\n",
       "    0.11940501909703016,\n",
       "    0.12564680352807045,\n",
       "    0.11732478719204664,\n",
       "    0.12034629937261343,\n",
       "    0.1194370468147099,\n",
       "    0.11981044709682465],\n",
       "   'val_mae': [0.12714633345603943,\n",
       "    0.11471013724803925,\n",
       "    0.1232534721493721,\n",
       "    0.10683506727218628,\n",
       "    0.10196022689342499,\n",
       "    0.1055346131324768,\n",
       "    0.09309990704059601,\n",
       "    0.10430657863616943,\n",
       "    0.10585448145866394,\n",
       "    0.10314542800188065,\n",
       "    0.1089620441198349,\n",
       "    0.1080000028014183,\n",
       "    0.0932106226682663,\n",
       "    0.12347107380628586,\n",
       "    0.10346249490976334,\n",
       "    0.09907255321741104,\n",
       "    0.10838103294372559,\n",
       "    0.10603609681129456,\n",
       "    0.10923796892166138,\n",
       "    0.10036148130893707,\n",
       "    0.1131972149014473,\n",
       "    0.0974011942744255,\n",
       "    0.12294115126132965,\n",
       "    0.10231412202119827,\n",
       "    0.10435616970062256,\n",
       "    0.10402867943048477,\n",
       "    0.1064407080411911,\n",
       "    0.09409406036138535,\n",
       "    0.11809416115283966,\n",
       "    0.09884648770093918,\n",
       "    0.10268139839172363,\n",
       "    0.10818038135766983,\n",
       "    0.10394534468650818,\n",
       "    0.11254843324422836,\n",
       "    0.09836804121732712,\n",
       "    0.09439540654420853,\n",
       "    0.10345130413770676,\n",
       "    0.11042813211679459,\n",
       "    0.10788386315107346,\n",
       "    0.1094052717089653,\n",
       "    0.10506922006607056,\n",
       "    0.104693204164505,\n",
       "    0.0945790633559227,\n",
       "    0.09255307912826538,\n",
       "    0.12194053828716278,\n",
       "    0.09340237081050873,\n",
       "    0.11098676174879074,\n",
       "    0.10187997668981552,\n",
       "    0.10634270310401917,\n",
       "    0.11186205595731735],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005]},\n",
       "  'metrics': {'mse': 0.03339409455657005,\n",
       "   'rmse': 0.18274051153635873,\n",
       "   'mae': 0.11186208575963974,\n",
       "   'r2': 0.43103325366973877,\n",
       "   'mape': 25.231674194335938},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.03850892953136388,\n",
       "    0.032292914083775354,\n",
       "    0.031062687911531505,\n",
       "    0.028840327964109534,\n",
       "    0.02795821735087563,\n",
       "    0.026503928663099512,\n",
       "    0.027213702745297375,\n",
       "    0.024732338593286628,\n",
       "    0.02534640098319334,\n",
       "    0.02450713509803309,\n",
       "    0.02523490446893608,\n",
       "    0.024288657285711345,\n",
       "    0.02462577447295189,\n",
       "    0.02343800658469691,\n",
       "    0.0242807682713165,\n",
       "    0.024296984733904108,\n",
       "    0.022808854358599466,\n",
       "    0.022486143745481968,\n",
       "    0.023464940214420065,\n",
       "    0.021473003003527138,\n",
       "    0.023603804409503937,\n",
       "    0.0230654738843441,\n",
       "    0.02209727542803568,\n",
       "    0.021581334335839048,\n",
       "    0.021862298852818852,\n",
       "    0.022064566612243652,\n",
       "    0.021122074412072405,\n",
       "    0.02007780098082388,\n",
       "    0.018787822576568407,\n",
       "    0.01857253142139491,\n",
       "    0.019414147690815085,\n",
       "    0.018252584042356294,\n",
       "    0.019154281738926384,\n",
       "    0.01822054194396033,\n",
       "    0.01849470314953257,\n",
       "    0.01859687444041757,\n",
       "    0.01925646190476768,\n",
       "    0.017624092781368422,\n",
       "    0.01675272809670252,\n",
       "    0.016937035026357454,\n",
       "    0.016475433894597432,\n",
       "    0.015595220631974585,\n",
       "    0.015957260833067054,\n",
       "    0.015994329717667663,\n",
       "    0.015595683718428892,\n",
       "    0.015189099454266183,\n",
       "    0.015184694901108742,\n",
       "    0.014959497815545867],\n",
       "   'val_loss': [0.015309859998524189,\n",
       "    0.015300149098038673,\n",
       "    0.014707333408296108,\n",
       "    0.014423842541873455,\n",
       "    0.01410029735416174,\n",
       "    0.01400389801710844,\n",
       "    0.013358627445995808,\n",
       "    0.01436805073171854,\n",
       "    0.013823643326759338,\n",
       "    0.014529290609061718,\n",
       "    0.013721217401325703,\n",
       "    0.013670017011463642,\n",
       "    0.013381758704781532,\n",
       "    0.01409704890102148,\n",
       "    0.013286558911204338,\n",
       "    0.01340631116181612,\n",
       "    0.013908777385950089,\n",
       "    0.013529011979699135,\n",
       "    0.013494264334440231,\n",
       "    0.014462732709944248,\n",
       "    0.01384477037936449,\n",
       "    0.01441134698688984,\n",
       "    0.014232255518436432,\n",
       "    0.01558700855821371,\n",
       "    0.014684304594993591,\n",
       "    0.013958202674984932,\n",
       "    0.013303340412676334,\n",
       "    0.013228921219706535,\n",
       "    0.013680300675332546,\n",
       "    0.015591767616569996,\n",
       "    0.014094047248363495,\n",
       "    0.014586308039724827,\n",
       "    0.014171013608574867,\n",
       "    0.014282913878560066,\n",
       "    0.01336422935128212,\n",
       "    0.013462900184094906,\n",
       "    0.016722768545150757,\n",
       "    0.01359737478196621,\n",
       "    0.01483394019305706,\n",
       "    0.014175244607031345,\n",
       "    0.014711004681885242,\n",
       "    0.014553248882293701,\n",
       "    0.01387814898043871,\n",
       "    0.01447967253625393,\n",
       "    0.014523166231811047,\n",
       "    0.016869474202394485,\n",
       "    0.015660271048545837,\n",
       "    0.016764290630817413],\n",
       "   'train_mae': [0.20518720412955566,\n",
       "    0.18113080193014705,\n",
       "    0.1796667777440127,\n",
       "    0.17026915094431708,\n",
       "    0.165625422316439,\n",
       "    0.15706720246988184,\n",
       "    0.1599687521948534,\n",
       "    0.153669453719083,\n",
       "    0.15336622955167994,\n",
       "    0.14972119559259975,\n",
       "    0.1526815102380865,\n",
       "    0.1477008589050349,\n",
       "    0.15178471759838216,\n",
       "    0.14689688384532928,\n",
       "    0.15200706133071115,\n",
       "    0.14598558229558609,\n",
       "    0.14510995412574096,\n",
       "    0.14210949443718968,\n",
       "    0.1470345412107075,\n",
       "    0.14052347137647517,\n",
       "    0.14564626313307705,\n",
       "    0.14384335893041947,\n",
       "    0.14377583870116403,\n",
       "    0.14009797923705158,\n",
       "    0.1406390154186417,\n",
       "    0.13875568044536254,\n",
       "    0.13596737253315308,\n",
       "    0.13391171877875047,\n",
       "    0.12942367090898402,\n",
       "    0.1266226623864735,\n",
       "    0.12974421942935271,\n",
       "    0.12657025894697974,\n",
       "    0.12895344416884816,\n",
       "    0.12462168975788004,\n",
       "    0.13064075699623892,\n",
       "    0.12424135690226275,\n",
       "    0.12834271963904886,\n",
       "    0.1254404454546816,\n",
       "    0.11925676158245872,\n",
       "    0.12198065396617441,\n",
       "    0.1179468605448218,\n",
       "    0.11716818940990112,\n",
       "    0.11768839376814225,\n",
       "    0.11839036774985931,\n",
       "    0.11648869821253945,\n",
       "    0.11462086263824911,\n",
       "    0.11470062285661697,\n",
       "    0.11363497770884458],\n",
       "   'val_mae': [0.10757329314947128,\n",
       "    0.11710423231124878,\n",
       "    0.09905741363763809,\n",
       "    0.09812451153993607,\n",
       "    0.09979067742824554,\n",
       "    0.09466511011123657,\n",
       "    0.09738018363714218,\n",
       "    0.1215992420911789,\n",
       "    0.09351713210344315,\n",
       "    0.12424362450838089,\n",
       "    0.10930193215608597,\n",
       "    0.10704641789197922,\n",
       "    0.10205398499965668,\n",
       "    0.117556631565094,\n",
       "    0.10381068289279938,\n",
       "    0.10580246150493622,\n",
       "    0.11548855900764465,\n",
       "    0.09721352905035019,\n",
       "    0.10713344067335129,\n",
       "    0.11958591639995575,\n",
       "    0.1070481538772583,\n",
       "    0.12074119597673416,\n",
       "    0.11267309635877609,\n",
       "    0.12808266282081604,\n",
       "    0.08407079428434372,\n",
       "    0.10018830001354218,\n",
       "    0.09844885766506195,\n",
       "    0.10108345001935959,\n",
       "    0.10680756717920303,\n",
       "    0.12559722363948822,\n",
       "    0.11122263222932816,\n",
       "    0.11824876070022583,\n",
       "    0.1023973822593689,\n",
       "    0.11442939192056656,\n",
       "    0.10239563137292862,\n",
       "    0.10713852196931839,\n",
       "    0.13616228103637695,\n",
       "    0.10414554178714752,\n",
       "    0.11877245455980301,\n",
       "    0.11002547293901443,\n",
       "    0.11470136791467667,\n",
       "    0.11396045982837677,\n",
       "    0.10828300565481186,\n",
       "    0.11374011635780334,\n",
       "    0.1124584972858429,\n",
       "    0.12528257071971893,\n",
       "    0.12167059630155563,\n",
       "    0.12650184333324432],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.00025,\n",
       "    0.00025,\n",
       "    0.00025,\n",
       "    0.00025,\n",
       "    0.00025,\n",
       "    0.00025,\n",
       "    0.00025,\n",
       "    0.00025,\n",
       "    0.00025]},\n",
       "  'metrics': {'mse': 0.03352858126163483,\n",
       "   'rmse': 0.18310811358766937,\n",
       "   'mae': 0.12650185823440552,\n",
       "   'r2': 0.22459298372268677,\n",
       "   'mape': 19.122329711914062},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.036900052800774574,\n",
       "    0.031245859546793833,\n",
       "    0.02845188147491879,\n",
       "    0.027156068839960627,\n",
       "    0.026132941866914432,\n",
       "    0.025326252397563722,\n",
       "    0.02517426231255134,\n",
       "    0.024153765394455858,\n",
       "    0.024261228119333584,\n",
       "    0.023515139395991962,\n",
       "    0.023919289362513356,\n",
       "    0.022943761872334614,\n",
       "    0.023407529418667156,\n",
       "    0.023186793964770105,\n",
       "    0.023894469978080854,\n",
       "    0.022999256228407223,\n",
       "    0.022863153368234634,\n",
       "    0.021068543661385775,\n",
       "    0.020821949415322807,\n",
       "    0.02124353963881731,\n",
       "    0.02116113616567519,\n",
       "    0.02108230395242572,\n",
       "    0.020559354219585657,\n",
       "    0.020711705450796418,\n",
       "    0.02015666937869456],\n",
       "   'val_loss': [0.028167415410280228,\n",
       "    0.026777109131217003,\n",
       "    0.025773223489522934,\n",
       "    0.02663450501859188,\n",
       "    0.025563117116689682,\n",
       "    0.026382530108094215,\n",
       "    0.025678694248199463,\n",
       "    0.025767970830202103,\n",
       "    0.025905171409249306,\n",
       "    0.02561355009675026,\n",
       "    0.0259705837816,\n",
       "    0.02592337317764759,\n",
       "    0.025863198563456535,\n",
       "    0.02611757256090641,\n",
       "    0.027285443618893623,\n",
       "    0.026656674221158028,\n",
       "    0.027263954281806946,\n",
       "    0.02715328335762024,\n",
       "    0.027577539905905724,\n",
       "    0.02906336449086666,\n",
       "    0.026251155883073807,\n",
       "    0.027917589992284775,\n",
       "    0.026460731402039528,\n",
       "    0.02831425704061985,\n",
       "    0.027219660580158234],\n",
       "   'train_mae': [0.19389801886346605,\n",
       "    0.17999260210328633,\n",
       "    0.16437583789229393,\n",
       "    0.16210661455988884,\n",
       "    0.15555519196722242,\n",
       "    0.15436146077182558,\n",
       "    0.1532873631351524,\n",
       "    0.14965620636940002,\n",
       "    0.14847905726896393,\n",
       "    0.1472375136282709,\n",
       "    0.1514473586446709,\n",
       "    0.14438564537300003,\n",
       "    0.14408384594652388,\n",
       "    0.14880778185195392,\n",
       "    0.1463409819536739,\n",
       "    0.14257963622609773,\n",
       "    0.1446535943282975,\n",
       "    0.13706717805729973,\n",
       "    0.1370795009036859,\n",
       "    0.13476047582096523,\n",
       "    0.1372517185906569,\n",
       "    0.13490301867326102,\n",
       "    0.1344105245338546,\n",
       "    0.13704683010776839,\n",
       "    0.13343576217691103],\n",
       "   'val_mae': [0.15954594314098358,\n",
       "    0.138843834400177,\n",
       "    0.13065943121910095,\n",
       "    0.13128149509429932,\n",
       "    0.1232934519648552,\n",
       "    0.12472041696310043,\n",
       "    0.11738041043281555,\n",
       "    0.13396872580051422,\n",
       "    0.13330046832561493,\n",
       "    0.11452797800302505,\n",
       "    0.1160263940691948,\n",
       "    0.137022003531456,\n",
       "    0.12745805084705353,\n",
       "    0.1250022053718567,\n",
       "    0.1452202945947647,\n",
       "    0.13405515253543854,\n",
       "    0.13658832013607025,\n",
       "    0.13253392279148102,\n",
       "    0.13450947403907776,\n",
       "    0.1494704633951187,\n",
       "    0.12262482196092606,\n",
       "    0.13810335099697113,\n",
       "    0.1235242486000061,\n",
       "    0.1441122144460678,\n",
       "    0.12796995043754578],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005]},\n",
       "  'metrics': {'mse': 0.05473346635699272,\n",
       "   'rmse': 0.2339518462354865,\n",
       "   'mae': 0.12796993553638458,\n",
       "   'r2': 0.3060649037361145,\n",
       "   'mape': 12.768417358398438},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.03690303950325439,\n",
       "    0.03078429851877062,\n",
       "    0.030230689891859105,\n",
       "    0.02748454950357738,\n",
       "    0.02692958114570693,\n",
       "    0.02575992067393504,\n",
       "    0.026045246049761772,\n",
       "    0.024774242437591677,\n",
       "    0.024602496506352173,\n",
       "    0.02674881971784328,\n",
       "    0.024407904497102687,\n",
       "    0.025021901795346486,\n",
       "    0.024282215653281463,\n",
       "    0.024419113993644714,\n",
       "    0.024553701918768257,\n",
       "    0.023972592757720696,\n",
       "    0.02322780086021674,\n",
       "    0.022045181377937918,\n",
       "    0.02254290069992605,\n",
       "    0.02259401947652039,\n",
       "    0.02141272698185946,\n",
       "    0.021695616557017752,\n",
       "    0.021087440318967168,\n",
       "    0.021495493864150422,\n",
       "    0.020961571827922996,\n",
       "    0.02142453953427704,\n",
       "    0.02045948847540115,\n",
       "    0.019924843879906756,\n",
       "    0.018549268011395868,\n",
       "    0.01798418965680819,\n",
       "    0.019329725029437167,\n",
       "    0.018477534679205793,\n",
       "    0.01774381331511234,\n",
       "    0.018139604135955636,\n",
       "    0.018408972769975662,\n",
       "    0.017678894788811083],\n",
       "   'val_loss': [0.02090335264801979,\n",
       "    0.020581508055329323,\n",
       "    0.020394911989569664,\n",
       "    0.020288845524191856,\n",
       "    0.01966756209731102,\n",
       "    0.018524324521422386,\n",
       "    0.018911398947238922,\n",
       "    0.018309030681848526,\n",
       "    0.017315920442342758,\n",
       "    0.017233634367585182,\n",
       "    0.018135633319616318,\n",
       "    0.017447439953684807,\n",
       "    0.017839262261986732,\n",
       "    0.016843561083078384,\n",
       "    0.017303617671132088,\n",
       "    0.016587253659963608,\n",
       "    0.01795395277440548,\n",
       "    0.01753394491970539,\n",
       "    0.017796408385038376,\n",
       "    0.017807714641094208,\n",
       "    0.0173280518501997,\n",
       "    0.01733436807990074,\n",
       "    0.018257347866892815,\n",
       "    0.01769173890352249,\n",
       "    0.016763733699917793,\n",
       "    0.017576053738594055,\n",
       "    0.017510348930954933,\n",
       "    0.016871219500899315,\n",
       "    0.01789892464876175,\n",
       "    0.01700100488960743,\n",
       "    0.018281085416674614,\n",
       "    0.017833257094025612,\n",
       "    0.017544539645314217,\n",
       "    0.017131341621279716,\n",
       "    0.017436949536204338,\n",
       "    0.017054300755262375],\n",
       "   'train_mae': [0.20206832728887858,\n",
       "    0.17757996524635114,\n",
       "    0.1733473633465014,\n",
       "    0.16408938130265788,\n",
       "    0.16068771795222633,\n",
       "    0.1558416862236826,\n",
       "    0.1569738980186613,\n",
       "    0.1512276851817181,\n",
       "    0.15193830156012586,\n",
       "    0.15624542220642693,\n",
       "    0.15009746347603045,\n",
       "    0.1494212460361029,\n",
       "    0.15240475378538432,\n",
       "    0.14731903491835846,\n",
       "    0.1492340996077186,\n",
       "    0.14672414997690603,\n",
       "    0.1449044884035462,\n",
       "    0.1402879420079683,\n",
       "    0.14272424029676536,\n",
       "    0.1389049993533837,\n",
       "    0.13892898159591774,\n",
       "    0.13872022181749344,\n",
       "    0.1354905794325628,\n",
       "    0.13688617160445765,\n",
       "    0.1354665211156795,\n",
       "    0.13780338630864494,\n",
       "    0.13308930083325035,\n",
       "    0.12924215315203919,\n",
       "    0.12650327070763237,\n",
       "    0.12457832341131411,\n",
       "    0.12938326401145836,\n",
       "    0.12395325852067847,\n",
       "    0.12427452677174618,\n",
       "    0.12467074002090253,\n",
       "    0.12292079313805229,\n",
       "    0.12626061392457863],\n",
       "   'val_mae': [0.15293586254119873,\n",
       "    0.14397519826889038,\n",
       "    0.1467447429895401,\n",
       "    0.13981220126152039,\n",
       "    0.1306248903274536,\n",
       "    0.14241333305835724,\n",
       "    0.1435803771018982,\n",
       "    0.141671285033226,\n",
       "    0.127040296792984,\n",
       "    0.122455894947052,\n",
       "    0.13331793248653412,\n",
       "    0.13043315708637238,\n",
       "    0.12495248764753342,\n",
       "    0.12384790927171707,\n",
       "    0.12615986168384552,\n",
       "    0.12827172875404358,\n",
       "    0.13481229543685913,\n",
       "    0.12953241169452667,\n",
       "    0.13017664849758148,\n",
       "    0.12682317197322845,\n",
       "    0.12348581105470657,\n",
       "    0.12549352645874023,\n",
       "    0.12116467207670212,\n",
       "    0.11663684248924255,\n",
       "    0.11439293622970581,\n",
       "    0.11178801953792572,\n",
       "    0.1209886223077774,\n",
       "    0.1265675127506256,\n",
       "    0.11728636175394058,\n",
       "    0.1256033480167389,\n",
       "    0.11477302014827728,\n",
       "    0.12460972368717194,\n",
       "    0.11674841493368149,\n",
       "    0.1130068376660347,\n",
       "    0.1333772838115692,\n",
       "    0.117491215467453],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005]},\n",
       "  'metrics': {'mse': 0.03410860151052475,\n",
       "   'rmse': 0.1846851415531979,\n",
       "   'mae': 0.1174912303686142,\n",
       "   'r2': 0.4808819890022278,\n",
       "   'mape': 22.039339065551758},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.03417258225381374,\n",
       "    0.0305876356549561,\n",
       "    0.02780792103148997,\n",
       "    0.027114065270870923,\n",
       "    0.026026808796450494,\n",
       "    0.025798117043450475,\n",
       "    0.02414234250318259,\n",
       "    0.025201758183538914,\n",
       "    0.024619772378355265,\n",
       "    0.023941616388037802,\n",
       "    0.02345044482499361,\n",
       "    0.02365434910170734,\n",
       "    0.02308728373609483,\n",
       "    0.023259647795930503,\n",
       "    0.022900807298719884,\n",
       "    0.02283620275557041,\n",
       "    0.021951156901195645,\n",
       "    0.022533703269436954,\n",
       "    0.022302375035360457,\n",
       "    0.020962695172056556,\n",
       "    0.02126497384160757,\n",
       "    0.020560036599636077,\n",
       "    0.02097446718253195,\n",
       "    0.02037578811869025,\n",
       "    0.02077121869660914,\n",
       "    0.019990091631188988,\n",
       "    0.019478546641767024,\n",
       "    0.02075270200148225,\n",
       "    0.019216729886829854,\n",
       "    0.01893639387562871,\n",
       "    0.018067705060821025,\n",
       "    0.016916284523904323,\n",
       "    0.016559935407713056,\n",
       "    0.015916943666525185,\n",
       "    0.016600326262414457,\n",
       "    0.015558683662675321,\n",
       "    0.016093410062603654,\n",
       "    0.01505223308922723,\n",
       "    0.015578779997304082],\n",
       "   'val_loss': [0.017899315804243088,\n",
       "    0.01783870719373226,\n",
       "    0.01876378431916237,\n",
       "    0.019835304468870163,\n",
       "    0.018181361258029938,\n",
       "    0.016246404498815536,\n",
       "    0.01625751703977585,\n",
       "    0.01780741475522518,\n",
       "    0.016435706987977028,\n",
       "    0.016064392402768135,\n",
       "    0.020921830087900162,\n",
       "    0.0169615987688303,\n",
       "    0.01983902044594288,\n",
       "    0.01632685400545597,\n",
       "    0.01620575226843357,\n",
       "    0.016397304832935333,\n",
       "    0.016383271664381027,\n",
       "    0.016015993431210518,\n",
       "    0.014846313744783401,\n",
       "    0.01776430942118168,\n",
       "    0.019986998289823532,\n",
       "    0.01622183248400688,\n",
       "    0.01876748539507389,\n",
       "    0.016346188262104988,\n",
       "    0.015679171308875084,\n",
       "    0.016353944316506386,\n",
       "    0.01942071132361889,\n",
       "    0.019058924168348312,\n",
       "    0.01686173491179943,\n",
       "    0.016638752073049545,\n",
       "    0.016869939863681793,\n",
       "    0.02085847035050392,\n",
       "    0.020428821444511414,\n",
       "    0.018833832815289497,\n",
       "    0.021683815866708755,\n",
       "    0.019512934610247612,\n",
       "    0.028465954586863518,\n",
       "    0.019917147234082222,\n",
       "    0.02497982606291771],\n",
       "   'train_mae': [0.19469161704182625,\n",
       "    0.17595874816179274,\n",
       "    0.16592033952474594,\n",
       "    0.16291562020778655,\n",
       "    0.15890435688197613,\n",
       "    0.15520423837006092,\n",
       "    0.14988591372966767,\n",
       "    0.15621798299252987,\n",
       "    0.14831120185554028,\n",
       "    0.14837260507047176,\n",
       "    0.14715350419282913,\n",
       "    0.1454698324203491,\n",
       "    0.14619973488152027,\n",
       "    0.1462399810552597,\n",
       "    0.14332125075161456,\n",
       "    0.14260600544512272,\n",
       "    0.1417919833213091,\n",
       "    0.13829055204987525,\n",
       "    0.14156604707241058,\n",
       "    0.13559570908546448,\n",
       "    0.13680677562952043,\n",
       "    0.1365281105041504,\n",
       "    0.13652655184268953,\n",
       "    0.13419512547552587,\n",
       "    0.1327854637056589,\n",
       "    0.1314047873020172,\n",
       "    0.13026544637978077,\n",
       "    0.13140739537775517,\n",
       "    0.13125392682850362,\n",
       "    0.1272333160042763,\n",
       "    0.12474799826741219,\n",
       "    0.11836050748825074,\n",
       "    0.12002897746860981,\n",
       "    0.11453701332211494,\n",
       "    0.11774827875196933,\n",
       "    0.11590737737715244,\n",
       "    0.11730979010462761,\n",
       "    0.11156389638781547,\n",
       "    0.11507299430668354],\n",
       "   'val_mae': [0.15360839664936066,\n",
       "    0.14990170300006866,\n",
       "    0.15809468924999237,\n",
       "    0.1542595773935318,\n",
       "    0.13820475339889526,\n",
       "    0.12498699128627777,\n",
       "    0.12543733417987823,\n",
       "    0.1287597417831421,\n",
       "    0.13081027567386627,\n",
       "    0.1283937245607376,\n",
       "    0.13873839378356934,\n",
       "    0.13360491394996643,\n",
       "    0.15796463191509247,\n",
       "    0.11703868955373764,\n",
       "    0.1147255003452301,\n",
       "    0.1203811764717102,\n",
       "    0.11353060603141785,\n",
       "    0.12329540401697159,\n",
       "    0.11685874313116074,\n",
       "    0.11850447952747345,\n",
       "    0.14948143064975739,\n",
       "    0.10088470578193665,\n",
       "    0.12797343730926514,\n",
       "    0.11291790008544922,\n",
       "    0.1192162036895752,\n",
       "    0.11267491430044174,\n",
       "    0.13023220002651215,\n",
       "    0.13658085465431213,\n",
       "    0.12090801447629929,\n",
       "    0.11468019336462021,\n",
       "    0.10963129252195358,\n",
       "    0.13626742362976074,\n",
       "    0.12904776632785797,\n",
       "    0.1239856630563736,\n",
       "    0.1325606405735016,\n",
       "    0.12179206311702728,\n",
       "    0.15286140143871307,\n",
       "    0.1273638755083084,\n",
       "    0.1407422423362732],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005]},\n",
       "  'metrics': {'mse': 0.04995965585112572,\n",
       "   'rmse': 0.22351656728557218,\n",
       "   'mae': 0.1407422423362732,\n",
       "   'r2': -0.03522384166717529,\n",
       "   'mape': 20.251352310180664},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.039007848039979025,\n",
       "    0.030282473014224143,\n",
       "    0.028622217964203583,\n",
       "    0.028620712459087372,\n",
       "    0.02632526308298111,\n",
       "    0.026164306815536248,\n",
       "    0.026083250175274554,\n",
       "    0.024499730650512946,\n",
       "    0.024062605441680977,\n",
       "    0.024729147065608276,\n",
       "    0.02596163284033537,\n",
       "    0.02342516396726881,\n",
       "    0.023924543965785278,\n",
       "    0.02371840010441485,\n",
       "    0.022831603042071776,\n",
       "    0.021939834845917567,\n",
       "    0.02258893701114825,\n",
       "    0.022063290389875572,\n",
       "    0.02193828583473251,\n",
       "    0.02110752339164416,\n",
       "    0.022315304282875287,\n",
       "    0.020689222429479872,\n",
       "    0.021198523186502,\n",
       "    0.020924715768723262,\n",
       "    0.02192479669160786,\n",
       "    0.020915625084723746,\n",
       "    0.01918744903412603,\n",
       "    0.01930153871043807,\n",
       "    0.018406303000769446,\n",
       "    0.018425454873414265,\n",
       "    0.018252630230216754,\n",
       "    0.019064214036223433,\n",
       "    0.01783795180242686,\n",
       "    0.01740694274416282,\n",
       "    0.017443957782927014],\n",
       "   'val_loss': [0.03641928359866142,\n",
       "    0.04026252403855324,\n",
       "    0.03482819348573685,\n",
       "    0.03501206636428833,\n",
       "    0.036436568945646286,\n",
       "    0.03705841675400734,\n",
       "    0.03464283421635628,\n",
       "    0.03433150053024292,\n",
       "    0.033482346683740616,\n",
       "    0.033601582050323486,\n",
       "    0.033992305397987366,\n",
       "    0.03671570122241974,\n",
       "    0.03434443101286888,\n",
       "    0.034066278487443924,\n",
       "    0.03345643728971481,\n",
       "    0.0349198579788208,\n",
       "    0.03408671170473099,\n",
       "    0.03569081053137779,\n",
       "    0.036011915653944016,\n",
       "    0.03510544076561928,\n",
       "    0.03358951583504677,\n",
       "    0.03455839678645134,\n",
       "    0.03795430436730385,\n",
       "    0.03458784893155098,\n",
       "    0.036468587815761566,\n",
       "    0.03911030292510986,\n",
       "    0.033486057072877884,\n",
       "    0.03499047830700874,\n",
       "    0.03769512102007866,\n",
       "    0.03908186033368111,\n",
       "    0.038839761167764664,\n",
       "    0.040662143379449844,\n",
       "    0.04256196320056915,\n",
       "    0.034019727259874344,\n",
       "    0.03976073116064072],\n",
       "   'train_mae': [0.21796602507432303,\n",
       "    0.1786539483638037,\n",
       "    0.17268782073543185,\n",
       "    0.1701751450697581,\n",
       "    0.16243240875857218,\n",
       "    0.15923437369721277,\n",
       "    0.1578725931190309,\n",
       "    0.1548639220141229,\n",
       "    0.15221866823378064,\n",
       "    0.1531503771742185,\n",
       "    0.15491584014324916,\n",
       "    0.149196856433437,\n",
       "    0.14572752125206448,\n",
       "    0.14893637952350436,\n",
       "    0.14352196809791384,\n",
       "    0.14125480822154454,\n",
       "    0.14396804180883227,\n",
       "    0.14174144502196992,\n",
       "    0.1412582074602445,\n",
       "    0.13827133036795117,\n",
       "    0.13980823790743238,\n",
       "    0.13526215723582677,\n",
       "    0.13668285806973776,\n",
       "    0.13802639430477506,\n",
       "    0.1386603716583479,\n",
       "    0.14001463531028657,\n",
       "    0.12991591081732795,\n",
       "    0.12937384240684055,\n",
       "    0.1265030623901458,\n",
       "    0.12382979903902326,\n",
       "    0.12502335686059224,\n",
       "    0.12767577455157325,\n",
       "    0.12426073707285382,\n",
       "    0.1233002884047372,\n",
       "    0.12222026643298921],\n",
       "   'val_mae': [0.21317552030086517,\n",
       "    0.23876336216926575,\n",
       "    0.19910570979118347,\n",
       "    0.18982282280921936,\n",
       "    0.19855642318725586,\n",
       "    0.1963004320859909,\n",
       "    0.19298791885375977,\n",
       "    0.17302638292312622,\n",
       "    0.16813603043556213,\n",
       "    0.1676608771085739,\n",
       "    0.16909083724021912,\n",
       "    0.20025426149368286,\n",
       "    0.1861131638288498,\n",
       "    0.17884881794452667,\n",
       "    0.1815822422504425,\n",
       "    0.18779323995113373,\n",
       "    0.17495602369308472,\n",
       "    0.19163501262664795,\n",
       "    0.17422795295715332,\n",
       "    0.18765504658222198,\n",
       "    0.17067201435565948,\n",
       "    0.19022195041179657,\n",
       "    0.204642653465271,\n",
       "    0.18892011046409607,\n",
       "    0.18357476592063904,\n",
       "    0.2135569155216217,\n",
       "    0.17478826642036438,\n",
       "    0.17268352210521698,\n",
       "    0.17264649271965027,\n",
       "    0.20326602458953857,\n",
       "    0.19354389607906342,\n",
       "    0.20552514493465424,\n",
       "    0.219121515750885,\n",
       "    0.1619309037923813,\n",
       "    0.18743208050727844],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005]},\n",
       "  'metrics': {'mse': 0.07952302694320679,\n",
       "   'rmse': 0.2819982747167202,\n",
       "   'mae': 0.18743208050727844,\n",
       "   'r2': 0.053123414516448975,\n",
       "   'mape': 45.07408905029297},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.03345635330135172,\n",
       "    0.030354529195888477,\n",
       "    0.027714960658076136,\n",
       "    0.027618524110452694,\n",
       "    0.027470639906823635,\n",
       "    0.028181255837394434,\n",
       "    0.02595345349982381,\n",
       "    0.025365387089550495,\n",
       "    0.024798473978245802,\n",
       "    0.0252335240048441,\n",
       "    0.02429020819677548,\n",
       "    0.024305803743614393,\n",
       "    0.023387830662117762,\n",
       "    0.023653005122799765,\n",
       "    0.022991589143533598,\n",
       "    0.023935463491149923,\n",
       "    0.022538111883808266,\n",
       "    0.022098294205286285,\n",
       "    0.0215382869778709,\n",
       "    0.02199986500834877,\n",
       "    0.021799348421733488,\n",
       "    0.02137603001161055,\n",
       "    0.021077275995842436,\n",
       "    0.02138286041603847,\n",
       "    0.02120153686370362],\n",
       "   'val_loss': [0.043615661561489105,\n",
       "    0.043632131069898605,\n",
       "    0.044433485716581345,\n",
       "    0.04355750232934952,\n",
       "    0.04083314165472984,\n",
       "    0.043873656541109085,\n",
       "    0.05564671382308006,\n",
       "    0.04932401701807976,\n",
       "    0.045271072536706924,\n",
       "    0.047678541392087936,\n",
       "    0.05140567570924759,\n",
       "    0.04340077564120293,\n",
       "    0.0435783714056015,\n",
       "    0.04380878061056137,\n",
       "    0.04845884442329407,\n",
       "    0.04662361368536949,\n",
       "    0.045173484832048416,\n",
       "    0.04770473763346672,\n",
       "    0.045196615159511566,\n",
       "    0.044189728796482086,\n",
       "    0.04684332758188248,\n",
       "    0.043716344982385635,\n",
       "    0.04644810035824776,\n",
       "    0.04286988824605942,\n",
       "    0.044946376234292984],\n",
       "   'train_mae': [0.18679988248781723,\n",
       "    0.17541795088486237,\n",
       "    0.16649970378388057,\n",
       "    0.16518968038938261,\n",
       "    0.16080579838969492,\n",
       "    0.16265358504923907,\n",
       "    0.15861224349249492,\n",
       "    0.15668589554049753,\n",
       "    0.15293984961780635,\n",
       "    0.15450851009650665,\n",
       "    0.15146109089255333,\n",
       "    0.14814671907912602,\n",
       "    0.1447224183516069,\n",
       "    0.1485066373239864,\n",
       "    0.14547025446187367,\n",
       "    0.1446549083021554,\n",
       "    0.14327609098770402,\n",
       "    0.14066216078671542,\n",
       "    0.1373837519098412,\n",
       "    0.13760206272656267,\n",
       "    0.14182731407609853,\n",
       "    0.13545763526450505,\n",
       "    0.13758790560744025,\n",
       "    0.13517511940815233,\n",
       "    0.13789902017875152],\n",
       "   'val_mae': [0.2350648194551468,\n",
       "    0.23556585609912872,\n",
       "    0.24113500118255615,\n",
       "    0.2355368733406067,\n",
       "    0.2315264344215393,\n",
       "    0.23488961160182953,\n",
       "    0.25695928931236267,\n",
       "    0.24822022020816803,\n",
       "    0.2434663623571396,\n",
       "    0.24401238560676575,\n",
       "    0.2498931884765625,\n",
       "    0.23360034823417664,\n",
       "    0.23135600984096527,\n",
       "    0.2342216521501541,\n",
       "    0.24031224846839905,\n",
       "    0.23980526626110077,\n",
       "    0.2343250811100006,\n",
       "    0.23925408720970154,\n",
       "    0.2348090410232544,\n",
       "    0.23101234436035156,\n",
       "    0.23759602010250092,\n",
       "    0.22987104952335358,\n",
       "    0.23784857988357544,\n",
       "    0.22809059917926788,\n",
       "    0.22939790785312653],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005]},\n",
       "  'metrics': {'mse': 0.08989275246858597,\n",
       "   'rmse': 0.29982120083240604,\n",
       "   'mae': 0.22939790785312653,\n",
       "   'r2': 0.20379048585891724,\n",
       "   'mape': 74.1172103881836},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.03791367627032425,\n",
       "    0.03230633278903754,\n",
       "    0.03039419877788295,\n",
       "    0.028754985445867413,\n",
       "    0.0286393826422484,\n",
       "    0.02862696009485618,\n",
       "    0.026921505918321403,\n",
       "    0.026183739061588825,\n",
       "    0.026177148375174274,\n",
       "    0.02806371053599793,\n",
       "    0.025856352046779964,\n",
       "    0.025549192143523174,\n",
       "    0.025939586815302788,\n",
       "    0.024229892529547215,\n",
       "    0.024473967354582703,\n",
       "    0.024199145886561146,\n",
       "    0.02414755721617004,\n",
       "    0.02590657803027526,\n",
       "    0.02393509888940531,\n",
       "    0.024273422303731026,\n",
       "    0.023727240405328896,\n",
       "    0.02191179735667032,\n",
       "    0.022645326290765534,\n",
       "    0.02187045857958172,\n",
       "    0.023471109731041866,\n",
       "    0.022090439117797043,\n",
       "    0.021516478094069855,\n",
       "    0.021635065908017365,\n",
       "    0.02121679950505495],\n",
       "   'val_loss': [0.07006961107254028,\n",
       "    0.0698176920413971,\n",
       "    0.06707488000392914,\n",
       "    0.05833349749445915,\n",
       "    0.06138690188527107,\n",
       "    0.06493372470140457,\n",
       "    0.059720396995544434,\n",
       "    0.05866072326898575,\n",
       "    0.057019591331481934,\n",
       "    0.05924215912818909,\n",
       "    0.06197964400053024,\n",
       "    0.05809445306658745,\n",
       "    0.06468673050403595,\n",
       "    0.05967060104012489,\n",
       "    0.059255070984363556,\n",
       "    0.06289038807153702,\n",
       "    0.06561689078807831,\n",
       "    0.07305068522691727,\n",
       "    0.06650897115468979,\n",
       "    0.06758152693510056,\n",
       "    0.06565636396408081,\n",
       "    0.06931345909833908,\n",
       "    0.06514403223991394,\n",
       "    0.06359792500734329,\n",
       "    0.06628867238759995,\n",
       "    0.07073959708213806,\n",
       "    0.07289663702249527,\n",
       "    0.06956008076667786,\n",
       "    0.0699826255440712],\n",
       "   'train_mae': [0.21245768860630368,\n",
       "    0.18804213275080142,\n",
       "    0.17992161279139313,\n",
       "    0.17341698641362396,\n",
       "    0.16494418421517248,\n",
       "    0.1691048206842464,\n",
       "    0.16265747080678525,\n",
       "    0.15927046407823978,\n",
       "    0.15776928481848343,\n",
       "    0.15973663103321326,\n",
       "    0.159178389803223,\n",
       "    0.15799925962220068,\n",
       "    0.1567713974610619,\n",
       "    0.151333377737066,\n",
       "    0.1511557205863621,\n",
       "    0.14787706117267194,\n",
       "    0.15124271259359692,\n",
       "    0.15575459891039392,\n",
       "    0.14927292582781418,\n",
       "    0.1510872089344522,\n",
       "    0.1475578374836756,\n",
       "    0.1413382521790007,\n",
       "    0.1433587349627329,\n",
       "    0.14130870414816815,\n",
       "    0.14617063947345899,\n",
       "    0.14068465971428415,\n",
       "    0.13927167621643646,\n",
       "    0.14087958471930545,\n",
       "    0.13832392057646875],\n",
       "   'val_mae': [0.2930898368358612,\n",
       "    0.3051632046699524,\n",
       "    0.2920079827308655,\n",
       "    0.28443124890327454,\n",
       "    0.2790672779083252,\n",
       "    0.28728851675987244,\n",
       "    0.28469836711883545,\n",
       "    0.2767031490802765,\n",
       "    0.2679789364337921,\n",
       "    0.2793353199958801,\n",
       "    0.280764639377594,\n",
       "    0.26951363682746887,\n",
       "    0.2911943793296814,\n",
       "    0.27776220440864563,\n",
       "    0.2716285288333893,\n",
       "    0.28061211109161377,\n",
       "    0.28577446937561035,\n",
       "    0.293591171503067,\n",
       "    0.2790987193584442,\n",
       "    0.28739747405052185,\n",
       "    0.27655741572380066,\n",
       "    0.2847159206867218,\n",
       "    0.27534621953964233,\n",
       "    0.2750456631183624,\n",
       "    0.27753910422325134,\n",
       "    0.27700015902519226,\n",
       "    0.2823910415172577,\n",
       "    0.27798375487327576,\n",
       "    0.2835594415664673],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005]},\n",
       "  'metrics': {'mse': 0.1400621086359024,\n",
       "   'rmse': 0.37424872563029843,\n",
       "   'mae': 0.2835594415664673,\n",
       "   'r2': 0.0917089581489563,\n",
       "   'mape': 77.8864517211914},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.035459964657607285,\n",
       "    0.03143429383635521,\n",
       "    0.029763080179691315,\n",
       "    0.028749639006412548,\n",
       "    0.027399305254220963,\n",
       "    0.02777455601355304,\n",
       "    0.02720444058270558,\n",
       "    0.027576304324295208,\n",
       "    0.026783162242044575,\n",
       "    0.026879241690039635,\n",
       "    0.0263522587230672,\n",
       "    0.02668030932545662,\n",
       "    0.026045554965410545,\n",
       "    0.025403985915624577,\n",
       "    0.025175086910957874,\n",
       "    0.02485339238267878,\n",
       "    0.024880295092968838,\n",
       "    0.02437962405383587,\n",
       "    0.02433848247417937,\n",
       "    0.024477811003832714,\n",
       "    0.024167315184098224,\n",
       "    0.023832111943351185,\n",
       "    0.023581943110279415],\n",
       "   'val_loss': [0.05309637263417244,\n",
       "    0.050087716430425644,\n",
       "    0.05001915246248245,\n",
       "    0.05144563317298889,\n",
       "    0.05180105194449425,\n",
       "    0.05064462125301361,\n",
       "    0.056580837815999985,\n",
       "    0.05153428390622139,\n",
       "    0.05218740180134773,\n",
       "    0.051399603486061096,\n",
       "    0.05348553508520126,\n",
       "    0.0556633397936821,\n",
       "    0.050928689539432526,\n",
       "    0.051724206656217575,\n",
       "    0.05190421640872955,\n",
       "    0.053778424859046936,\n",
       "    0.05459213629364967,\n",
       "    0.05421353504061699,\n",
       "    0.05183423310518265,\n",
       "    0.055714838206768036,\n",
       "    0.05561327561736107,\n",
       "    0.05444831773638725,\n",
       "    0.053922802209854126],\n",
       "   'train_mae': [0.1977872382039609,\n",
       "    0.17978552372559256,\n",
       "    0.17128865744756616,\n",
       "    0.16828796073146488,\n",
       "    0.1612234410384427,\n",
       "    0.16402669574903406,\n",
       "    0.15877858225418173,\n",
       "    0.1638672947883606,\n",
       "    0.15772517511378165,\n",
       "    0.16023273442102515,\n",
       "    0.15599829930326212,\n",
       "    0.1575095867333205,\n",
       "    0.15473400153543637,\n",
       "    0.1524421880426614,\n",
       "    0.15065328323322794,\n",
       "    0.15186755132416022,\n",
       "    0.15113169861876447,\n",
       "    0.1476950059118478,\n",
       "    0.14594722085672876,\n",
       "    0.14867728073959766,\n",
       "    0.15075474070466083,\n",
       "    0.14520806009354797,\n",
       "    0.1455746587851773],\n",
       "   'val_mae': [0.23447491228580475,\n",
       "    0.22435320913791656,\n",
       "    0.2268494963645935,\n",
       "    0.22907204926013947,\n",
       "    0.22283241152763367,\n",
       "    0.2171727865934372,\n",
       "    0.2509051263332367,\n",
       "    0.2237710803747177,\n",
       "    0.22499775886535645,\n",
       "    0.22520719468593597,\n",
       "    0.23255716264247894,\n",
       "    0.2460511028766632,\n",
       "    0.21567605435848236,\n",
       "    0.2228648066520691,\n",
       "    0.2122250348329544,\n",
       "    0.22013363242149353,\n",
       "    0.22151006758213043,\n",
       "    0.2271769642829895,\n",
       "    0.21296480298042297,\n",
       "    0.22645261883735657,\n",
       "    0.22269345819950104,\n",
       "    0.221234992146492,\n",
       "    0.2191706746816635],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005]},\n",
       "  'metrics': {'mse': 0.10822409391403198,\n",
       "   'rmse': 0.32897430585690424,\n",
       "   'mae': 0.2191706746816635,\n",
       "   'r2': 0.1041150689125061,\n",
       "   'mape': 19.701723098754883},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.039363169111311436,\n",
       "    0.032825886349504195,\n",
       "    0.031216413558771212,\n",
       "    0.0305664277014633,\n",
       "    0.03049215612312158,\n",
       "    0.02925003948621452,\n",
       "    0.02843654827059557,\n",
       "    0.027770940912887454,\n",
       "    0.02821290275702874,\n",
       "    0.02784357957231502,\n",
       "    0.027526229658784967,\n",
       "    0.026845655171200633,\n",
       "    0.027596814208664,\n",
       "    0.026342376174094777,\n",
       "    0.026372793208186824,\n",
       "    0.026004818306925397,\n",
       "    0.02584955709365507,\n",
       "    0.026125367886076372,\n",
       "    0.025164528129001457,\n",
       "    0.0245372461892354,\n",
       "    0.024357450117046636,\n",
       "    0.02396428305655718,\n",
       "    0.023685726647575695,\n",
       "    0.022835378418676555,\n",
       "    0.023673988607091207,\n",
       "    0.022716145069959264,\n",
       "    0.023601217040171225,\n",
       "    0.022291398762414854,\n",
       "    0.02322794155528148,\n",
       "    0.022118856199085712,\n",
       "    0.02146610408090055,\n",
       "    0.020909748699826498,\n",
       "    0.021145096863619983,\n",
       "    0.02047923960102101,\n",
       "    0.01992143294773996],\n",
       "   'val_loss': [0.053921233862638474,\n",
       "    0.0523546002805233,\n",
       "    0.048357319086790085,\n",
       "    0.05053962022066116,\n",
       "    0.040261976420879364,\n",
       "    0.04085015878081322,\n",
       "    0.037801217287778854,\n",
       "    0.03745284304022789,\n",
       "    0.040578365325927734,\n",
       "    0.03833340108394623,\n",
       "    0.043158456683158875,\n",
       "    0.03946152329444885,\n",
       "    0.04107778146862984,\n",
       "    0.03980819508433342,\n",
       "    0.03745217248797417,\n",
       "    0.04043237119913101,\n",
       "    0.041492193937301636,\n",
       "    0.03892533481121063,\n",
       "    0.04205872118473053,\n",
       "    0.04039476066827774,\n",
       "    0.04518493264913559,\n",
       "    0.042288411408662796,\n",
       "    0.041603781282901764,\n",
       "    0.04709558188915253,\n",
       "    0.04110270366072655,\n",
       "    0.042365990579128265,\n",
       "    0.046979211270809174,\n",
       "    0.04019645228981972,\n",
       "    0.041065800935029984,\n",
       "    0.050225768238306046,\n",
       "    0.04606092721223831,\n",
       "    0.04848114028573036,\n",
       "    0.04394996166229248,\n",
       "    0.04802523925900459,\n",
       "    0.051257625222206116],\n",
       "   'train_mae': [0.20996633420387903,\n",
       "    0.18560820693771043,\n",
       "    0.17711885211368403,\n",
       "    0.17319964555402598,\n",
       "    0.1757493708282709,\n",
       "    0.16851188552876314,\n",
       "    0.16670416947454214,\n",
       "    0.1632473006223639,\n",
       "    0.16496555848668018,\n",
       "    0.1635092655196786,\n",
       "    0.16202272412677607,\n",
       "    0.15545085103561482,\n",
       "    0.1609496157616377,\n",
       "    0.15819274634122849,\n",
       "    0.15706553868949413,\n",
       "    0.15512834520389637,\n",
       "    0.15144398187597594,\n",
       "    0.1594525290032228,\n",
       "    0.15046620927751064,\n",
       "    0.14802067012836537,\n",
       "    0.1499296153585116,\n",
       "    0.1468082694336772,\n",
       "    0.14574295406540236,\n",
       "    0.14295625562469164,\n",
       "    0.14468964375555515,\n",
       "    0.14347631949931383,\n",
       "    0.14314674834410349,\n",
       "    0.14130343279490867,\n",
       "    0.1417463095858693,\n",
       "    0.14228051900863647,\n",
       "    0.13607913938661417,\n",
       "    0.13544856570661068,\n",
       "    0.1344115516791741,\n",
       "    0.13450960100938877,\n",
       "    0.13211951156457266],\n",
       "   'val_mae': [0.2687419354915619,\n",
       "    0.24891796708106995,\n",
       "    0.25100111961364746,\n",
       "    0.24594828486442566,\n",
       "    0.22238077223300934,\n",
       "    0.2219705581665039,\n",
       "    0.21057827770709991,\n",
       "    0.20183081924915314,\n",
       "    0.22640536725521088,\n",
       "    0.20998549461364746,\n",
       "    0.22507397830486298,\n",
       "    0.21738116443157196,\n",
       "    0.21818162500858307,\n",
       "    0.20836672186851501,\n",
       "    0.2108166664838791,\n",
       "    0.20939604938030243,\n",
       "    0.21725013852119446,\n",
       "    0.2083498239517212,\n",
       "    0.22108612954616547,\n",
       "    0.2128218412399292,\n",
       "    0.22369638085365295,\n",
       "    0.21835476160049438,\n",
       "    0.21282422542572021,\n",
       "    0.22215914726257324,\n",
       "    0.21189093589782715,\n",
       "    0.21403996646404266,\n",
       "    0.22372467815876007,\n",
       "    0.20823490619659424,\n",
       "    0.2112463414669037,\n",
       "    0.22920508682727814,\n",
       "    0.22152896225452423,\n",
       "    0.2207460254430771,\n",
       "    0.2146225869655609,\n",
       "    0.21959847211837769,\n",
       "    0.2253907322883606],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.00025,\n",
       "    0.00025,\n",
       "    0.00025,\n",
       "    0.00025,\n",
       "    0.00025]},\n",
       "  'metrics': {'mse': 0.10251526534557343,\n",
       "   'rmse': 0.3201800514485145,\n",
       "   'mae': 0.2253907322883606,\n",
       "   'r2': 0.27848291397094727,\n",
       "   'mape': 138.4080352783203},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.03688508361577988,\n",
       "    0.033242621794342994,\n",
       "    0.03132289163768291,\n",
       "    0.03022624034434557,\n",
       "    0.029392594546079634,\n",
       "    0.028748088404536246,\n",
       "    0.028721955865621568,\n",
       "    0.027830734252929687,\n",
       "    0.02790990024805069,\n",
       "    0.027282213121652604,\n",
       "    0.02843987725675106,\n",
       "    0.027611985988914966,\n",
       "    0.02729731485247612,\n",
       "    0.02744858533143997,\n",
       "    0.026343723870813848,\n",
       "    0.025971322879195215,\n",
       "    0.026479184702038763,\n",
       "    0.026471633948385715,\n",
       "    0.02577460628002882,\n",
       "    0.025671148747205733,\n",
       "    0.026581073701381682,\n",
       "    0.024927011206746102,\n",
       "    0.02479558076709509,\n",
       "    0.024475802928209306,\n",
       "    0.02416322622448206,\n",
       "    0.024046627543866633,\n",
       "    0.024127707332372666,\n",
       "    0.023074961155653,\n",
       "    0.022707443237304687,\n",
       "    0.022865619435906412,\n",
       "    0.022564697675406933,\n",
       "    0.021608186177909373,\n",
       "    0.023284958936274053,\n",
       "    0.020716744773089886,\n",
       "    0.020680389627814294,\n",
       "    0.019599018953740596,\n",
       "    0.01935957297682762,\n",
       "    0.01896594962105155,\n",
       "    0.018884973376989366,\n",
       "    0.018248601406812667,\n",
       "    0.017143009305000304,\n",
       "    0.01711744464933872],\n",
       "   'val_loss': [0.03754710406064987,\n",
       "    0.036625538021326065,\n",
       "    0.03456244617700577,\n",
       "    0.03172201290726662,\n",
       "    0.031414639204740524,\n",
       "    0.030257057398557663,\n",
       "    0.029378363862633705,\n",
       "    0.028768956661224365,\n",
       "    0.029021305963397026,\n",
       "    0.03035329282283783,\n",
       "    0.029397496953606606,\n",
       "    0.028126655146479607,\n",
       "    0.030395876616239548,\n",
       "    0.029214223846793175,\n",
       "    0.029222900047898293,\n",
       "    0.0280842836946249,\n",
       "    0.028127353638410568,\n",
       "    0.030292941257357597,\n",
       "    0.027559679001569748,\n",
       "    0.028439410030841827,\n",
       "    0.028960909694433212,\n",
       "    0.02627243846654892,\n",
       "    0.02679031528532505,\n",
       "    0.026807093992829323,\n",
       "    0.02693812921643257,\n",
       "    0.02813040465116501,\n",
       "    0.027921924367547035,\n",
       "    0.028344247490167618,\n",
       "    0.028207408264279366,\n",
       "    0.028448665514588356,\n",
       "    0.029725288972258568,\n",
       "    0.02786034345626831,\n",
       "    0.031259145587682724,\n",
       "    0.030449699610471725,\n",
       "    0.032215870916843414,\n",
       "    0.03201534226536751,\n",
       "    0.03185736760497093,\n",
       "    0.0365276001393795,\n",
       "    0.03387421369552612,\n",
       "    0.03870227187871933,\n",
       "    0.037409719079732895,\n",
       "    0.03972093388438225],\n",
       "   'train_mae': [0.1986418682336807,\n",
       "    0.18539897739887237,\n",
       "    0.17883268535137176,\n",
       "    0.17390672683715822,\n",
       "    0.168857661485672,\n",
       "    0.16686775743961335,\n",
       "    0.16899899303913116,\n",
       "    0.16203825533390045,\n",
       "    0.16345082938671113,\n",
       "    0.16109181731939315,\n",
       "    0.16335080444812775,\n",
       "    0.160151786506176,\n",
       "    0.15879782021045685,\n",
       "    0.1610918763279915,\n",
       "    0.15598247855901717,\n",
       "    0.15467202365398408,\n",
       "    0.15710540890693664,\n",
       "    0.159447360932827,\n",
       "    0.1545900759100914,\n",
       "    0.15120764166116715,\n",
       "    0.15934311479330063,\n",
       "    0.15247347950935364,\n",
       "    0.1490141561627388,\n",
       "    0.14989524930715561,\n",
       "    0.14632147073745727,\n",
       "    0.1466990253329277,\n",
       "    0.144550222158432,\n",
       "    0.14446195095777511,\n",
       "    0.1419028154015541,\n",
       "    0.1462435582280159,\n",
       "    0.14066585808992385,\n",
       "    0.13933592349290846,\n",
       "    0.14310984462499618,\n",
       "    0.13535159856081008,\n",
       "    0.13411097705364228,\n",
       "    0.1320006999373436,\n",
       "    0.12994419664144516,\n",
       "    0.12745718121528626,\n",
       "    0.12941616773605347,\n",
       "    0.12648722141981125,\n",
       "    0.12144838809967042,\n",
       "    0.12202223062515259],\n",
       "   'val_mae': [0.17696991562843323,\n",
       "    0.16794238984584808,\n",
       "    0.17815949022769928,\n",
       "    0.15634268522262573,\n",
       "    0.15158526599407196,\n",
       "    0.1639062613248825,\n",
       "    0.1628102958202362,\n",
       "    0.1485883891582489,\n",
       "    0.15569184720516205,\n",
       "    0.1506783366203308,\n",
       "    0.15959107875823975,\n",
       "    0.15835697948932648,\n",
       "    0.15753406286239624,\n",
       "    0.15801873803138733,\n",
       "    0.16432824730873108,\n",
       "    0.17191244661808014,\n",
       "    0.16307592391967773,\n",
       "    0.1499129980802536,\n",
       "    0.1561819314956665,\n",
       "    0.17079328000545502,\n",
       "    0.14525757730007172,\n",
       "    0.14717498421669006,\n",
       "    0.1570591926574707,\n",
       "    0.15397414565086365,\n",
       "    0.15851134061813354,\n",
       "    0.16600100696086884,\n",
       "    0.16222232580184937,\n",
       "    0.15902197360992432,\n",
       "    0.16219496726989746,\n",
       "    0.1551024168729782,\n",
       "    0.1707320511341095,\n",
       "    0.16433140635490417,\n",
       "    0.1586541086435318,\n",
       "    0.16257254779338837,\n",
       "    0.16705352067947388,\n",
       "    0.1642436683177948,\n",
       "    0.16926738619804382,\n",
       "    0.1750025898218155,\n",
       "    0.15725864470005035,\n",
       "    0.17338827252388,\n",
       "    0.1830684095621109,\n",
       "    0.1731199324131012],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005]},\n",
       "  'metrics': {'mse': 0.07944187521934509,\n",
       "   'rmse': 0.28185435107399903,\n",
       "   'mae': 0.1731199324131012,\n",
       "   'r2': 0.26749753952026367,\n",
       "   'mape': 58.626792907714844},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.03862749742200741,\n",
       "    0.032487530870219834,\n",
       "    0.032413586257742,\n",
       "    0.03040480026258872,\n",
       "    0.029751032805786684,\n",
       "    0.02947699959174945,\n",
       "    0.029030624633798234,\n",
       "    0.028562369254919198,\n",
       "    0.028702227756954156,\n",
       "    0.02880109603015276,\n",
       "    0.0284591568275713,\n",
       "    0.02756237990867633,\n",
       "    0.02644221305560607,\n",
       "    0.027478258949346267,\n",
       "    0.027421675282172285,\n",
       "    0.027750944503797934,\n",
       "    0.027174454612227585,\n",
       "    0.026734535976384696,\n",
       "    0.026236968831374094,\n",
       "    0.02595617023941416,\n",
       "    0.025168556397637494,\n",
       "    0.025348977508166663,\n",
       "    0.02544841019866558,\n",
       "    0.02471789330817186,\n",
       "    0.024709766300824974,\n",
       "    0.024378615646408155,\n",
       "    0.02347136690066411,\n",
       "    0.02326958433080178,\n",
       "    0.02274198901767914,\n",
       "    0.023060928457058393,\n",
       "    0.02228996827482031,\n",
       "    0.021708814463076685],\n",
       "   'val_loss': [0.032930318266153336,\n",
       "    0.03293569013476372,\n",
       "    0.03222866728901863,\n",
       "    0.027179550379514694,\n",
       "    0.02756151556968689,\n",
       "    0.024285275489091873,\n",
       "    0.024058425799012184,\n",
       "    0.02736351639032364,\n",
       "    0.024776063859462738,\n",
       "    0.025131819769740105,\n",
       "    0.024595890194177628,\n",
       "    0.022863252088427544,\n",
       "    0.024084337055683136,\n",
       "    0.025441234931349754,\n",
       "    0.02859949693083763,\n",
       "    0.02363850735127926,\n",
       "    0.023121794685721397,\n",
       "    0.02418173849582672,\n",
       "    0.028957994654774666,\n",
       "    0.02790362574160099,\n",
       "    0.02486354112625122,\n",
       "    0.03118368424475193,\n",
       "    0.02607678808271885,\n",
       "    0.029431333765387535,\n",
       "    0.02892373874783516,\n",
       "    0.033034782856702805,\n",
       "    0.03163847327232361,\n",
       "    0.029313672333955765,\n",
       "    0.03304239362478256,\n",
       "    0.03090687468647957,\n",
       "    0.036405812948942184,\n",
       "    0.03165500983595848],\n",
       "   'train_mae': [0.20296949434738892,\n",
       "    0.18519237064398253,\n",
       "    0.18035466797076738,\n",
       "    0.1741403152163212,\n",
       "    0.17099392872590286,\n",
       "    0.16761967425162977,\n",
       "    0.1703212599341686,\n",
       "    0.16643053379196387,\n",
       "    0.16431069374084473,\n",
       "    0.1647683923634199,\n",
       "    0.16357683734251902,\n",
       "    0.1627330997815499,\n",
       "    0.15919119406204957,\n",
       "    0.15989175582161316,\n",
       "    0.159382025209757,\n",
       "    0.15986185549543455,\n",
       "    0.16103779811125535,\n",
       "    0.1588710812995067,\n",
       "    0.15529540954874113,\n",
       "    0.15449381963564798,\n",
       "    0.15274960117844436,\n",
       "    0.15197872943603075,\n",
       "    0.15290105801362258,\n",
       "    0.14963805904755226,\n",
       "    0.15071303483385307,\n",
       "    0.14666570579776397,\n",
       "    0.14674531295895576,\n",
       "    0.14385023741767958,\n",
       "    0.1430073558137967,\n",
       "    0.14208665289557898,\n",
       "    0.142094143308126,\n",
       "    0.13885987320771584],\n",
       "   'val_mae': [0.17938096821308136,\n",
       "    0.17611178755760193,\n",
       "    0.16087207198143005,\n",
       "    0.15784701704978943,\n",
       "    0.14846976101398468,\n",
       "    0.15149138867855072,\n",
       "    0.15092119574546814,\n",
       "    0.1439250409603119,\n",
       "    0.1422017216682434,\n",
       "    0.14655877649784088,\n",
       "    0.14515337347984314,\n",
       "    0.14703747630119324,\n",
       "    0.1450119912624359,\n",
       "    0.14673148095607758,\n",
       "    0.1478612720966339,\n",
       "    0.14389586448669434,\n",
       "    0.14355550706386566,\n",
       "    0.14366257190704346,\n",
       "    0.15115416049957275,\n",
       "    0.14391624927520752,\n",
       "    0.1403372585773468,\n",
       "    0.16030928492546082,\n",
       "    0.14380428194999695,\n",
       "    0.14896559715270996,\n",
       "    0.15036192536354065,\n",
       "    0.1604834794998169,\n",
       "    0.15171532332897186,\n",
       "    0.1531655192375183,\n",
       "    0.15852726995944977,\n",
       "    0.15444785356521606,\n",
       "    0.16557848453521729,\n",
       "    0.15218313038349152],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005]},\n",
       "  'metrics': {'mse': 0.06331002712249756,\n",
       "   'rmse': 0.2516148388360622,\n",
       "   'mae': 0.1521831452846527,\n",
       "   'r2': 0.23688292503356934,\n",
       "   'mape': 41.8788948059082},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.03795562063654264,\n",
       "    0.03245304183413585,\n",
       "    0.031550671698318586,\n",
       "    0.030170782020798436,\n",
       "    0.029805340493718784,\n",
       "    0.02832150379954665,\n",
       "    0.02837119417058097,\n",
       "    0.02793414490642371,\n",
       "    0.027552417444962042,\n",
       "    0.027798228655700332,\n",
       "    0.027976250793370936,\n",
       "    0.027146137836906645,\n",
       "    0.02671710022345737,\n",
       "    0.027304653043824213,\n",
       "    0.026570586211703443,\n",
       "    0.025709215344654188,\n",
       "    0.025737208969615125,\n",
       "    0.025506630746854678,\n",
       "    0.024815378986574984,\n",
       "    0.02482307167654788,\n",
       "    0.02506020747952991,\n",
       "    0.024387790407571528,\n",
       "    0.02385918702930212,\n",
       "    0.02451360018716918,\n",
       "    0.024068594806724124,\n",
       "    0.02385775365487293,\n",
       "    0.023600566925274,\n",
       "    0.02259376455374338,\n",
       "    0.02230848199515431,\n",
       "    0.021537161915114632,\n",
       "    0.021432566525483573,\n",
       "    0.02154190220904571,\n",
       "    0.021556675296138833,\n",
       "    0.02083354136320176,\n",
       "    0.02042652849383928,\n",
       "    0.020465789190321056,\n",
       "    0.020252116403921886,\n",
       "    0.020535737448544415,\n",
       "    0.020192414867105307,\n",
       "    0.020055703370383492,\n",
       "    0.01995870115718356,\n",
       "    0.019514426416545,\n",
       "    0.01959437621688401,\n",
       "    0.01903953527410825,\n",
       "    0.01831757404875976,\n",
       "    0.018067888363643928,\n",
       "    0.017954859399685153,\n",
       "    0.017916469731264643,\n",
       "    0.017875322934102128,\n",
       "    0.017995860220657453],\n",
       "   'val_loss': [0.008044889196753502,\n",
       "    0.008819467388093472,\n",
       "    0.007887742482125759,\n",
       "    0.007530142553150654,\n",
       "    0.007961592637002468,\n",
       "    0.007553101982921362,\n",
       "    0.008770552463829517,\n",
       "    0.008507251739501953,\n",
       "    0.008426537737250328,\n",
       "    0.007540615275502205,\n",
       "    0.008550389669835567,\n",
       "    0.007934908382594585,\n",
       "    0.008817791938781738,\n",
       "    0.00878144335001707,\n",
       "    0.008481455966830254,\n",
       "    0.007381268311291933,\n",
       "    0.008641555905342102,\n",
       "    0.008537826128304005,\n",
       "    0.008164020255208015,\n",
       "    0.009205815382301807,\n",
       "    0.009762936271727085,\n",
       "    0.009218594990670681,\n",
       "    0.007387062069028616,\n",
       "    0.007897828705608845,\n",
       "    0.008439871482551098,\n",
       "    0.008234694600105286,\n",
       "    0.008444574661552906,\n",
       "    0.0075432052835822105,\n",
       "    0.009423982352018356,\n",
       "    0.007918252609670162,\n",
       "    0.008712963201105595,\n",
       "    0.007313064765185118,\n",
       "    0.009717702865600586,\n",
       "    0.008122518658638,\n",
       "    0.007985640317201614,\n",
       "    0.008892884477972984,\n",
       "    0.007621077820658684,\n",
       "    0.009089451283216476,\n",
       "    0.007388785947114229,\n",
       "    0.0077912816777825356,\n",
       "    0.008008121512830257,\n",
       "    0.007727101445198059,\n",
       "    0.007344659883528948,\n",
       "    0.007308389525860548,\n",
       "    0.007888969965279102,\n",
       "    0.007242304272949696,\n",
       "    0.00739390030503273,\n",
       "    0.007097357884049416,\n",
       "    0.007157212123274803,\n",
       "    0.0069125620648264885],\n",
       "   'train_mae': [0.20334552449208718,\n",
       "    0.18578045070171356,\n",
       "    0.17885877909483733,\n",
       "    0.17516334961961816,\n",
       "    0.17109404338730705,\n",
       "    0.16742491473754248,\n",
       "    0.16466714000260388,\n",
       "    0.16623513234986198,\n",
       "    0.16105548457966912,\n",
       "    0.1614485322325318,\n",
       "    0.16219388086486747,\n",
       "    0.15885163070978942,\n",
       "    0.1614698905635763,\n",
       "    0.16015860521131092,\n",
       "    0.1581071471726453,\n",
       "    0.15469633180786063,\n",
       "    0.15220781967595773,\n",
       "    0.1515823464702677,\n",
       "    0.1528776944787414,\n",
       "    0.14998283899492687,\n",
       "    0.14914132544287928,\n",
       "    0.14884264629196237,\n",
       "    0.14847041556128748,\n",
       "    0.15012449743571105,\n",
       "    0.14643752961247056,\n",
       "    0.14599577834208807,\n",
       "    0.1453752393523852,\n",
       "    0.14379622042179108,\n",
       "    0.14012502630551657,\n",
       "    0.14019465198119482,\n",
       "    0.13833358718289268,\n",
       "    0.13730254603756797,\n",
       "    0.1390401298801104,\n",
       "    0.1353076649484811,\n",
       "    0.1354055114918285,\n",
       "    0.13415265359260417,\n",
       "    0.1329691125838845,\n",
       "    0.13332970330008753,\n",
       "    0.13396305839220682,\n",
       "    0.13352578574860538,\n",
       "    0.13269544668771602,\n",
       "    0.13144678125778833,\n",
       "    0.12922870202196968,\n",
       "    0.13182765466195565,\n",
       "    0.12765975655229003,\n",
       "    0.12712158842219246,\n",
       "    0.12429077316213537,\n",
       "    0.1252209508308658,\n",
       "    0.1238157409760687,\n",
       "    0.12574148895563902],\n",
       "   'val_mae': [0.09700217843055725,\n",
       "    0.11220118403434753,\n",
       "    0.09681713581085205,\n",
       "    0.08593250066041946,\n",
       "    0.09708725661039352,\n",
       "    0.08017586171627045,\n",
       "    0.10744882375001907,\n",
       "    0.09887751936912537,\n",
       "    0.09175834059715271,\n",
       "    0.08455181866884232,\n",
       "    0.08533991873264313,\n",
       "    0.09473288804292679,\n",
       "    0.10131128877401352,\n",
       "    0.0902414470911026,\n",
       "    0.09508726000785828,\n",
       "    0.07560067623853683,\n",
       "    0.0979449450969696,\n",
       "    0.09130514413118362,\n",
       "    0.09133164584636688,\n",
       "    0.10374065488576889,\n",
       "    0.10066147148609161,\n",
       "    0.0962744653224945,\n",
       "    0.08791863918304443,\n",
       "    0.07755590975284576,\n",
       "    0.08849945664405823,\n",
       "    0.09125634282827377,\n",
       "    0.08436042070388794,\n",
       "    0.08230248093605042,\n",
       "    0.094286248087883,\n",
       "    0.07932943105697632,\n",
       "    0.09193293005228043,\n",
       "    0.08086526393890381,\n",
       "    0.10323748737573624,\n",
       "    0.08704009652137756,\n",
       "    0.08015793561935425,\n",
       "    0.09013883024454117,\n",
       "    0.08438678830862045,\n",
       "    0.09490755945444107,\n",
       "    0.08039391785860062,\n",
       "    0.08682039380073547,\n",
       "    0.09333491325378418,\n",
       "    0.08424042165279388,\n",
       "    0.08313096314668655,\n",
       "    0.08457739651203156,\n",
       "    0.08569769561290741,\n",
       "    0.07428472489118576,\n",
       "    0.08108263462781906,\n",
       "    0.08046466112136841,\n",
       "    0.0817793533205986,\n",
       "    0.07570352405309677],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.00025,\n",
       "    0.00025,\n",
       "    0.00025,\n",
       "    0.00025,\n",
       "    0.00025,\n",
       "    0.00025,\n",
       "    0.00025,\n",
       "    0.00025,\n",
       "    0.00025,\n",
       "    0.00025,\n",
       "    0.00025,\n",
       "    0.00025,\n",
       "    0.00025,\n",
       "    0.00025,\n",
       "    0.00025,\n",
       "    0.00025,\n",
       "    0.000125,\n",
       "    0.000125,\n",
       "    0.000125,\n",
       "    0.000125,\n",
       "    0.000125,\n",
       "    0.000125,\n",
       "    0.000125]},\n",
       "  'metrics': {'mse': 0.0138251269236207,\n",
       "   'rmse': 0.11758029989594643,\n",
       "   'mae': 0.07570353895425797,\n",
       "   'r2': 0.663294792175293,\n",
       "   'mape': 10.910395622253418},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.034986546130052636,\n",
       "    0.03186688206291625,\n",
       "    0.029952087439596653,\n",
       "    0.029600969549002393,\n",
       "    0.029096757460917746,\n",
       "    0.02755409186439855,\n",
       "    0.027766721855316843,\n",
       "    0.02729891100898385,\n",
       "    0.02686611463182739,\n",
       "    0.02663215875093426,\n",
       "    0.026158209457727417,\n",
       "    0.02643365486125861,\n",
       "    0.026454020025474683,\n",
       "    0.026022800743313774,\n",
       "    0.02615611581131816,\n",
       "    0.02651328227615782,\n",
       "    0.025440830271691084,\n",
       "    0.025031532493552992,\n",
       "    0.024945064713912352,\n",
       "    0.02416515619760113,\n",
       "    0.025037690930600678,\n",
       "    0.025275525271094272,\n",
       "    0.024392534546287998,\n",
       "    0.024043753310771927,\n",
       "    0.02273520647681185,\n",
       "    0.02356523448335273,\n",
       "    0.02300677644754095,\n",
       "    0.023186860739120414,\n",
       "    0.02315059766572501,\n",
       "    0.023014764667355588,\n",
       "    0.02130113685104464,\n",
       "    0.02004730312286743,\n",
       "    0.019413009518757463,\n",
       "    0.018863654529143657,\n",
       "    0.018483037195567573,\n",
       "    0.017986637772992253,\n",
       "    0.01828732674143144,\n",
       "    0.017535582284576127,\n",
       "    0.016815325977014645,\n",
       "    0.01620588102377951,\n",
       "    0.01654500138413693,\n",
       "    0.01614715514837631,\n",
       "    0.015717560425400734,\n",
       "    0.014974507320273136,\n",
       "    0.014561950195846813,\n",
       "    0.013791648055692869,\n",
       "    0.013980034489317663,\n",
       "    0.013784886720324201,\n",
       "    0.013619225206119674,\n",
       "    0.013540992047637701],\n",
       "   'val_loss': [0.021252784878015518,\n",
       "    0.022659234702587128,\n",
       "    0.022058609873056412,\n",
       "    0.02236958220601082,\n",
       "    0.020544543862342834,\n",
       "    0.021142687648534775,\n",
       "    0.020572034642100334,\n",
       "    0.02038097009062767,\n",
       "    0.020484205335378647,\n",
       "    0.02050228789448738,\n",
       "    0.020083533599972725,\n",
       "    0.02061123587191105,\n",
       "    0.020431214943528175,\n",
       "    0.02020999602973461,\n",
       "    0.02074013277888298,\n",
       "    0.0206241887062788,\n",
       "    0.020697785541415215,\n",
       "    0.020159689709544182,\n",
       "    0.02048138901591301,\n",
       "    0.019995881244540215,\n",
       "    0.020784007385373116,\n",
       "    0.020370787009596825,\n",
       "    0.022362960502505302,\n",
       "    0.02007635496556759,\n",
       "    0.02016913704574108,\n",
       "    0.02046990767121315,\n",
       "    0.020906411111354828,\n",
       "    0.020606864243745804,\n",
       "    0.021256526932120323,\n",
       "    0.02004188671708107,\n",
       "    0.020822135731577873,\n",
       "    0.019978372380137444,\n",
       "    0.01998046040534973,\n",
       "    0.020536581054329872,\n",
       "    0.02044660784304142,\n",
       "    0.020392315462231636,\n",
       "    0.020510034635663033,\n",
       "    0.020047903060913086,\n",
       "    0.02003178372979164,\n",
       "    0.02001688815653324,\n",
       "    0.020063335075974464,\n",
       "    0.020222021266818047,\n",
       "    0.02028951421380043,\n",
       "    0.020495565608143806,\n",
       "    0.020215919241309166,\n",
       "    0.020124496892094612,\n",
       "    0.020220162346959114,\n",
       "    0.020119821652770042,\n",
       "    0.020338136702775955,\n",
       "    0.020185083150863647],\n",
       "   'train_mae': [0.19524268060922623,\n",
       "    0.1835965599332537,\n",
       "    0.17630855153713906,\n",
       "    0.1710011355046715,\n",
       "    0.17055052305970872,\n",
       "    0.16181933666978562,\n",
       "    0.1630976546023573,\n",
       "    0.15976617485284805,\n",
       "    0.1597945647580283,\n",
       "    0.1573942018938916,\n",
       "    0.15540364624134131,\n",
       "    0.15923557111195155,\n",
       "    0.15608329299305165,\n",
       "    0.15440381850515092,\n",
       "    0.15472790439214026,\n",
       "    0.15875925283346856,\n",
       "    0.15218610156859672,\n",
       "    0.15088984290403978,\n",
       "    0.15100646018981934,\n",
       "    0.14856842585972377,\n",
       "    0.15055546696696961,\n",
       "    0.1477956125246627,\n",
       "    0.1492842693946191,\n",
       "    0.14795080545757497,\n",
       "    0.14112907275557518,\n",
       "    0.1468686812690326,\n",
       "    0.142944887014372,\n",
       "    0.14429798243301256,\n",
       "    0.14228964197848523,\n",
       "    0.1454285377902644,\n",
       "    0.1378402691334486,\n",
       "    0.13228982181421348,\n",
       "    0.12984808241682394,\n",
       "    0.12935893583510602,\n",
       "    0.12652539328805038,\n",
       "    0.12509317243737833,\n",
       "    0.12597589833395823,\n",
       "    0.12337863285626684,\n",
       "    0.12185230771345752,\n",
       "    0.11948950987841402,\n",
       "    0.12041888146528176,\n",
       "    0.11852993895964963,\n",
       "    0.11704667152038642,\n",
       "    0.11649649616863046,\n",
       "    0.1105947702058724,\n",
       "    0.11090117959039551,\n",
       "    0.11118243874183723,\n",
       "    0.10856813272195202,\n",
       "    0.10866815197680678,\n",
       "    0.10874349384435586],\n",
       "   'val_mae': [0.1182350292801857,\n",
       "    0.1155073493719101,\n",
       "    0.12359635531902313,\n",
       "    0.13375090062618256,\n",
       "    0.09061132371425629,\n",
       "    0.10022694617509842,\n",
       "    0.09284612536430359,\n",
       "    0.09479699283838272,\n",
       "    0.08394190669059753,\n",
       "    0.07413812726736069,\n",
       "    0.08212370425462723,\n",
       "    0.07437315583229065,\n",
       "    0.0976320132613182,\n",
       "    0.07385330647230148,\n",
       "    0.1018470972776413,\n",
       "    0.08629541844129562,\n",
       "    0.09497470408678055,\n",
       "    0.07568106800317764,\n",
       "    0.06972856819629669,\n",
       "    0.08134143799543381,\n",
       "    0.08638651669025421,\n",
       "    0.08919110149145126,\n",
       "    0.1287546455860138,\n",
       "    0.0756705179810524,\n",
       "    0.08428878337144852,\n",
       "    0.09855078160762787,\n",
       "    0.10455189645290375,\n",
       "    0.09168145805597305,\n",
       "    0.11739648878574371,\n",
       "    0.07985079288482666,\n",
       "    0.09164654463529587,\n",
       "    0.07194919884204865,\n",
       "    0.08137187361717224,\n",
       "    0.06813083589076996,\n",
       "    0.07551734149456024,\n",
       "    0.07275412976741791,\n",
       "    0.09302043914794922,\n",
       "    0.08217480778694153,\n",
       "    0.07266397029161453,\n",
       "    0.07690811157226562,\n",
       "    0.07798000425100327,\n",
       "    0.08379893004894257,\n",
       "    0.08100868761539459,\n",
       "    0.07715517282485962,\n",
       "    0.08531346917152405,\n",
       "    0.07312466204166412,\n",
       "    0.07670442014932632,\n",
       "    0.08116123825311661,\n",
       "    0.08543659001588821,\n",
       "    0.08051954209804535],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.00025,\n",
       "    0.00025,\n",
       "    0.00025,\n",
       "    0.00025,\n",
       "    0.00025,\n",
       "    0.00025,\n",
       "    0.00025]},\n",
       "  'metrics': {'mse': 0.04064112901687622,\n",
       "   'rmse': 0.20159645090347256,\n",
       "   'mae': 0.08051953464746475,\n",
       "   'r2': 0.2924610376358032,\n",
       "   'mape': 4.737889766693115},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.037655064129623876,\n",
       "    0.032649315444046055,\n",
       "    0.03064448919532628,\n",
       "    0.029584229377837015,\n",
       "    0.029124318718396384,\n",
       "    0.028156832438604586,\n",
       "    0.027212041976123022,\n",
       "    0.027960894468786388,\n",
       "    0.026249491320601826,\n",
       "    0.02698752367547874,\n",
       "    0.026616405438760232,\n",
       "    0.027411008350037295,\n",
       "    0.026816989327299184,\n",
       "    0.025542126936388427,\n",
       "    0.026651194832962136,\n",
       "    0.025506542224822373,\n",
       "    0.025367348743923778,\n",
       "    0.024910257711749654,\n",
       "    0.025570247675581224,\n",
       "    0.025088220417242627,\n",
       "    0.02479252672015593,\n",
       "    0.024740485635040135,\n",
       "    0.02409836665952,\n",
       "    0.024082517765205484,\n",
       "    0.024270669043321032,\n",
       "    0.02301239475993247,\n",
       "    0.02354771178215742,\n",
       "    0.02308075830083469,\n",
       "    0.022950791336339097,\n",
       "    0.022370709202669817,\n",
       "    0.02227962492354985,\n",
       "    0.02183889629768914,\n",
       "    0.021969921501546072,\n",
       "    0.020436550715360147,\n",
       "    0.019764563004518378,\n",
       "    0.02083074248492204,\n",
       "    0.01949614504801816,\n",
       "    0.019118679141433073,\n",
       "    0.019110271321802305,\n",
       "    0.01837742688327,\n",
       "    0.01816883121199649,\n",
       "    0.018345950845757436,\n",
       "    0.016960560344159603,\n",
       "    0.017223400401015734,\n",
       "    0.017423166957651746,\n",
       "    0.017603704577376103,\n",
       "    0.017639926795301766,\n",
       "    0.016806793447327,\n",
       "    0.015467236974629862,\n",
       "    0.015622506046603465],\n",
       "   'val_loss': [0.005095501895993948,\n",
       "    0.006722958292812109,\n",
       "    0.005234993062913418,\n",
       "    0.00460275262594223,\n",
       "    0.004709327593445778,\n",
       "    0.00456258887425065,\n",
       "    0.0044926973059773445,\n",
       "    0.004981980659067631,\n",
       "    0.004605826456099749,\n",
       "    0.004324547480791807,\n",
       "    0.004531851503998041,\n",
       "    0.005085146054625511,\n",
       "    0.005320905242115259,\n",
       "    0.003989104647189379,\n",
       "    0.0043943352065980434,\n",
       "    0.004334195517003536,\n",
       "    0.006145771127194166,\n",
       "    0.004262000322341919,\n",
       "    0.0040619042702019215,\n",
       "    0.004294085316359997,\n",
       "    0.00476604700088501,\n",
       "    0.004089662339538336,\n",
       "    0.003938724286854267,\n",
       "    0.00427652383223176,\n",
       "    0.0041917855851352215,\n",
       "    0.0038722939789295197,\n",
       "    0.0040402947925031185,\n",
       "    0.004427381791174412,\n",
       "    0.003896786831319332,\n",
       "    0.004082908853888512,\n",
       "    0.00409676181152463,\n",
       "    0.004180331248790026,\n",
       "    0.003939021844416857,\n",
       "    0.004374989774078131,\n",
       "    0.004319936502724886,\n",
       "    0.0044796038419008255,\n",
       "    0.0038509482983499765,\n",
       "    0.0046129534021019936,\n",
       "    0.004053798504173756,\n",
       "    0.004104208201169968,\n",
       "    0.004301867447793484,\n",
       "    0.00384550541639328,\n",
       "    0.0040711211040616035,\n",
       "    0.003958188463002443,\n",
       "    0.004130315501242876,\n",
       "    0.004261174239218235,\n",
       "    0.0038850505370646715,\n",
       "    0.004101465921849012,\n",
       "    0.004064378794282675,\n",
       "    0.004301160573959351],\n",
       "   'train_mae': [0.2041602884900981,\n",
       "    0.1816690327792332,\n",
       "    0.17860711134713272,\n",
       "    0.17074274451568208,\n",
       "    0.16807880072758116,\n",
       "    0.1647438805164962,\n",
       "    0.16139515155348286,\n",
       "    0.16292501163893733,\n",
       "    0.1568341201235508,\n",
       "    0.1585422183932929,\n",
       "    0.15740968723749293,\n",
       "    0.15656262404959778,\n",
       "    0.1612080237988768,\n",
       "    0.1526587608045545,\n",
       "    0.1571440552843028,\n",
       "    0.15295036676628837,\n",
       "    0.15220651338840352,\n",
       "    0.15090162127182402,\n",
       "    0.15311638137389874,\n",
       "    0.15026565230098263,\n",
       "    0.14973864986978727,\n",
       "    0.14865006968892855,\n",
       "    0.1453764014716806,\n",
       "    0.14721410978457022,\n",
       "    0.145966873086732,\n",
       "    0.14398745421705575,\n",
       "    0.14426994632030354,\n",
       "    0.1416252581723805,\n",
       "    0.14139940615358024,\n",
       "    0.1401222032205812,\n",
       "    0.13858779828096257,\n",
       "    0.13833766019549862,\n",
       "    0.1356789341260647,\n",
       "    0.13310748595615912,\n",
       "    0.13002748350644933,\n",
       "    0.1331125880623686,\n",
       "    0.1298594664910744,\n",
       "    0.1288166750093986,\n",
       "    0.1277757384653749,\n",
       "    0.12479891319727075,\n",
       "    0.1248343242653485,\n",
       "    0.12659759793815942,\n",
       "    0.12097999016786444,\n",
       "    0.12133062759350086,\n",
       "    0.12121553230902245,\n",
       "    0.12232354257641168,\n",
       "    0.12020885327766681,\n",
       "    0.12180388641768489,\n",
       "    0.11431614335240989,\n",
       "    0.1141181056355608],\n",
       "   'val_mae': [0.0696435272693634,\n",
       "    0.08871205151081085,\n",
       "    0.05999954417347908,\n",
       "    0.04480218514800072,\n",
       "    0.0621461421251297,\n",
       "    0.04732159152626991,\n",
       "    0.0592753030359745,\n",
       "    0.056615620851516724,\n",
       "    0.06390530616044998,\n",
       "    0.04862581565976143,\n",
       "    0.061761144548654556,\n",
       "    0.07315098494291306,\n",
       "    0.07617845386266708,\n",
       "    0.0440564788877964,\n",
       "    0.057341381907463074,\n",
       "    0.056796107441186905,\n",
       "    0.08855532854795456,\n",
       "    0.04697804898023605,\n",
       "    0.04776076599955559,\n",
       "    0.042465053498744965,\n",
       "    0.06717721372842789,\n",
       "    0.050554294139146805,\n",
       "    0.041271716356277466,\n",
       "    0.043185047805309296,\n",
       "    0.049102533608675,\n",
       "    0.036216091364622116,\n",
       "    0.04459406062960625,\n",
       "    0.05553761497139931,\n",
       "    0.04059279337525368,\n",
       "    0.04558518901467323,\n",
       "    0.04265427216887474,\n",
       "    0.049753669649362564,\n",
       "    0.04352084547281265,\n",
       "    0.058455049991607666,\n",
       "    0.057127196341753006,\n",
       "    0.06250489503145218,\n",
       "    0.03962336853146553,\n",
       "    0.05165211856365204,\n",
       "    0.050255585461854935,\n",
       "    0.052307259291410446,\n",
       "    0.05449577793478966,\n",
       "    0.04529976844787598,\n",
       "    0.051451053470373154,\n",
       "    0.05038346350193024,\n",
       "    0.05253495275974274,\n",
       "    0.04847968742251396,\n",
       "    0.046479731798172,\n",
       "    0.051146816462278366,\n",
       "    0.050506267696619034,\n",
       "    0.05789831653237343],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001]},\n",
       "  'metrics': {'mse': 0.008602321147918701,\n",
       "   'rmse': 0.09274869890148703,\n",
       "   'mae': 0.057898301631212234,\n",
       "   'r2': 0.7785164713859558,\n",
       "   'mape': 7.6511406898498535},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.036774479473630585,\n",
       "    0.03164051752537489,\n",
       "    0.029573908013602098,\n",
       "    0.028697788218657174,\n",
       "    0.027597479491184154,\n",
       "    0.02777229870359103,\n",
       "    0.027648473034302394,\n",
       "    0.026416366298993427,\n",
       "    0.026040569537629683,\n",
       "    0.026227100441853207,\n",
       "    0.02643069646631678,\n",
       "    0.025893276712546747,\n",
       "    0.025846163493891558,\n",
       "    0.025250105746090414,\n",
       "    0.024946024517218272,\n",
       "    0.02546491681908568,\n",
       "    0.024331431556493043,\n",
       "    0.025037970021367074,\n",
       "    0.025659675213197865,\n",
       "    0.024885105341672896,\n",
       "    0.02474496072779099,\n",
       "    0.024444146764775118,\n",
       "    0.02424832616622249,\n",
       "    0.023654193555315336,\n",
       "    0.023062435537576677,\n",
       "    0.02318545536448558,\n",
       "    0.022717890795320272,\n",
       "    0.023450675730903942,\n",
       "    0.022570980712771416,\n",
       "    0.02329204302902023,\n",
       "    0.02227740700667103,\n",
       "    0.021911879318455854,\n",
       "    0.02198303593322635,\n",
       "    0.021330542303621768,\n",
       "    0.020650896771500508,\n",
       "    0.020200124941766262,\n",
       "    0.021287376806139947,\n",
       "    0.020636881577471893,\n",
       "    0.018854212450484433,\n",
       "    0.018138506558413307,\n",
       "    0.01787564034263293,\n",
       "    0.01714206483835975,\n",
       "    0.016900936188176275,\n",
       "    0.016893656551837923,\n",
       "    0.01687667431930701,\n",
       "    0.015722491933653753,\n",
       "    0.016451506906499467,\n",
       "    0.01518784196426471,\n",
       "    0.015054224167640011,\n",
       "    0.014896728672708075],\n",
       "   'val_loss': [0.010567956604063511,\n",
       "    0.00858230795711279,\n",
       "    0.010274250991642475,\n",
       "    0.008649341762065887,\n",
       "    0.008742688223719597,\n",
       "    0.009298738092184067,\n",
       "    0.008351149968802929,\n",
       "    0.007765456568449736,\n",
       "    0.007619089912623167,\n",
       "    0.008079783990979195,\n",
       "    0.007895121350884438,\n",
       "    0.007886877283453941,\n",
       "    0.007850557565689087,\n",
       "    0.00846069771796465,\n",
       "    0.00833730585873127,\n",
       "    0.008040492422878742,\n",
       "    0.0071265543811023235,\n",
       "    0.00717800622805953,\n",
       "    0.008053015917539597,\n",
       "    0.007130337879061699,\n",
       "    0.007615495007485151,\n",
       "    0.009636208415031433,\n",
       "    0.006951660383492708,\n",
       "    0.007300013210624456,\n",
       "    0.010569113306701183,\n",
       "    0.006267233286052942,\n",
       "    0.006232198793441057,\n",
       "    0.007846945896744728,\n",
       "    0.012349896132946014,\n",
       "    0.006925544235855341,\n",
       "    0.006491928361356258,\n",
       "    0.008834575302898884,\n",
       "    0.006698952987790108,\n",
       "    0.00911709200590849,\n",
       "    0.007238815538585186,\n",
       "    0.011222334578633308,\n",
       "    0.007328474894165993,\n",
       "    0.007035470567643642,\n",
       "    0.008318123407661915,\n",
       "    0.007793883793056011,\n",
       "    0.005891631823033094,\n",
       "    0.0069694216363132,\n",
       "    0.007966541685163975,\n",
       "    0.007982469163835049,\n",
       "    0.006293332204222679,\n",
       "    0.006065854337066412,\n",
       "    0.005939108319580555,\n",
       "    0.008203057572245598,\n",
       "    0.007605447433888912,\n",
       "    0.007377123460173607],\n",
       "   'train_mae': [0.19977018237113953,\n",
       "    0.17890638659397762,\n",
       "    0.16812630991141,\n",
       "    0.1675825039545695,\n",
       "    0.16098636140426,\n",
       "    0.15907541265090305,\n",
       "    0.16318068131804467,\n",
       "    0.15629227533936502,\n",
       "    0.155015912403663,\n",
       "    0.1543982525666555,\n",
       "    0.15447314927975336,\n",
       "    0.15389039516448974,\n",
       "    0.15324528589844705,\n",
       "    0.15092203716437022,\n",
       "    0.1515633448958397,\n",
       "    0.1504136085510254,\n",
       "    0.14983591164151827,\n",
       "    0.14633327374855679,\n",
       "    0.1524874990185102,\n",
       "    0.14715175479650497,\n",
       "    0.1494881456096967,\n",
       "    0.1463563879330953,\n",
       "    0.14747312292456627,\n",
       "    0.14161230400204658,\n",
       "    0.14285433168212572,\n",
       "    0.14250100950400035,\n",
       "    0.1398621934155623,\n",
       "    0.14102293426791826,\n",
       "    0.13699672222137452,\n",
       "    0.14186765551567077,\n",
       "    0.13839302683869997,\n",
       "    0.1345620428522428,\n",
       "    0.13760623062650362,\n",
       "    0.1338242381811142,\n",
       "    0.13168086831768352,\n",
       "    0.1306827167669932,\n",
       "    0.132792882869641,\n",
       "    0.13182035088539124,\n",
       "    0.12384515826900801,\n",
       "    0.1230795090397199,\n",
       "    0.12121048321326573,\n",
       "    0.11842467710375786,\n",
       "    0.11804852907856306,\n",
       "    0.1191516508658727,\n",
       "    0.11809950868288675,\n",
       "    0.11367142274975776,\n",
       "    0.11617535278201103,\n",
       "    0.11094131047526995,\n",
       "    0.11233172838886579,\n",
       "    0.10994572465618452],\n",
       "   'val_mae': [0.10268953442573547,\n",
       "    0.07192350924015045,\n",
       "    0.10828512161970139,\n",
       "    0.07673831284046173,\n",
       "    0.08702011406421661,\n",
       "    0.09677659720182419,\n",
       "    0.05903865769505501,\n",
       "    0.07490866631269455,\n",
       "    0.05485909804701805,\n",
       "    0.0765024870634079,\n",
       "    0.05303822457790375,\n",
       "    0.07299808412790298,\n",
       "    0.07243148982524872,\n",
       "    0.049379341304302216,\n",
       "    0.06934936344623566,\n",
       "    0.06767523288726807,\n",
       "    0.06395195424556732,\n",
       "    0.06707116216421127,\n",
       "    0.0693509504199028,\n",
       "    0.06312501430511475,\n",
       "    0.05669244006276131,\n",
       "    0.06650172173976898,\n",
       "    0.05614650249481201,\n",
       "    0.07086310535669327,\n",
       "    0.06588464230298996,\n",
       "    0.06027878820896149,\n",
       "    0.05429895222187042,\n",
       "    0.048602983355522156,\n",
       "    0.08148665726184845,\n",
       "    0.06052594259381294,\n",
       "    0.0566214881837368,\n",
       "    0.07210715115070343,\n",
       "    0.058621443808078766,\n",
       "    0.05951235070824623,\n",
       "    0.06088316813111305,\n",
       "    0.06357266753911972,\n",
       "    0.0749017670750618,\n",
       "    0.055342983454465866,\n",
       "    0.048846617341041565,\n",
       "    0.06575936079025269,\n",
       "    0.05393201485276222,\n",
       "    0.0551646463572979,\n",
       "    0.05653168261051178,\n",
       "    0.04773830622434616,\n",
       "    0.062336649745702744,\n",
       "    0.047433048486709595,\n",
       "    0.052486296743154526,\n",
       "    0.06113608554005623,\n",
       "    0.04890824481844902,\n",
       "    0.05990278720855713],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005]},\n",
       "  'metrics': {'mse': 0.014754248782992363,\n",
       "   'rmse': 0.12146706871820181,\n",
       "   'mae': 0.059902798384428024,\n",
       "   'r2': 0.6268211603164673,\n",
       "   'mape': 7.398682117462158},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.03170556487936166,\n",
       "    0.030399572044130293,\n",
       "    0.028696969301710205,\n",
       "    0.02772540719278397,\n",
       "    0.026864401004727808,\n",
       "    0.026394641687793118,\n",
       "    0.02671956577368321,\n",
       "    0.025735180224141768,\n",
       "    0.02515876927082577,\n",
       "    0.025604925629112027,\n",
       "    0.025256412103772163,\n",
       "    0.025312788033437346,\n",
       "    0.024309210283982176,\n",
       "    0.02508759023922105,\n",
       "    0.024492319252702496,\n",
       "    0.023752783515280294,\n",
       "    0.02431328336317693,\n",
       "    0.02403278916233009,\n",
       "    0.02327116209292604,\n",
       "    0.0235017005714678,\n",
       "    0.023689257551825816,\n",
       "    0.023564696492206667,\n",
       "    0.023062980673726526,\n",
       "    0.023325215694644758,\n",
       "    0.022587021932967247,\n",
       "    0.022450041746900927,\n",
       "    0.022341125674786105,\n",
       "    0.022467078070246404,\n",
       "    0.022133723321941592,\n",
       "    0.023259771897667838,\n",
       "    0.0223619872103295,\n",
       "    0.020067221455035672,\n",
       "    0.020368119791871118,\n",
       "    0.019539906252776424,\n",
       "    0.019807988387202064,\n",
       "    0.01849078594316398,\n",
       "    0.017529093149688937,\n",
       "    0.01691704655006047,\n",
       "    0.01678169170214284,\n",
       "    0.016181288136830254,\n",
       "    0.015926394461383744,\n",
       "    0.015283490531146526,\n",
       "    0.01564431355725373,\n",
       "    0.0145371196671359],\n",
       "   'val_loss': [0.05478015914559364,\n",
       "    0.055384572595357895,\n",
       "    0.050058651715517044,\n",
       "    0.05023850500583649,\n",
       "    0.05143212154507637,\n",
       "    0.05109049007296562,\n",
       "    0.04645991325378418,\n",
       "    0.05098412558436394,\n",
       "    0.0451415553689003,\n",
       "    0.04923522472381592,\n",
       "    0.044738464057445526,\n",
       "    0.04863343760371208,\n",
       "    0.048539821058511734,\n",
       "    0.05405263230204582,\n",
       "    0.04638097062706947,\n",
       "    0.0523996502161026,\n",
       "    0.05731799453496933,\n",
       "    0.04744794964790344,\n",
       "    0.045221902430057526,\n",
       "    0.0481017641723156,\n",
       "    0.04328799247741699,\n",
       "    0.053201302886009216,\n",
       "    0.047383733093738556,\n",
       "    0.04307424649596214,\n",
       "    0.048511702567338943,\n",
       "    0.05283011868596077,\n",
       "    0.05864769220352173,\n",
       "    0.055339787155389786,\n",
       "    0.054522424936294556,\n",
       "    0.05156756564974785,\n",
       "    0.055067844688892365,\n",
       "    0.05858460068702698,\n",
       "    0.05907740816473961,\n",
       "    0.062047962099313736,\n",
       "    0.05387197807431221,\n",
       "    0.05450614541769028,\n",
       "    0.05707355588674545,\n",
       "    0.055844005197286606,\n",
       "    0.05476386472582817,\n",
       "    0.05894653499126434,\n",
       "    0.06514359265565872,\n",
       "    0.06843341886997223,\n",
       "    0.0629432424902916,\n",
       "    0.06948225200176239],\n",
       "   'train_mae': [0.17751045813483576,\n",
       "    0.17353041518119075,\n",
       "    0.1623442588794616,\n",
       "    0.16262213188794353,\n",
       "    0.157604644615804,\n",
       "    0.1558304472315696,\n",
       "    0.15482154680836585,\n",
       "    0.15292021295716685,\n",
       "    0.15062123368824681,\n",
       "    0.15036921731887326,\n",
       "    0.15183300116369802,\n",
       "    0.1487581429462279,\n",
       "    0.1468099060077821,\n",
       "    0.14677782885489926,\n",
       "    0.14653137061865099,\n",
       "    0.14204578053566716,\n",
       "    0.1448951949996333,\n",
       "    0.14315392341344588,\n",
       "    0.14227235413366748,\n",
       "    0.14007125049829483,\n",
       "    0.14489166630852607,\n",
       "    0.13953722220274709,\n",
       "    0.1395624652504921,\n",
       "    0.1403553906467653,\n",
       "    0.13874011342563936,\n",
       "    0.1385759884311307,\n",
       "    0.13836247666228202,\n",
       "    0.13586239252359636,\n",
       "    0.13739636564447033,\n",
       "    0.1413860647909103,\n",
       "    0.1380187285042578,\n",
       "    0.12836270082381465,\n",
       "    0.12956545165469568,\n",
       "    0.12843723907586066,\n",
       "    0.1257041824921485,\n",
       "    0.12436873585947099,\n",
       "    0.12020419610123481,\n",
       "    0.11599768722249616,\n",
       "    0.11842278511293473,\n",
       "    0.1152117264366919,\n",
       "    0.11430168488333302,\n",
       "    0.11190233427670694,\n",
       "    0.11166541038020965,\n",
       "    0.10895029359286831],\n",
       "   'val_mae': [0.2678653597831726,\n",
       "    0.2545113265514374,\n",
       "    0.2513205409049988,\n",
       "    0.2396898716688156,\n",
       "    0.23925291001796722,\n",
       "    0.2300134003162384,\n",
       "    0.2318374067544937,\n",
       "    0.22341911494731903,\n",
       "    0.2221132516860962,\n",
       "    0.22904320061206818,\n",
       "    0.2198582887649536,\n",
       "    0.22608090937137604,\n",
       "    0.21966592967510223,\n",
       "    0.22244907915592194,\n",
       "    0.21631045639514923,\n",
       "    0.2096380889415741,\n",
       "    0.22909143567085266,\n",
       "    0.2191033512353897,\n",
       "    0.22747597098350525,\n",
       "    0.21664808690547943,\n",
       "    0.2086511254310608,\n",
       "    0.21392521262168884,\n",
       "    0.21762678027153015,\n",
       "    0.2121572494506836,\n",
       "    0.2158638834953308,\n",
       "    0.21335656940937042,\n",
       "    0.21386824548244476,\n",
       "    0.22563961148262024,\n",
       "    0.2152664065361023,\n",
       "    0.212795227766037,\n",
       "    0.20726783573627472,\n",
       "    0.21987397968769073,\n",
       "    0.2095349282026291,\n",
       "    0.2254662811756134,\n",
       "    0.21821191906929016,\n",
       "    0.2137819528579712,\n",
       "    0.21416018903255463,\n",
       "    0.217905655503273,\n",
       "    0.22769972681999207,\n",
       "    0.21541129052639008,\n",
       "    0.21706588566303253,\n",
       "    0.2282530963420868,\n",
       "    0.2128095030784607,\n",
       "    0.23732855916023254],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005]},\n",
       "  'metrics': {'mse': 0.13906854391098022,\n",
       "   'rmse': 0.3729189508606129,\n",
       "   'mae': 0.23732857406139374,\n",
       "   'r2': -0.06895661354064941,\n",
       "   'mape': 109.05182647705078},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.03581985365599394,\n",
       "    0.03178089958964847,\n",
       "    0.0289438437030185,\n",
       "    0.027902719477424398,\n",
       "    0.027228204358834773,\n",
       "    0.02698092977516353,\n",
       "    0.027140356076415628,\n",
       "    0.026930652413284406,\n",
       "    0.026241999643389136,\n",
       "    0.026461945759365335,\n",
       "    0.025867509189993143,\n",
       "    0.025182817480526865,\n",
       "    0.025159989221720025,\n",
       "    0.02596219244878739,\n",
       "    0.024798380938591436,\n",
       "    0.025347885559313,\n",
       "    0.024794358760118484,\n",
       "    0.02378488951944746,\n",
       "    0.024272311275126413,\n",
       "    0.02377306527341716,\n",
       "    0.02462202514288947,\n",
       "    0.02366350233205594,\n",
       "    0.02355854120105505,\n",
       "    0.023897979146568105,\n",
       "    0.023949560447363183,\n",
       "    0.02264803044090513,\n",
       "    0.02235654616379179,\n",
       "    0.022225449210964143,\n",
       "    0.022169157295138575,\n",
       "    0.020873565139481798,\n",
       "    0.02124770436785184,\n",
       "    0.021282693021930754,\n",
       "    0.02172892278758809,\n",
       "    0.02005242300219834],\n",
       "   'val_loss': [0.03850644454360008,\n",
       "    0.04162047803401947,\n",
       "    0.03818574547767639,\n",
       "    0.03923474997282028,\n",
       "    0.03573459014296532,\n",
       "    0.0362585224211216,\n",
       "    0.03520342335104942,\n",
       "    0.03651752695441246,\n",
       "    0.03671782836318016,\n",
       "    0.03567059710621834,\n",
       "    0.03821220621466637,\n",
       "    0.03449505195021629,\n",
       "    0.03578610345721245,\n",
       "    0.03409787639975548,\n",
       "    0.03506879881024361,\n",
       "    0.03582002595067024,\n",
       "    0.03448043018579483,\n",
       "    0.037084002047777176,\n",
       "    0.035231880843639374,\n",
       "    0.035213932394981384,\n",
       "    0.03544348478317261,\n",
       "    0.04096950218081474,\n",
       "    0.04019184410572052,\n",
       "    0.04231806844472885,\n",
       "    0.035613853484392166,\n",
       "    0.03959383815526962,\n",
       "    0.03851957619190216,\n",
       "    0.03991490975022316,\n",
       "    0.04610701650381088,\n",
       "    0.04585990682244301,\n",
       "    0.04514516144990921,\n",
       "    0.04379422962665558,\n",
       "    0.039772022515535355,\n",
       "    0.047586120665073395],\n",
       "   'train_mae': [0.19687242712825537,\n",
       "    0.17716717068105936,\n",
       "    0.16987884743139148,\n",
       "    0.16200371365994215,\n",
       "    0.15909270686097443,\n",
       "    0.1590712801553309,\n",
       "    0.1580672520212829,\n",
       "    0.1585990118328482,\n",
       "    0.15454341401346028,\n",
       "    0.15620816498994827,\n",
       "    0.1538336598314345,\n",
       "    0.15314364270307124,\n",
       "    0.14980491856113076,\n",
       "    0.15191800380125642,\n",
       "    0.1514205054845661,\n",
       "    0.1507402656134218,\n",
       "    0.14941864041611552,\n",
       "    0.14476290601305664,\n",
       "    0.1461209838744253,\n",
       "    0.14409279986284673,\n",
       "    0.14527030126191676,\n",
       "    0.1431174275930971,\n",
       "    0.1448821066878736,\n",
       "    0.1438277771230787,\n",
       "    0.1441867204848677,\n",
       "    0.13998470176011324,\n",
       "    0.13765733176842332,\n",
       "    0.13904777821153402,\n",
       "    0.1367801851592958,\n",
       "    0.13324434962123632,\n",
       "    0.13316409778781235,\n",
       "    0.133997340220958,\n",
       "    0.1338156193960458,\n",
       "    0.13138476805761456],\n",
       "   'val_mae': [0.24025775492191315,\n",
       "    0.24966442584991455,\n",
       "    0.23179832100868225,\n",
       "    0.23457017540931702,\n",
       "    0.19331417977809906,\n",
       "    0.21924999356269836,\n",
       "    0.21589554846286774,\n",
       "    0.2007116675376892,\n",
       "    0.2167568951845169,\n",
       "    0.21413852274417877,\n",
       "    0.22792379558086395,\n",
       "    0.1965893805027008,\n",
       "    0.21683169901371002,\n",
       "    0.2040274739265442,\n",
       "    0.19831621646881104,\n",
       "    0.2130267471075058,\n",
       "    0.2096320539712906,\n",
       "    0.2182985097169876,\n",
       "    0.21322661638259888,\n",
       "    0.19884416460990906,\n",
       "    0.20893047749996185,\n",
       "    0.2418832629919052,\n",
       "    0.23080623149871826,\n",
       "    0.24481944739818573,\n",
       "    0.2022397518157959,\n",
       "    0.23231352865695953,\n",
       "    0.22320739924907684,\n",
       "    0.22602152824401855,\n",
       "    0.24858848750591278,\n",
       "    0.24759282171726227,\n",
       "    0.24455556273460388,\n",
       "    0.24266165494918823,\n",
       "    0.21143954992294312,\n",
       "    0.24945147335529327],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005]},\n",
       "  'metrics': {'mse': 0.09517224878072739,\n",
       "   'rmse': 0.30849999802386935,\n",
       "   'mae': 0.24945147335529327,\n",
       "   'r2': 0.058207809925079346,\n",
       "   'mape': 56.5208625793457},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.0352557043855389,\n",
       "    0.03061219293511275,\n",
       "    0.02961973829025572,\n",
       "    0.027896171915486004,\n",
       "    0.028223333823861496,\n",
       "    0.027241047612871185,\n",
       "    0.026322944682430138,\n",
       "    0.02637651145006671,\n",
       "    0.02644456544834556,\n",
       "    0.027339668775146656,\n",
       "    0.025850920796845898,\n",
       "    0.025821455359233147,\n",
       "    0.025517053718706877,\n",
       "    0.025185872546651146,\n",
       "    0.025133296424015003,\n",
       "    0.0243991944148685,\n",
       "    0.025262467563152313,\n",
       "    0.025175237689505924,\n",
       "    0.02429990428076549,\n",
       "    0.02452975819169572,\n",
       "    0.02374654987149618,\n",
       "    0.023501977211598194,\n",
       "    0.025052751273368343,\n",
       "    0.0242367402622194,\n",
       "    0.02446339452977885,\n",
       "    0.023279854914907253,\n",
       "    0.02314310653530287,\n",
       "    0.024960470329405682,\n",
       "    0.022723908592579944,\n",
       "    0.022958572389501514,\n",
       "    0.02263296523949865,\n",
       "    0.022054989596433712,\n",
       "    0.021503178346337692,\n",
       "    0.020621357372764385,\n",
       "    0.020692035301842472,\n",
       "    0.020444235228227848,\n",
       "    0.019728009476128853,\n",
       "    0.01898101121751648,\n",
       "    0.018535610147272095,\n",
       "    0.01860328425060619,\n",
       "    0.018543141132051296,\n",
       "    0.01812364035427119,\n",
       "    0.019422331119351315,\n",
       "    0.01753177759096478,\n",
       "    0.016082172394927704,\n",
       "    0.015664418369080082,\n",
       "    0.01479892408701055,\n",
       "    0.014814589595930143,\n",
       "    0.01500321226194501,\n",
       "    0.01518006238973502],\n",
       "   'val_loss': [0.07517950981855392,\n",
       "    0.07393023371696472,\n",
       "    0.07124756276607513,\n",
       "    0.07351992279291153,\n",
       "    0.0711299404501915,\n",
       "    0.07098332792520523,\n",
       "    0.07024744153022766,\n",
       "    0.06962283700704575,\n",
       "    0.06786364316940308,\n",
       "    0.06724125146865845,\n",
       "    0.06968725472688675,\n",
       "    0.07141229510307312,\n",
       "    0.06722354143857956,\n",
       "    0.06592248380184174,\n",
       "    0.06747384369373322,\n",
       "    0.06559018790721893,\n",
       "    0.06800531595945358,\n",
       "    0.06705465912818909,\n",
       "    0.06553575396537781,\n",
       "    0.06656933575868607,\n",
       "    0.06561556458473206,\n",
       "    0.06649736315011978,\n",
       "    0.06805630028247833,\n",
       "    0.0673026368021965,\n",
       "    0.07076305896043777,\n",
       "    0.06667973101139069,\n",
       "    0.06522496789693832,\n",
       "    0.06946404278278351,\n",
       "    0.06753567606210709,\n",
       "    0.06080625206232071,\n",
       "    0.057782869786024094,\n",
       "    0.0672234296798706,\n",
       "    0.05502212047576904,\n",
       "    0.060705095529556274,\n",
       "    0.05556023120880127,\n",
       "    0.06917399168014526,\n",
       "    0.06427124887704849,\n",
       "    0.058341894298791885,\n",
       "    0.061129771173000336,\n",
       "    0.06243271753191948,\n",
       "    0.06443159282207489,\n",
       "    0.0652393326163292,\n",
       "    0.0587146021425724,\n",
       "    0.06243908405303955,\n",
       "    0.05567776411771774,\n",
       "    0.0691656693816185,\n",
       "    0.06412708759307861,\n",
       "    0.062271323055028915,\n",
       "    0.06727989763021469,\n",
       "    0.07370300590991974],\n",
       "   'train_mae': [0.19463262887615146,\n",
       "    0.1717850398836714,\n",
       "    0.16924190205154996,\n",
       "    0.16192094372077423,\n",
       "    0.16196877125537756,\n",
       "    0.15859030683835348,\n",
       "    0.1565067366217122,\n",
       "    0.15537609453454163,\n",
       "    0.15311939698277097,\n",
       "    0.15506858536691376,\n",
       "    0.15421380125211948,\n",
       "    0.15216881239956076,\n",
       "    0.14881973578171295,\n",
       "    0.1516024698362206,\n",
       "    0.14848557408108856,\n",
       "    0.14474801764343725,\n",
       "    0.1486506423715389,\n",
       "    0.14956870878284628,\n",
       "    0.14457979279034067,\n",
       "    0.14669555154713718,\n",
       "    0.1432598741217093,\n",
       "    0.14005855199965564,\n",
       "    0.14826071736487476,\n",
       "    0.14614384531071692,\n",
       "    0.1447846209912589,\n",
       "    0.1447324515743689,\n",
       "    0.13883443786339325,\n",
       "    0.15036211275693143,\n",
       "    0.14021250108877817,\n",
       "    0.13836594625855936,\n",
       "    0.13869836294289792,\n",
       "    0.13680894898645807,\n",
       "    0.13697580112652344,\n",
       "    0.130910865510955,\n",
       "    0.1303887807510116,\n",
       "    0.13038441042105356,\n",
       "    0.1296063740596627,\n",
       "    0.1264693389336268,\n",
       "    0.12491554044412845,\n",
       "    0.12423921421621785,\n",
       "    0.12450634665561444,\n",
       "    0.12170884487303821,\n",
       "    0.12514747408303348,\n",
       "    0.12148553674871271,\n",
       "    0.1155366651488073,\n",
       "    0.11409112020875468,\n",
       "    0.10909757853457422,\n",
       "    0.11030235389868419,\n",
       "    0.11016569873600295,\n",
       "    0.11105423082004894],\n",
       "   'val_mae': [0.2833964228630066,\n",
       "    0.30220159888267517,\n",
       "    0.2861112356185913,\n",
       "    0.2768956422805786,\n",
       "    0.2833670675754547,\n",
       "    0.29151222109794617,\n",
       "    0.2821907699108124,\n",
       "    0.2641906440258026,\n",
       "    0.27193430066108704,\n",
       "    0.25551143288612366,\n",
       "    0.29280921816825867,\n",
       "    0.3022516071796417,\n",
       "    0.2679414749145508,\n",
       "    0.2682698667049408,\n",
       "    0.2732431888580322,\n",
       "    0.26576942205429077,\n",
       "    0.27383819222450256,\n",
       "    0.27906256914138794,\n",
       "    0.2690327763557434,\n",
       "    0.2795095145702362,\n",
       "    0.26597389578819275,\n",
       "    0.26047953963279724,\n",
       "    0.2883201837539673,\n",
       "    0.2871444523334503,\n",
       "    0.27874380350112915,\n",
       "    0.27623286843299866,\n",
       "    0.27915966510772705,\n",
       "    0.2987775504589081,\n",
       "    0.29400935769081116,\n",
       "    0.24943898618221283,\n",
       "    0.25767233967781067,\n",
       "    0.2668219804763794,\n",
       "    0.24787400662899017,\n",
       "    0.2590795159339905,\n",
       "    0.2589012682437897,\n",
       "    0.2622840106487274,\n",
       "    0.27755245566368103,\n",
       "    0.27111104130744934,\n",
       "    0.2651554048061371,\n",
       "    0.2469869703054428,\n",
       "    0.27764278650283813,\n",
       "    0.27636921405792236,\n",
       "    0.25288593769073486,\n",
       "    0.24559840559959412,\n",
       "    0.23783990740776062,\n",
       "    0.2818753719329834,\n",
       "    0.2687808871269226,\n",
       "    0.2584298849105835,\n",
       "    0.2660440504550934,\n",
       "    0.2870626747608185],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005]},\n",
       "  'metrics': {'mse': 0.14769043028354645,\n",
       "   'rmse': 0.3843051265382059,\n",
       "   'mae': 0.2870626747608185,\n",
       "   'r2': 0.07417482137680054,\n",
       "   'mape': 62.45063400268555},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}}]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965dd293",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.utils:Saved arrays to data/phas5_data_*.npy\n",
      "INFO:src.utils:X shape: (1056, 7, 11, 9), Y shape: (1056, 1, 11)\n",
      "INFO:src.pipeline:Loading data...\n",
      "INFO:src.utils:Loaded arrays from data/phas5_data_*.npy\n",
      "INFO:src.utils:X shape: (1056, 7, 11, 9), Y shape: (1056, 1, 11)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 2: Building model arrays (X, Y) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:src.pipeline:Failed to construct reference dataframe for NAM comparison: could not convert string to float: '2014-01-03 14:00:00+00:00'\n",
      "INFO:src.utils:Loaded 35 folds from exp-003/exp-003rolling_origin_splits.json\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 1 ===\n",
      "INFO:src.pipeline:Train samples: 21, Val samples: 18\n",
      "INFO:src.pipeline:Model parameters: 94,475\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.056521, Val Loss: 0.097582, Train MAE: 0.241174, Val MAE: 0.341793, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.051135, Val Loss: 0.089080, Train MAE: 0.223228, Val MAE: 0.332661, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.044316, Val Loss: 0.078961, Train MAE: 0.192104, Val MAE: 0.334169, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.027990, Val Loss: 0.184373, Train MAE: 0.153841, Val MAE: 0.511356, LR: 0.001000\n",
      "INFO:src.engine:Epoch [50/50] - Train Loss: 0.024508, Val Loss: 0.144790, Train MAE: 0.150110, Val MAE: 0.462067, LR: 0.000500\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 1.\n",
      "INFO:src.pipeline:Fold 1 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.289604\n",
      "INFO:src.pipeline:  rmse: 0.538149\n",
      "INFO:src.pipeline:  mae: 0.462067\n",
      "INFO:src.pipeline:  r2: -0.749676\n",
      "INFO:src.pipeline:  mape: 68.026848\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 2 ===\n",
      "INFO:src.pipeline:Train samples: 40, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 94,475\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.061697, Val Loss: 0.050070, Train MAE: 0.291221, Val MAE: 0.278748, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.058376, Val Loss: 0.056609, Train MAE: 0.282970, Val MAE: 0.302836, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 26\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 2.\n",
      "INFO:src.pipeline:Fold 2 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.116785\n",
      "INFO:src.pipeline:  rmse: 0.341739\n",
      "INFO:src.pipeline:  mae: 0.310008\n",
      "INFO:src.pipeline:  r2: -0.049910\n",
      "INFO:src.pipeline:  mape: 68.845589\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 3 ===\n",
      "INFO:src.pipeline:Train samples: 71, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 94,475\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.054084, Val Loss: 0.033211, Train MAE: 0.266147, Val MAE: 0.209085, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.052676, Val Loss: 0.027628, Train MAE: 0.256740, Val MAE: 0.184221, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.037011, Val Loss: 0.027467, Train MAE: 0.196689, Val MAE: 0.171648, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.024642, Val Loss: 0.028390, Train MAE: 0.156203, Val MAE: 0.176091, LR: 0.001000\n",
      "INFO:src.engine:Epoch [50/50] - Train Loss: 0.025213, Val Loss: 0.028041, Train MAE: 0.150430, Val MAE: 0.157203, LR: 0.000500\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 3.\n",
      "INFO:src.pipeline:Fold 3 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.056081\n",
      "INFO:src.pipeline:  rmse: 0.236815\n",
      "INFO:src.pipeline:  mae: 0.157203\n",
      "INFO:src.pipeline:  r2: 0.324149\n",
      "INFO:src.pipeline:  mape: 38.256535\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 4 ===\n",
      "INFO:src.pipeline:Train samples: 101, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 94,475\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.039917, Val Loss: 0.014063, Train MAE: 0.221201, Val MAE: 0.139516, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.033806, Val Loss: 0.012698, Train MAE: 0.190277, Val MAE: 0.106916, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.029452, Val Loss: 0.011651, Train MAE: 0.170673, Val MAE: 0.107846, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.031116, Val Loss: 0.011944, Train MAE: 0.179662, Val MAE: 0.102601, LR: 0.001000\n",
      "Exception ignored in: <function _releaseLock at 0x7fa552e9e7a0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/logging/__init__.py\", line 228, in _releaseLock\n",
      "    def _releaseLock():\n",
      "KeyboardInterrupt: \n",
      "INFO:src.engine:Epoch [50/50] - Train Loss: 0.021400, Val Loss: 0.011889, Train MAE: 0.140039, Val MAE: 0.094648, LR: 0.000500\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 4.\n",
      "INFO:src.pipeline:Fold 4 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.023779\n",
      "INFO:src.pipeline:  rmse: 0.154203\n",
      "INFO:src.pipeline:  mae: 0.094648\n",
      "INFO:src.pipeline:  r2: 0.489852\n",
      "INFO:src.pipeline:  mape: 14.489473\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 5 ===\n",
      "INFO:src.pipeline:Train samples: 132, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 94,475\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.041353, Val Loss: 0.008767, Train MAE: 0.221385, Val MAE: 0.123045, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.036919, Val Loss: 0.006561, Train MAE: 0.182816, Val MAE: 0.075655, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.034941, Val Loss: 0.007404, Train MAE: 0.185780, Val MAE: 0.080267, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 35\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 5.\n",
      "INFO:src.pipeline:Fold 5 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.018599\n",
      "INFO:src.pipeline:  rmse: 0.136378\n",
      "INFO:src.pipeline:  mae: 0.096828\n",
      "INFO:src.pipeline:  r2: 0.303155\n",
      "INFO:src.pipeline:  mape: 11.521437\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 6 ===\n",
      "INFO:src.pipeline:Train samples: 162, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 94,475\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n"
     ]
    }
   ],
   "source": [
    "from src.preprocessing import KBinConfig, process_splits_to_kbins, build_model_arrays, to_fixedgrid_multiindex\n",
    "import pandas as pd\n",
    "from src.utils import DataManager\n",
    "from src.pipeline import SolarForecastingPipeline\n",
    "\n",
    "\n",
    "df_phase5 = df_interpolated[['CSI_ghi', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos',\n",
    "     'nam_cc',\n",
    "        '80_cloud_cover',\n",
    "       '56_cloud_cover', '20_cloud_cover',\n",
    "       '88_cloud_cover']]\n",
    "\n",
    "\n",
    "# --- Block 1: Define variables ---\n",
    "USE_KBINS = False\n",
    "k_bins = 60\n",
    "TARGET_COL = \"CSI_ghi\"  # <-- FIX 1: Define Target FIRST\n",
    "history_days = 7\n",
    "horizon_days = 1\n",
    "\n",
    "# This list now correctly excludes the target\n",
    "feature_cols = [c for c in df_phase5.columns.tolist() if c != TARGET_COL]\n",
    "\n",
    "# --- Block 2: Build model arrays (Your code was correct here) ---\n",
    "if USE_KBINS:\n",
    "        print(\"\\n--- Step 2: Converting full dataset to K-Bins format ---\")\n",
    "        # ... (rest of your K-Bins code) ...\n",
    "else:\n",
    "    print(\"\\n--- Step 2: Building model arrays (X, Y) ---\")\n",
    "    fixed_df = to_fixedgrid_multiindex(df_phase5, timestamp_col=\"measurement_time\", expected_T=None)  # or set T\n",
    "    \n",
    "    # You correctly passed the filtered 'feature_cols' here, fixing the data leak!\n",
    "    ph5_X, ph5_Y, ph5_labels_list = build_model_arrays(\n",
    "        fixed_df,\n",
    "        feature_cols=feature_cols,  # <-- This is correct!\n",
    "        target_col=TARGET_COL,\n",
    "        history_days=history_days,\n",
    "        horizon_days=horizon_days,\n",
    "    )\n",
    "\n",
    "# --- Block 3: Save arrays ---\n",
    "data_manager = DataManager()\n",
    "data_manager.save_arrays(\n",
    "    ph5_X, ph5_Y,\n",
    "    pd.DataFrame(index=pd.to_datetime(ph5_labels_list, utc=True)),\n",
    "    filename_prefix='phas5_data',  # <-- You are saving as 'phas2_data'\n",
    "    feature_cols=feature_cols,\n",
    "    target_col=TARGET_COL,\n",
    "    metadata={\n",
    "        \"input_csv\": \"data/processed/dayTime_NAM_spatial_5locations_dayahead_features_processed.csv\",\n",
    "        \"timestamp_col\": \"measurement_time\",\n",
    "        \"feature_set\": feature_cols,\n",
    "        \"history_days\": 7,\n",
    "        \"horizon_days\": 1,\n",
    "        \"k_bins\": None,\n",
    "    }\n",
    ")\n",
    "\n",
    "# --- Block 4: Configure and Run ---\n",
    "LSTM_CONFIG = {\n",
    "    \"experiment_name\": \"LSTM_UniDirectional_phase5_exp02\",\n",
    "    \"model_type\": \"LSTM\",\n",
    "    \"model_config\": {\n",
    "        \"hidden_size\": 64,\n",
    "        \"num_layers\": 2,\n",
    "        \"dropout\": 0.25,\n",
    "        \"bidirectional\": False,\n",
    "    },\n",
    "    \"data_prefix\": \"phas5_data\",  # <-- FIX 2: Match the filename_prefix\n",
    "    \"splits_file\": \"exp-003/exp-003rolling_origin_splits.json\",\n",
    "    \"feature_cols\": feature_cols,\n",
    "    \"feature_selection\": feature_cols, # <-- This is correct!\n",
    "    \"target_col\": TARGET_COL,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_epochs\": 50,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"loss_function\": \"Huber\",  # <-- This setting needs the fix below\n",
    "    \"early_stopping_patience\": 20,\n",
    "    \"max_folds\": 35,\n",
    "}\n",
    "\n",
    "# This will now work after you apply the fixes below\n",
    "pipeline = SolarForecastingPipeline(LSTM_CONFIG)\n",
    "phas5_LSTM_UniDir_fold_results, summary = pipeline.run()\n",
    "\n",
    "# To get the model from the LAST fold:\n",
    "phas5_LSTM_UniDir_fold_results_model = phas5_LSTM_UniDir_fold_results[-1]['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103155c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactive plot saved successfully to: solar_irradiance_fold35.html\n",
      "Open this file in any web browser to view the interactive visualization.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2c9c54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
