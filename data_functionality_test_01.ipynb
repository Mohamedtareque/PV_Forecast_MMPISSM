{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8b3925a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Data\n",
    "from src.data_preparation import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f69e9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 11,560 records\n",
      "Date range: 2014-01-03 14:00:00+00:00 to 2016-12-30 23:00:00+00:00\n",
      "Timezone: UTC\n",
      "Loaded 10,510 records\n",
      "Date range: 2014-01-03 14:00:00+00:00 to 2016-12-30 23:00:00+00:00\n",
      "Timezone: UTC\n"
     ]
    }
   ],
   "source": [
    "PROCESSED_DATA_PATH = \"data/processed/df_1h_lag_BLV_spatial_images.csv\"\n",
    "df = load_data(PROCESSED_DATA_PATH, date_col=\"measurement_time\")\n",
    "\n",
    "PROCESSED_DATA_PATH_2 = \"data/processed/df_3h_lag_BLV_spatial_images.csv\"\n",
    "df2 = load_data(PROCESSED_DATA_PATH_2, date_col=\"measurement_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a7d74f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"Unnamed: 0\", \"timestamp\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "575ce992",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.drop(columns=[\"Unnamed: 0\", \"timestamp\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92729be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ghi</th>\n",
       "      <th>dni</th>\n",
       "      <th>PST_Time_meas</th>\n",
       "      <th>solar_zenith</th>\n",
       "      <th>time_gap_hours</th>\n",
       "      <th>time_gap_norm</th>\n",
       "      <th>day_boundary_flag</th>\n",
       "      <th>hour_progression</th>\n",
       "      <th>absolute_hour</th>\n",
       "      <th>GHI_cs</th>\n",
       "      <th>...</th>\n",
       "      <th>ENT(G)</th>\n",
       "      <th>AVG(B)</th>\n",
       "      <th>STD(B)</th>\n",
       "      <th>ENT(B)</th>\n",
       "      <th>AVG(RB)</th>\n",
       "      <th>STD(RB)</th>\n",
       "      <th>ENT(RB)</th>\n",
       "      <th>AVG(NRB)</th>\n",
       "      <th>STD(NRB)</th>\n",
       "      <th>ENT(NRB)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>measurement_time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-01-03 14:00:00+00:00</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2014-01-03 06:29:30-08:00</td>\n",
       "      <td>100.276046</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.006944</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>6.491667</td>\n",
       "      <td>24.491667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-03 15:00:00+00:00</th>\n",
       "      <td>12.914500</td>\n",
       "      <td>89.245833</td>\n",
       "      <td>2014-01-03 07:29:30-08:00</td>\n",
       "      <td>89.724162</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.491667</td>\n",
       "      <td>25.491667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.564803</td>\n",
       "      <td>151.246458</td>\n",
       "      <td>28.520968</td>\n",
       "      <td>5.360367</td>\n",
       "      <td>0.808373</td>\n",
       "      <td>0.144332</td>\n",
       "      <td>4.688985</td>\n",
       "      <td>-0.113270</td>\n",
       "      <td>0.086132</td>\n",
       "      <td>4.008980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-03 16:00:00+00:00</th>\n",
       "      <td>143.775500</td>\n",
       "      <td>636.475000</td>\n",
       "      <td>2014-01-03 08:29:30-08:00</td>\n",
       "      <td>80.146724</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.491667</td>\n",
       "      <td>26.491667</td>\n",
       "      <td>35.159500</td>\n",
       "      <td>...</td>\n",
       "      <td>5.510188</td>\n",
       "      <td>141.713212</td>\n",
       "      <td>33.481258</td>\n",
       "      <td>5.414777</td>\n",
       "      <td>0.909988</td>\n",
       "      <td>0.148683</td>\n",
       "      <td>4.742913</td>\n",
       "      <td>-0.053787</td>\n",
       "      <td>0.079303</td>\n",
       "      <td>3.913008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-03 17:00:00+00:00</th>\n",
       "      <td>289.955000</td>\n",
       "      <td>783.193333</td>\n",
       "      <td>2014-01-03 09:29:30-08:00</td>\n",
       "      <td>71.988814</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.491667</td>\n",
       "      <td>27.491667</td>\n",
       "      <td>184.554711</td>\n",
       "      <td>...</td>\n",
       "      <td>5.478535</td>\n",
       "      <td>137.213417</td>\n",
       "      <td>35.403827</td>\n",
       "      <td>5.377718</td>\n",
       "      <td>0.965367</td>\n",
       "      <td>0.142668</td>\n",
       "      <td>4.686905</td>\n",
       "      <td>-0.022647</td>\n",
       "      <td>0.071275</td>\n",
       "      <td>3.777387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-03 18:00:00+00:00</th>\n",
       "      <td>402.643333</td>\n",
       "      <td>843.050000</td>\n",
       "      <td>2014-01-03 10:29:30-08:00</td>\n",
       "      <td>65.816158</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.491667</td>\n",
       "      <td>28.491667</td>\n",
       "      <td>321.992462</td>\n",
       "      <td>...</td>\n",
       "      <td>5.456043</td>\n",
       "      <td>138.616210</td>\n",
       "      <td>36.162608</td>\n",
       "      <td>5.356960</td>\n",
       "      <td>0.947050</td>\n",
       "      <td>0.140950</td>\n",
       "      <td>4.682247</td>\n",
       "      <td>-0.032250</td>\n",
       "      <td>0.072482</td>\n",
       "      <td>3.788302</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 83 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  ghi         dni              PST_Time_meas  \\\n",
       "measurement_time                                                               \n",
       "2014-01-03 14:00:00+00:00    0.000000    0.000000  2014-01-03 06:29:30-08:00   \n",
       "2014-01-03 15:00:00+00:00   12.914500   89.245833  2014-01-03 07:29:30-08:00   \n",
       "2014-01-03 16:00:00+00:00  143.775500  636.475000  2014-01-03 08:29:30-08:00   \n",
       "2014-01-03 17:00:00+00:00  289.955000  783.193333  2014-01-03 09:29:30-08:00   \n",
       "2014-01-03 18:00:00+00:00  402.643333  843.050000  2014-01-03 10:29:30-08:00   \n",
       "\n",
       "                           solar_zenith  time_gap_hours  time_gap_norm  \\\n",
       "measurement_time                                                         \n",
       "2014-01-03 14:00:00+00:00    100.276046        0.166667       0.006944   \n",
       "2014-01-03 15:00:00+00:00     89.724162        0.016667       0.000694   \n",
       "2014-01-03 16:00:00+00:00     80.146724        0.016667       0.000694   \n",
       "2014-01-03 17:00:00+00:00     71.988814        0.016667       0.000694   \n",
       "2014-01-03 18:00:00+00:00     65.816158        0.016667       0.000694   \n",
       "\n",
       "                           day_boundary_flag  hour_progression  absolute_hour  \\\n",
       "measurement_time                                                                \n",
       "2014-01-03 14:00:00+00:00           0.016667          6.491667      24.491667   \n",
       "2014-01-03 15:00:00+00:00           0.000000          7.491667      25.491667   \n",
       "2014-01-03 16:00:00+00:00           0.000000          8.491667      26.491667   \n",
       "2014-01-03 17:00:00+00:00           0.000000          9.491667      27.491667   \n",
       "2014-01-03 18:00:00+00:00           0.000000         10.491667      28.491667   \n",
       "\n",
       "                               GHI_cs  ...    ENT(G)      AVG(B)     STD(B)  \\\n",
       "measurement_time                       ...                                    \n",
       "2014-01-03 14:00:00+00:00    0.000000  ...       NaN         NaN        NaN   \n",
       "2014-01-03 15:00:00+00:00    0.000000  ...  5.564803  151.246458  28.520968   \n",
       "2014-01-03 16:00:00+00:00   35.159500  ...  5.510188  141.713212  33.481258   \n",
       "2014-01-03 17:00:00+00:00  184.554711  ...  5.478535  137.213417  35.403827   \n",
       "2014-01-03 18:00:00+00:00  321.992462  ...  5.456043  138.616210  36.162608   \n",
       "\n",
       "                             ENT(B)   AVG(RB)   STD(RB)   ENT(RB)  AVG(NRB)  \\\n",
       "measurement_time                                                              \n",
       "2014-01-03 14:00:00+00:00       NaN       NaN       NaN       NaN       NaN   \n",
       "2014-01-03 15:00:00+00:00  5.360367  0.808373  0.144332  4.688985 -0.113270   \n",
       "2014-01-03 16:00:00+00:00  5.414777  0.909988  0.148683  4.742913 -0.053787   \n",
       "2014-01-03 17:00:00+00:00  5.377718  0.965367  0.142668  4.686905 -0.022647   \n",
       "2014-01-03 18:00:00+00:00  5.356960  0.947050  0.140950  4.682247 -0.032250   \n",
       "\n",
       "                           STD(NRB)  ENT(NRB)  \n",
       "measurement_time                               \n",
       "2014-01-03 14:00:00+00:00       NaN       NaN  \n",
       "2014-01-03 15:00:00+00:00  0.086132  4.008980  \n",
       "2014-01-03 16:00:00+00:00  0.079303  3.913008  \n",
       "2014-01-03 17:00:00+00:00  0.071275  3.777387  \n",
       "2014-01-03 18:00:00+00:00  0.072482  3.788302  \n",
       "\n",
       "[5 rows x 83 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10ba68f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ghi', 'dni', 'PST_Time_meas', 'solar_zenith', 'time_gap_hours',\n",
       "       'time_gap_norm', 'day_boundary_flag', 'hour_progression',\n",
       "       'absolute_hour', 'GHI_cs', 'DNI_cs', 'CSI_ghi', 'CSI_dni',\n",
       "       'season_flag', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos',\n",
       "       'nam_ghi', 'nam_dni', 'nam_cc', 'PST_Time_nam', 'nam_target_time',\n",
       "       'B_CSI_ghi_8h', 'V_CSI_ghi_8h', 'L_CSI_ghi_8h', 'B_CSI_ghi_9h',\n",
       "       'V_CSI_ghi_9h', 'L_CSI_ghi_9h', 'B_CSI_ghi_10h', 'V_CSI_ghi_10h',\n",
       "       'L_CSI_ghi_10h', 'B_CSI_ghi_11h', 'V_CSI_ghi_11h', 'L_CSI_ghi_11h',\n",
       "       'B_CSI_ghi_12h', 'V_CSI_ghi_12h', 'L_CSI_ghi_12h', 'B_CSI_ghi_13h',\n",
       "       'V_CSI_ghi_13h', 'L_CSI_ghi_13h', 'B_CSI_ghi_14h', 'V_CSI_ghi_14h',\n",
       "       'L_CSI_ghi_14h', 'B_CSI_ghi_15h', 'V_CSI_ghi_15h', 'L_CSI_ghi_15h',\n",
       "       'B_CSI_ghi_16h', 'V_CSI_ghi_16h', 'L_CSI_ghi_16h', 'B_CSI_ghi_17h',\n",
       "       'V_CSI_ghi_17h', 'L_CSI_ghi_17h', 'B_CSI_ghi_18h', 'V_CSI_ghi_18h',\n",
       "       'L_CSI_ghi_18h', 'B_CSI_ghi_19h', 'V_CSI_ghi_19h', 'L_CSI_ghi_19h',\n",
       "       '80_dwsw', 'valtime', '80_cloud_cover', '56_dwsw', '56_cloud_cover',\n",
       "       '20_dwsw', '20_cloud_cover', '88_dwsw', '88_cloud_cover', 'AVG(R)',\n",
       "       'STD(R)', 'ENT(R)', 'AVG(G)', 'STD(G)', 'ENT(G)', 'AVG(B)', 'STD(B)',\n",
       "       'ENT(B)', 'AVG(RB)', 'STD(RB)', 'ENT(RB)', 'AVG(NRB)', 'STD(NRB)',\n",
       "       'ENT(NRB)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1f58e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ROLLING ORIGIN EVALUATION\n",
      "============================================================\n",
      "Total folds: 35\n",
      "Frequency: MS\n",
      "Data range: 2014-01-03 to 2016-12-30\n",
      "\n",
      "Fold Summary:\n",
      "  Fold 1: Train [2014-01-03 to 2014-02-01] (297 records) → Val [2014-02-02 to 2014-02-28] (186 records)\n",
      "  Fold 2: Train [2014-01-03 to 2014-03-01] (494 records) → Val [2014-03-02 to 2014-03-31] (319 records)\n",
      "  Fold 3: Train [2014-01-03 to 2014-04-01] (835 records) → Val [2014-04-02 to 2014-04-30] (308 records)\n",
      "  Fold 4: Train [2014-01-03 to 2014-05-01] (1,165 records) → Val [2014-05-02 to 2014-05-31] (319 records)\n",
      "  Fold 5: Train [2014-01-03 to 2014-06-01] (1,506 records) → Val [2014-06-02 to 2014-06-30] (308 records)\n",
      "  Fold 6: Train [2014-01-03 to 2014-07-01] (1,836 records) → Val [2014-07-02 to 2014-07-31] (319 records)\n",
      "  Fold 7: Train [2014-01-03 to 2014-08-01] (2,177 records) → Val [2014-08-02 to 2014-08-31] (319 records)\n",
      "  Fold 8: Train [2014-01-03 to 2014-09-01] (2,518 records) → Val [2014-09-02 to 2014-09-30] (308 records)\n",
      "  Fold 9: Train [2014-01-03 to 2014-10-01] (2,848 records) → Val [2014-10-02 to 2014-10-31] (319 records)\n",
      "  Fold 10: Train [2014-01-03 to 2014-11-01] (3,189 records) → Val [2014-11-02 to 2014-11-30] (308 records)\n",
      "  Fold 11: Train [2014-01-03 to 2014-12-01] (3,519 records) → Val [2014-12-02 to 2014-12-31] (319 records)\n",
      "  Fold 12: Train [2014-01-03 to 2015-01-01] (3,860 records) → Val [2015-01-02 to 2015-01-31] (319 records)\n",
      "  Fold 13: Train [2014-01-03 to 2015-02-01] (4,201 records) → Val [2015-02-02 to 2015-02-28] (286 records)\n",
      "  Fold 14: Train [2014-01-03 to 2015-03-01] (4,509 records) → Val [2015-03-02 to 2015-03-31] (319 records)\n",
      "  Fold 15: Train [2014-01-03 to 2015-04-01] (4,850 records) → Val [2015-04-02 to 2015-04-30] (308 records)\n",
      "  Fold 16: Train [2014-01-03 to 2015-05-01] (5,180 records) → Val [2015-05-02 to 2015-05-31] (319 records)\n",
      "  Fold 17: Train [2014-01-03 to 2015-06-01] (5,521 records) → Val [2015-06-02 to 2015-06-30] (308 records)\n",
      "  Fold 18: Train [2014-01-03 to 2015-07-01] (5,851 records) → Val [2015-07-02 to 2015-07-31] (319 records)\n",
      "  Fold 19: Train [2014-01-03 to 2015-08-01] (6,192 records) → Val [2015-08-02 to 2015-08-31] (308 records)\n",
      "  Fold 20: Train [2014-01-03 to 2015-09-01] (6,522 records) → Val [2015-09-02 to 2015-09-30] (308 records)\n",
      "  Fold 21: Train [2014-01-03 to 2015-10-01] (6,852 records) → Val [2015-10-02 to 2015-10-31] (319 records)\n",
      "  Fold 22: Train [2014-01-03 to 2015-11-01] (7,193 records) → Val [2015-11-02 to 2015-11-30] (308 records)\n",
      "  Fold 23: Train [2014-01-03 to 2015-12-01] (7,523 records) → Val [2015-12-02 to 2015-12-31] (319 records)\n",
      "  Fold 24: Train [2014-01-03 to 2016-01-01] (7,854 records) → Val [2016-01-02 to 2016-01-31] (220 records)\n",
      "  Fold 25: Train [2014-01-03 to 2016-02-01] (8,095 records) → Val [2016-02-02 to 2016-02-29] (231 records)\n",
      "  Fold 26: Train [2014-01-03 to 2016-03-01] (8,348 records) → Val [2016-03-02 to 2016-03-31] (319 records)\n",
      "  Fold 27: Train [2014-01-03 to 2016-04-01] (8,689 records) → Val [2016-04-02 to 2016-04-30] (308 records)\n",
      "  Fold 28: Train [2014-01-03 to 2016-05-01] (9,019 records) → Val [2016-05-02 to 2016-05-31] (319 records)\n",
      "  Fold 29: Train [2014-01-03 to 2016-06-01] (9,360 records) → Val [2016-06-02 to 2016-06-30] (308 records)\n",
      "  Fold 30: Train [2014-01-03 to 2016-07-01] (9,690 records) → Val [2016-07-02 to 2016-07-31] (297 records)\n",
      "  Fold 31: Train [2014-01-03 to 2016-08-01] (10,009 records) → Val [2016-08-02 to 2016-08-31] (319 records)\n",
      "  Fold 32: Train [2014-01-03 to 2016-09-01] (10,350 records) → Val [2016-09-02 to 2016-09-30] (308 records)\n",
      "  Fold 33: Train [2014-01-03 to 2016-10-01] (10,680 records) → Val [2016-10-02 to 2016-10-31] (319 records)\n",
      "  Fold 34: Train [2014-01-03 to 2016-11-01] (11,021 records) → Val [2016-11-02 to 2016-11-30] (308 records)\n",
      "  Fold 35: Train [2014-01-03 to 2016-12-01] (11,351 records) → Val [2016-12-02 to 2016-12-31] (208 records)\n",
      "============================================================\n",
      "\n",
      "Split metadata saved to 'splits/exp-004/' directory\n"
     ]
    }
   ],
   "source": [
    "# 2. Rolling Origin Split (More Suitable in TimeSeries) like K-Folds\n",
    "from src.data_preparation import rolling_origin_evaluation,save_splits_info\n",
    "\n",
    "rollingSplits_df = rolling_origin_evaluation(df=df, start_train = '2014-01-2',\n",
    "    end_train = '2016-12-31')\n",
    "save_splits_info({}, rollingSplits_df, experiment_name=\"exp-004\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "028b5b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ROLLING ORIGIN EVALUATION\n",
      "============================================================\n",
      "Total folds: 35\n",
      "Frequency: MS\n",
      "Data range: 2014-01-03 to 2016-12-30\n",
      "\n",
      "Fold Summary:\n",
      "  Fold 1: Train [2014-01-03 to 2014-02-01] (270 records) → Val [2014-02-02 to 2014-02-28] (170 records)\n",
      "  Fold 2: Train [2014-01-03 to 2014-03-01] (450 records) → Val [2014-03-02 to 2014-03-31] (290 records)\n",
      "  Fold 3: Train [2014-01-03 to 2014-04-01] (760 records) → Val [2014-04-02 to 2014-04-30] (280 records)\n",
      "  Fold 4: Train [2014-01-03 to 2014-05-01] (1,060 records) → Val [2014-05-02 to 2014-05-31] (290 records)\n",
      "  Fold 5: Train [2014-01-03 to 2014-06-01] (1,370 records) → Val [2014-06-02 to 2014-06-30] (280 records)\n",
      "  Fold 6: Train [2014-01-03 to 2014-07-01] (1,670 records) → Val [2014-07-02 to 2014-07-31] (290 records)\n",
      "  Fold 7: Train [2014-01-03 to 2014-08-01] (1,980 records) → Val [2014-08-02 to 2014-08-31] (290 records)\n",
      "  Fold 8: Train [2014-01-03 to 2014-09-01] (2,290 records) → Val [2014-09-02 to 2014-09-30] (280 records)\n",
      "  Fold 9: Train [2014-01-03 to 2014-10-01] (2,590 records) → Val [2014-10-02 to 2014-10-31] (290 records)\n",
      "  Fold 10: Train [2014-01-03 to 2014-11-01] (2,900 records) → Val [2014-11-02 to 2014-11-30] (280 records)\n",
      "  Fold 11: Train [2014-01-03 to 2014-12-01] (3,200 records) → Val [2014-12-02 to 2014-12-31] (290 records)\n",
      "  Fold 12: Train [2014-01-03 to 2015-01-01] (3,510 records) → Val [2015-01-02 to 2015-01-31] (290 records)\n",
      "  Fold 13: Train [2014-01-03 to 2015-02-01] (3,820 records) → Val [2015-02-02 to 2015-02-28] (260 records)\n",
      "  Fold 14: Train [2014-01-03 to 2015-03-01] (4,100 records) → Val [2015-03-02 to 2015-03-31] (290 records)\n",
      "  Fold 15: Train [2014-01-03 to 2015-04-01] (4,410 records) → Val [2015-04-02 to 2015-04-30] (280 records)\n",
      "  Fold 16: Train [2014-01-03 to 2015-05-01] (4,710 records) → Val [2015-05-02 to 2015-05-31] (290 records)\n",
      "  Fold 17: Train [2014-01-03 to 2015-06-01] (5,020 records) → Val [2015-06-02 to 2015-06-30] (280 records)\n",
      "  Fold 18: Train [2014-01-03 to 2015-07-01] (5,320 records) → Val [2015-07-02 to 2015-07-31] (290 records)\n",
      "  Fold 19: Train [2014-01-03 to 2015-08-01] (5,630 records) → Val [2015-08-02 to 2015-08-31] (280 records)\n",
      "  Fold 20: Train [2014-01-03 to 2015-09-01] (5,930 records) → Val [2015-09-02 to 2015-09-30] (280 records)\n",
      "  Fold 21: Train [2014-01-03 to 2015-10-01] (6,230 records) → Val [2015-10-02 to 2015-10-31] (290 records)\n",
      "  Fold 22: Train [2014-01-03 to 2015-11-01] (6,540 records) → Val [2015-11-02 to 2015-11-30] (280 records)\n",
      "  Fold 23: Train [2014-01-03 to 2015-12-01] (6,840 records) → Val [2015-12-02 to 2015-12-31] (290 records)\n",
      "  Fold 24: Train [2014-01-03 to 2016-01-01] (7,140 records) → Val [2016-01-02 to 2016-01-31] (200 records)\n",
      "  Fold 25: Train [2014-01-03 to 2016-02-01] (7,360 records) → Val [2016-02-02 to 2016-02-29] (210 records)\n",
      "  Fold 26: Train [2014-01-03 to 2016-03-01] (7,590 records) → Val [2016-03-02 to 2016-03-31] (290 records)\n",
      "  Fold 27: Train [2014-01-03 to 2016-04-01] (7,900 records) → Val [2016-04-02 to 2016-04-30] (280 records)\n",
      "  Fold 28: Train [2014-01-03 to 2016-05-01] (8,200 records) → Val [2016-05-02 to 2016-05-31] (290 records)\n",
      "  Fold 29: Train [2014-01-03 to 2016-06-01] (8,510 records) → Val [2016-06-02 to 2016-06-30] (280 records)\n",
      "  Fold 30: Train [2014-01-03 to 2016-07-01] (8,810 records) → Val [2016-07-02 to 2016-07-31] (270 records)\n",
      "  Fold 31: Train [2014-01-03 to 2016-08-01] (9,100 records) → Val [2016-08-02 to 2016-08-31] (290 records)\n",
      "  Fold 32: Train [2014-01-03 to 2016-09-01] (9,410 records) → Val [2016-09-02 to 2016-09-30] (280 records)\n",
      "  Fold 33: Train [2014-01-03 to 2016-10-01] (9,710 records) → Val [2016-10-02 to 2016-10-31] (290 records)\n",
      "  Fold 34: Train [2014-01-03 to 2016-11-01] (10,020 records) → Val [2016-11-02 to 2016-11-30] (280 records)\n",
      "  Fold 35: Train [2014-01-03 to 2016-12-01] (10,320 records) → Val [2016-12-02 to 2016-12-31] (190 records)\n",
      "============================================================\n",
      "\n",
      "Split metadata saved to 'splits/exp-005/' directory\n"
     ]
    }
   ],
   "source": [
    "# 2. Rolling Origin Split (More Suitable in TimeSeries) like K-Folds\n",
    "from src.data_preparation import rolling_origin_evaluation,save_splits_info\n",
    "\n",
    "rollingSplits_df2 = rolling_origin_evaluation(df=df2, start_train = '2014-01-2',\n",
    "    end_train = '2016-12-31')\n",
    "save_splits_info({}, rollingSplits_df2, experiment_name=\"exp-005\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9cb756f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does NAM records have a 1-hour lag ahead from measurements? Yes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_noIndex = df.reset_index()\n",
    "df_noIndex['nam_target_time'] = pd.to_datetime(df_noIndex['nam_target_time'])\n",
    "df_noIndex['measurement_time'] = pd.to_datetime(df_noIndex['measurement_time'])\n",
    "df_noIndex['time_diff'] = (df_noIndex['nam_target_time'] - df_noIndex['measurement_time']).dt.total_seconds() / 3600  # Calculate the time difference in hours\n",
    "    \n",
    "# Check if the time difference is exactly 1 hour\n",
    "is_nam_lag_correct = np.allclose(df_noIndex['time_diff'], 1)  # All time differences should be 1 hour\n",
    "print(f\"Does NAM records have a 1-hour lag ahead from measurements? {'Yes' if is_nam_lag_correct else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14df23d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2. Check if sequences have the same length across each day\n",
    "# # Extract date from the 'measurement_time'\n",
    "# df_noIndex['date'] = df_noIndex['measurement_time'].dt.date\n",
    "    \n",
    "# # Group by date and count records per day\n",
    "# daily_counts = df_noIndex.groupby('date').size()\n",
    "    \n",
    "# # Find dates where the record count is not equal to 24 (assuming 24 records per day)\n",
    "# inconsistent_dates = daily_counts[daily_counts != 11] \n",
    "    \n",
    "# # Print dates where records differ from 24\n",
    "# if not inconsistent_dates.empty:\n",
    "#     print(\"Dates with record counts different from 14 hours:\")\n",
    "#     print(inconsistent_dates)\n",
    "#     print(len(inconsistent_dates))\n",
    "# else:\n",
    "#     print(\"All dates have the expected 14-hour record count.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c93f72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a list of dates to drop\n",
    "# dates_to_drop = [\n",
    "#     '2014-01-03', '2014-01-30', '2014-02-05', '2014-02-13', '2014-02-19',\n",
    "#     '2015-08-23', '2015-08-24', '2015-12-31', '2016-01-01', '2016-07-13',\n",
    "#     '2016-07-15', '2016-12-31'\n",
    "# ]\n",
    "\n",
    "# # Print the first few rows of the index to see its format\n",
    "# print(\"Index sample before processing:\")\n",
    "# print(df.index[:5])\n",
    "# print(\"\\nIndex timezone:\", df.index.tz)\n",
    "\n",
    "# # Convert dates to datetime with UTC timezone\n",
    "# dates_to_drop = pd.to_datetime(dates_to_drop).tz_localize('UTC')\n",
    "\n",
    "# # Get the original size of the DataFrame\n",
    "# original_size = len(df)\n",
    "\n",
    "# df = df[~df.index.normalize().isin(dates_to_drop)]\n",
    "\n",
    "# # Print the number of rows dropped\n",
    "# print(f\"\\nOriginal number of records: {original_size}\")\n",
    "# print(f\"Number of records after dropping dates: {len(df)}\")\n",
    "# print(f\"Number of records dropped: {original_size - len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a003357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ghi', 'dni', 'PST_Time_meas', 'solar_zenith', 'time_gap_hours',\n",
       "       'time_gap_norm', 'day_boundary_flag', 'hour_progression',\n",
       "       'absolute_hour', 'GHI_cs', 'DNI_cs', 'CSI_ghi', 'CSI_dni',\n",
       "       'season_flag', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos',\n",
       "       'nam_ghi', 'nam_dni', 'nam_cc', 'PST_Time_nam', 'nam_target_time',\n",
       "       'B_CSI_ghi_8h', 'V_CSI_ghi_8h', 'L_CSI_ghi_8h', 'B_CSI_ghi_9h',\n",
       "       'V_CSI_ghi_9h', 'L_CSI_ghi_9h', 'B_CSI_ghi_10h', 'V_CSI_ghi_10h',\n",
       "       'L_CSI_ghi_10h', 'B_CSI_ghi_11h', 'V_CSI_ghi_11h', 'L_CSI_ghi_11h',\n",
       "       'B_CSI_ghi_12h', 'V_CSI_ghi_12h', 'L_CSI_ghi_12h', 'B_CSI_ghi_13h',\n",
       "       'V_CSI_ghi_13h', 'L_CSI_ghi_13h', 'B_CSI_ghi_14h', 'V_CSI_ghi_14h',\n",
       "       'L_CSI_ghi_14h', 'B_CSI_ghi_15h', 'V_CSI_ghi_15h', 'L_CSI_ghi_15h',\n",
       "       'B_CSI_ghi_16h', 'V_CSI_ghi_16h', 'L_CSI_ghi_16h', 'B_CSI_ghi_17h',\n",
       "       'V_CSI_ghi_17h', 'L_CSI_ghi_17h', 'B_CSI_ghi_18h', 'V_CSI_ghi_18h',\n",
       "       'L_CSI_ghi_18h', 'B_CSI_ghi_19h', 'V_CSI_ghi_19h', 'L_CSI_ghi_19h',\n",
       "       '80_dwsw', 'valtime', '80_cloud_cover', '56_dwsw', '56_cloud_cover',\n",
       "       '20_dwsw', '20_cloud_cover', '88_dwsw', '88_cloud_cover', 'AVG(R)',\n",
       "       'STD(R)', 'ENT(R)', 'AVG(G)', 'STD(G)', 'ENT(G)', 'AVG(B)', 'STD(B)',\n",
       "       'ENT(B)', 'AVG(RB)', 'STD(RB)', 'ENT(RB)', 'AVG(NRB)', 'STD(NRB)',\n",
       "       'ENT(NRB)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd1cb126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Features With NaN Values ---\n",
      "The following features have NaN values, with the total count for each:\n",
      "AVG(R)      1259\n",
      "STD(R)      1259\n",
      "ENT(R)      1259\n",
      "AVG(G)      1259\n",
      "STD(G)      1259\n",
      "ENT(G)      1259\n",
      "AVG(B)      1259\n",
      "STD(B)      1259\n",
      "ENT(B)      1259\n",
      "AVG(RB)     1259\n",
      "STD(RB)     1259\n",
      "ENT(RB)     1259\n",
      "AVG(NRB)    1259\n",
      "STD(NRB)    1259\n",
      "ENT(NRB)    1259\n",
      "dtype: int64\n",
      "\n",
      "========================================\n",
      "\n",
      "--- Distribution of NaN-Value Records by Hour ---\n",
      "Distribution of records (rows) containing at least one NaN, by hour:\n",
      "measurement_time\n",
      "2     1050\n",
      "14      84\n",
      "15      14\n",
      "16      14\n",
      "17      14\n",
      "18      14\n",
      "19      14\n",
      "20      13\n",
      "21      14\n",
      "22      14\n",
      "23      14\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total number of rows with at least one NaN: 1259\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "features_to_check = ['ghi', 'dni', 'solar_zenith', 'GHI_cs', 'DNI_cs', 'CSI_ghi', \n",
    "                                   'CSI_dni',\n",
    "                                   'nam_ghi', 'nam_dni', 'nam_cc', 'B_CSI_ghi_8h', 'V_CSI_ghi_8h', 'L_CSI_ghi_8h', 'B_CSI_ghi_9h',\n",
    "       'V_CSI_ghi_9h', 'L_CSI_ghi_9h', 'B_CSI_ghi_10h', 'V_CSI_ghi_10h',\n",
    "       'L_CSI_ghi_10h', 'B_CSI_ghi_11h', 'V_CSI_ghi_11h', 'L_CSI_ghi_11h',\n",
    "       'B_CSI_ghi_12h', 'V_CSI_ghi_12h', 'L_CSI_ghi_12h', 'B_CSI_ghi_13h',\n",
    "       'V_CSI_ghi_13h', 'L_CSI_ghi_13h', 'B_CSI_ghi_14h', 'V_CSI_ghi_14h',\n",
    "       'L_CSI_ghi_14h', 'B_CSI_ghi_15h', 'V_CSI_ghi_15h', 'L_CSI_ghi_15h',\n",
    "       'B_CSI_ghi_16h', 'V_CSI_ghi_16h', 'L_CSI_ghi_16h', 'B_CSI_ghi_17h',\n",
    "       'V_CSI_ghi_17h', 'L_CSI_ghi_17h', 'B_CSI_ghi_18h', 'V_CSI_ghi_18h',\n",
    "       'L_CSI_ghi_18h', 'B_CSI_ghi_19h', 'V_CSI_ghi_19h', 'L_CSI_ghi_19h',\n",
    "       '80_dwsw', '80_cloud_cover', '56_dwsw', '56_cloud_cover',\n",
    "       '20_dwsw', '20_cloud_cover', '88_dwsw', '88_cloud_cover', 'AVG(R)',\n",
    "       'STD(R)', 'ENT(R)', 'AVG(G)', 'STD(G)', 'ENT(G)', 'AVG(B)', 'STD(B)',\n",
    "       'ENT(B)', 'AVG(RB)', 'STD(RB)', 'ENT(RB)', 'AVG(NRB)', 'STD(NRB)',\n",
    "       'ENT(NRB)']\n",
    "\n",
    "# 2. Calculate the number of NaN values for each feature\n",
    "# We use .isna() instead of == 0\n",
    "nans_per_feature = df[features_to_check].isna().sum()\n",
    "\n",
    "# 3. Filter to get only features that actually have NaN values\n",
    "features_with_nans = nans_per_feature[nans_per_feature > 0]\n",
    "\n",
    "# 4. Report the findings for which features have NaNs\n",
    "if features_with_nans.empty:\n",
    "    print(\"No NaN (missing) values found in any of the specified features.\")\n",
    "else:\n",
    "    print(\"--- Features With NaN Values ---\")\n",
    "    print(\"The following features have NaN values, with the total count for each:\")\n",
    "    # Sort for clearer output\n",
    "    print(features_with_nans.sort_values(ascending=False))\n",
    "    print(\"\\n\" + \"=\"*40 + \"\\n\")\n",
    "\n",
    "    # 5. Analyze the distribution of hours for rows containing NaNs\n",
    "    print(\"--- Distribution of NaN-Value Records by Hour ---\")\n",
    "    \n",
    "    # Create a boolean mask for rows that contain *at least one* NaN\n",
    "    # in the specified columns\n",
    "    rows_with_any_nan = df[features_to_check].isna().any(axis=1)\n",
    "    \n",
    "    if rows_with_any_nan.sum() > 0:\n",
    "        # Get the index for these rows\n",
    "        nan_rows_index = df.index[rows_with_any_nan]\n",
    "        \n",
    "        # Extract the hour from the DatetimeIndex and get the value counts\n",
    "        hour_distribution = nan_rows_index.hour.value_counts().sort_index()\n",
    "        \n",
    "        print(\"Distribution of records (rows) containing at least one NaN, by hour:\")\n",
    "        print(hour_distribution)\n",
    "        \n",
    "        # Optional: Print total number of affected rows\n",
    "        print(f\"\\nTotal number of rows with at least one NaN: {rows_with_any_nan.sum()}\")\n",
    "    else:\n",
    "        # This case shouldn't be hit if features_with_nans was not empty,\n",
    "        # but it's good practice to include.\n",
    "        print(\"No rows found with NaN values (this is unexpected, check logic).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2971adc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2893040/931910069.py:1: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df_interpolated = df.interpolate(method='linear')\n"
     ]
    }
   ],
   "source": [
    "df_interpolated = df.interpolate(method='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74827a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2893040/1599088726.py:1: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df_interpolated2 = df2.interpolate(method='linear')\n"
     ]
    }
   ],
   "source": [
    "df_interpolated2 = df2.interpolate(method='linear')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33763587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Features With NaN Values ---\n",
      "The following features have NaN values, with the total count for each:\n",
      "AVG(R)      1\n",
      "STD(R)      1\n",
      "ENT(R)      1\n",
      "AVG(G)      1\n",
      "STD(G)      1\n",
      "ENT(G)      1\n",
      "AVG(B)      1\n",
      "STD(B)      1\n",
      "ENT(B)      1\n",
      "AVG(RB)     1\n",
      "STD(RB)     1\n",
      "ENT(RB)     1\n",
      "AVG(NRB)    1\n",
      "STD(NRB)    1\n",
      "ENT(NRB)    1\n",
      "dtype: int64\n",
      "\n",
      "========================================\n",
      "\n",
      "--- Distribution of NaN-Value Records by Hour ---\n",
      "Distribution of records (rows) containing at least one NaN, by hour:\n",
      "measurement_time\n",
      "14    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total number of rows with at least one NaN: 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "features_to_check = ['ghi', 'dni', 'solar_zenith', 'GHI_cs', 'DNI_cs', 'CSI_ghi', \n",
    "                                   'CSI_dni',\n",
    "                                   'nam_ghi', 'nam_dni', 'nam_cc', 'B_CSI_ghi_8h', 'V_CSI_ghi_8h', 'L_CSI_ghi_8h', 'B_CSI_ghi_9h',\n",
    "       'V_CSI_ghi_9h', 'L_CSI_ghi_9h', 'B_CSI_ghi_10h', 'V_CSI_ghi_10h',\n",
    "       'L_CSI_ghi_10h', 'B_CSI_ghi_11h', 'V_CSI_ghi_11h', 'L_CSI_ghi_11h',\n",
    "       'B_CSI_ghi_12h', 'V_CSI_ghi_12h', 'L_CSI_ghi_12h', 'B_CSI_ghi_13h',\n",
    "       'V_CSI_ghi_13h', 'L_CSI_ghi_13h', 'B_CSI_ghi_14h', 'V_CSI_ghi_14h',\n",
    "       'L_CSI_ghi_14h', 'B_CSI_ghi_15h', 'V_CSI_ghi_15h', 'L_CSI_ghi_15h',\n",
    "       'B_CSI_ghi_16h', 'V_CSI_ghi_16h', 'L_CSI_ghi_16h', 'B_CSI_ghi_17h',\n",
    "       'V_CSI_ghi_17h', 'L_CSI_ghi_17h', 'B_CSI_ghi_18h', 'V_CSI_ghi_18h',\n",
    "       'L_CSI_ghi_18h', 'B_CSI_ghi_19h', 'V_CSI_ghi_19h', 'L_CSI_ghi_19h',\n",
    "       '80_dwsw', '80_cloud_cover', '56_dwsw', '56_cloud_cover',\n",
    "       '20_dwsw', '20_cloud_cover', '88_dwsw', '88_cloud_cover', 'AVG(R)',\n",
    "       'STD(R)', 'ENT(R)', 'AVG(G)', 'STD(G)', 'ENT(G)', 'AVG(B)', 'STD(B)',\n",
    "       'ENT(B)', 'AVG(RB)', 'STD(RB)', 'ENT(RB)', 'AVG(NRB)', 'STD(NRB)',\n",
    "       'ENT(NRB)']\n",
    "\n",
    "# 2. Calculate the number of NaN values for each feature\n",
    "# We use .isna() instead of == 0\n",
    "nans_per_feature = df_interpolated[features_to_check].isna().sum()\n",
    "\n",
    "# 3. Filter to get only features that actually have NaN values\n",
    "features_with_nans = nans_per_feature[nans_per_feature > 0]\n",
    "\n",
    "# 4. Report the findings for which features have NaNs\n",
    "if features_with_nans.empty:\n",
    "    print(\"No NaN (missing) values found in any of the specified features.\")\n",
    "else:\n",
    "    print(\"--- Features With NaN Values ---\")\n",
    "    print(\"The following features have NaN values, with the total count for each:\")\n",
    "    # Sort for clearer output\n",
    "    print(features_with_nans.sort_values(ascending=False))\n",
    "    print(\"\\n\" + \"=\"*40 + \"\\n\")\n",
    "\n",
    "    # 5. Analyze the distribution of hours for rows containing NaNs\n",
    "    print(\"--- Distribution of NaN-Value Records by Hour ---\")\n",
    "    \n",
    "    # Create a boolean mask for rows that contain *at least one* NaN\n",
    "    # in the specified columns\n",
    "    rows_with_any_nan = df_interpolated[features_to_check].isna().any(axis=1)\n",
    "    \n",
    "    if rows_with_any_nan.sum() > 0:\n",
    "        # Get the index for these rows\n",
    "        nan_rows_index = df_interpolated.index[rows_with_any_nan]\n",
    "        \n",
    "        # Extract the hour from the DatetimeIndex and get the value counts\n",
    "        hour_distribution = nan_rows_index.hour.value_counts().sort_index()\n",
    "        \n",
    "        print(\"Distribution of records (rows) containing at least one NaN, by hour:\")\n",
    "        print(hour_distribution)\n",
    "        \n",
    "        # Optional: Print total number of affected rows\n",
    "        print(f\"\\nTotal number of rows with at least one NaN: {rows_with_any_nan.sum()}\")\n",
    "    else:\n",
    "        # This case shouldn't be hit if features_with_nans was not empty,\n",
    "        # but it's good practice to include.\n",
    "        print(\"No rows found with NaN values (this is unexpected, check logic).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a903c55a",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e150b7f",
   "metadata": {},
   "source": [
    "### Data Preparation for Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a075dbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4c99fbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ghi', 'dni', 'PST_Time_meas', 'solar_zenith', 'time_gap_hours',\n",
       "       'time_gap_norm', 'day_boundary_flag', 'hour_progression',\n",
       "       'absolute_hour', 'GHI_cs', 'DNI_cs', 'CSI_ghi', 'CSI_dni',\n",
       "       'season_flag', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos',\n",
       "       'nam_ghi', 'nam_dni', 'nam_cc', 'PST_Time_nam', 'nam_target_time',\n",
       "       'B_CSI_ghi_8h', 'V_CSI_ghi_8h', 'L_CSI_ghi_8h', 'B_CSI_ghi_9h',\n",
       "       'V_CSI_ghi_9h', 'L_CSI_ghi_9h', 'B_CSI_ghi_10h', 'V_CSI_ghi_10h',\n",
       "       'L_CSI_ghi_10h', 'B_CSI_ghi_11h', 'V_CSI_ghi_11h', 'L_CSI_ghi_11h',\n",
       "       'B_CSI_ghi_12h', 'V_CSI_ghi_12h', 'L_CSI_ghi_12h', 'B_CSI_ghi_13h',\n",
       "       'V_CSI_ghi_13h', 'L_CSI_ghi_13h', 'B_CSI_ghi_14h', 'V_CSI_ghi_14h',\n",
       "       'L_CSI_ghi_14h', 'B_CSI_ghi_15h', 'V_CSI_ghi_15h', 'L_CSI_ghi_15h',\n",
       "       'B_CSI_ghi_16h', 'V_CSI_ghi_16h', 'L_CSI_ghi_16h', 'B_CSI_ghi_17h',\n",
       "       'V_CSI_ghi_17h', 'L_CSI_ghi_17h', 'B_CSI_ghi_18h', 'V_CSI_ghi_18h',\n",
       "       'L_CSI_ghi_18h', 'B_CSI_ghi_19h', 'V_CSI_ghi_19h', 'L_CSI_ghi_19h',\n",
       "       '80_dwsw', 'valtime', '80_cloud_cover', '56_dwsw', '56_cloud_cover',\n",
       "       '20_dwsw', '20_cloud_cover', '88_dwsw', '88_cloud_cover', 'AVG(R)',\n",
       "       'STD(R)', 'ENT(R)', 'AVG(G)', 'STD(G)', 'ENT(G)', 'AVG(B)', 'STD(B)',\n",
       "       'ENT(B)', 'AVG(RB)', 'STD(RB)', 'ENT(RB)', 'AVG(NRB)', 'STD(NRB)',\n",
       "       'ENT(NRB)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_interpolated2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8fe3b436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ghi', 'dni', 'PST_Time_meas', 'solar_zenith', 'time_gap_hours',\n",
       "       'time_gap_norm', 'day_boundary_flag', 'hour_progression',\n",
       "       'absolute_hour', 'GHI_cs', 'DNI_cs', 'CSI_ghi', 'CSI_dni',\n",
       "       'season_flag', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos',\n",
       "       'nam_ghi', 'nam_dni', 'nam_cc', 'PST_Time_nam', 'nam_target_time',\n",
       "       'B_CSI_ghi_8h', 'V_CSI_ghi_8h', 'L_CSI_ghi_8h', 'B_CSI_ghi_9h',\n",
       "       'V_CSI_ghi_9h', 'L_CSI_ghi_9h', 'B_CSI_ghi_10h', 'V_CSI_ghi_10h',\n",
       "       'L_CSI_ghi_10h', 'B_CSI_ghi_11h', 'V_CSI_ghi_11h', 'L_CSI_ghi_11h',\n",
       "       'B_CSI_ghi_12h', 'V_CSI_ghi_12h', 'L_CSI_ghi_12h', 'B_CSI_ghi_13h',\n",
       "       'V_CSI_ghi_13h', 'L_CSI_ghi_13h', 'B_CSI_ghi_14h', 'V_CSI_ghi_14h',\n",
       "       'L_CSI_ghi_14h', 'B_CSI_ghi_15h', 'V_CSI_ghi_15h', 'L_CSI_ghi_15h',\n",
       "       'B_CSI_ghi_16h', 'V_CSI_ghi_16h', 'L_CSI_ghi_16h', 'B_CSI_ghi_17h',\n",
       "       'V_CSI_ghi_17h', 'L_CSI_ghi_17h', 'B_CSI_ghi_18h', 'V_CSI_ghi_18h',\n",
       "       'L_CSI_ghi_18h', 'B_CSI_ghi_19h', 'V_CSI_ghi_19h', 'L_CSI_ghi_19h',\n",
       "       '80_dwsw', 'valtime', '80_cloud_cover', '56_dwsw', '56_cloud_cover',\n",
       "       '20_dwsw', '20_cloud_cover', '88_dwsw', '88_cloud_cover', 'AVG(R)',\n",
       "       'STD(R)', 'ENT(R)', 'AVG(G)', 'STD(G)', 'ENT(G)', 'AVG(B)', 'STD(B)',\n",
       "       'ENT(B)', 'AVG(RB)', 'STD(RB)', 'ENT(RB)', 'AVG(NRB)', 'STD(NRB)',\n",
       "       'ENT(NRB)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_interpolated.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b271582",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_phase1 = df_interpolated[['solar_zenith', 'CSI_ghi','time_gap_hours',\n",
    "       'time_gap_norm', 'day_boundary_flag', 'hour_progression',\n",
    "       'absolute_hour', 'PST_Time_meas',\n",
    "       'season_flag', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos']]\n",
    "\n",
    "\n",
    "df_phase2 = df_interpolated[['solar_zenith', 'CSI_ghi','time_gap_hours',\n",
    "       'time_gap_norm', 'day_boundary_flag', 'hour_progression',\n",
    "       'absolute_hour',\n",
    "       'season_flag', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos', 'B_CSI_ghi_8h', 'V_CSI_ghi_8h', 'L_CSI_ghi_8h', 'B_CSI_ghi_9h',\n",
    "       'V_CSI_ghi_9h', 'L_CSI_ghi_9h', 'B_CSI_ghi_10h', 'V_CSI_ghi_10h',\n",
    "       'L_CSI_ghi_10h', 'B_CSI_ghi_11h', 'V_CSI_ghi_11h', 'L_CSI_ghi_11h',\n",
    "       'B_CSI_ghi_12h', 'V_CSI_ghi_12h', 'L_CSI_ghi_12h', 'B_CSI_ghi_13h',\n",
    "       'V_CSI_ghi_13h', 'L_CSI_ghi_13h', 'B_CSI_ghi_14h', 'V_CSI_ghi_14h',\n",
    "       'L_CSI_ghi_14h', 'B_CSI_ghi_15h', 'V_CSI_ghi_15h', 'L_CSI_ghi_15h',\n",
    "       'B_CSI_ghi_16h', 'V_CSI_ghi_16h', 'L_CSI_ghi_16h', 'B_CSI_ghi_17h',\n",
    "       'V_CSI_ghi_17h', 'L_CSI_ghi_17h', 'B_CSI_ghi_18h', 'V_CSI_ghi_18h',\n",
    "       'L_CSI_ghi_18h', 'B_CSI_ghi_19h', 'V_CSI_ghi_19h', 'L_CSI_ghi_19h']]\n",
    "\n",
    "df_phase3 = df_interpolated[['solar_zenith', 'CSI_ghi','time_gap_hours',\n",
    "       'time_gap_norm', 'day_boundary_flag', 'hour_progression',\n",
    "       'absolute_hour',\n",
    "       'season_flag', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos', 'B_CSI_ghi_8h', 'V_CSI_ghi_8h', 'L_CSI_ghi_8h', 'B_CSI_ghi_9h',\n",
    "       'V_CSI_ghi_9h', 'L_CSI_ghi_9h', 'B_CSI_ghi_10h', 'V_CSI_ghi_10h',\n",
    "       'L_CSI_ghi_10h', 'B_CSI_ghi_11h', 'V_CSI_ghi_11h', 'L_CSI_ghi_11h',\n",
    "       'B_CSI_ghi_12h', 'V_CSI_ghi_12h', 'L_CSI_ghi_12h', 'B_CSI_ghi_13h',\n",
    "       'V_CSI_ghi_13h', 'L_CSI_ghi_13h', 'B_CSI_ghi_14h', 'V_CSI_ghi_14h',\n",
    "       'L_CSI_ghi_14h', 'B_CSI_ghi_15h', 'V_CSI_ghi_15h', 'L_CSI_ghi_15h',\n",
    "       'B_CSI_ghi_16h', 'V_CSI_ghi_16h', 'L_CSI_ghi_16h', 'B_CSI_ghi_17h',\n",
    "       'V_CSI_ghi_17h', 'L_CSI_ghi_17h', 'B_CSI_ghi_18h', 'V_CSI_ghi_18h',\n",
    "       'L_CSI_ghi_18h', 'B_CSI_ghi_19h', 'V_CSI_ghi_19h', 'L_CSI_ghi_19h',\n",
    "       'nam_ghi', 'nam_cc']]\n",
    "\n",
    "df_phase4 = df_interpolated[['solar_zenith', 'CSI_ghi','time_gap_hours',\n",
    "       'time_gap_norm', 'day_boundary_flag', 'hour_progression',\n",
    "       'absolute_hour',\n",
    "       'season_flag', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos', 'B_CSI_ghi_8h', 'V_CSI_ghi_8h', 'L_CSI_ghi_8h', 'B_CSI_ghi_9h',\n",
    "       'V_CSI_ghi_9h', 'L_CSI_ghi_9h', 'B_CSI_ghi_10h', 'V_CSI_ghi_10h',\n",
    "       'L_CSI_ghi_10h', 'B_CSI_ghi_11h', 'V_CSI_ghi_11h', 'L_CSI_ghi_11h',\n",
    "       'B_CSI_ghi_12h', 'V_CSI_ghi_12h', 'L_CSI_ghi_12h', 'B_CSI_ghi_13h',\n",
    "       'V_CSI_ghi_13h', 'L_CSI_ghi_13h', 'B_CSI_ghi_14h', 'V_CSI_ghi_14h',\n",
    "       'L_CSI_ghi_14h', 'B_CSI_ghi_15h', 'V_CSI_ghi_15h', 'L_CSI_ghi_15h',\n",
    "       'B_CSI_ghi_16h', 'V_CSI_ghi_16h', 'L_CSI_ghi_16h', 'B_CSI_ghi_17h',\n",
    "       'V_CSI_ghi_17h', 'L_CSI_ghi_17h', 'B_CSI_ghi_18h', 'V_CSI_ghi_18h',\n",
    "       'L_CSI_ghi_18h', 'B_CSI_ghi_19h', 'V_CSI_ghi_19h', 'L_CSI_ghi_19h',\n",
    "       'nam_ghi', 'nam_cc', '80_dwsw', 'valtime', '80_cloud_cover', '56_dwsw', '56_cloud_cover',\n",
    "       '20_dwsw', '20_cloud_cover', '88_dwsw', '88_cloud_cover']]\n",
    "\n",
    "df_phase5 = df_interpolated[['solar_zenith', 'CSI_ghi','time_gap_hours',\n",
    "       'time_gap_norm', 'day_boundary_flag', 'hour_progression',\n",
    "       'absolute_hour',\n",
    "       'season_flag', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos', 'B_CSI_ghi_8h', 'V_CSI_ghi_8h', 'L_CSI_ghi_8h', 'B_CSI_ghi_9h',\n",
    "       'V_CSI_ghi_9h', 'L_CSI_ghi_9h', 'B_CSI_ghi_10h', 'V_CSI_ghi_10h',\n",
    "       'L_CSI_ghi_10h', 'B_CSI_ghi_11h', 'V_CSI_ghi_11h', 'L_CSI_ghi_11h',\n",
    "       'B_CSI_ghi_12h', 'V_CSI_ghi_12h', 'L_CSI_ghi_12h', 'B_CSI_ghi_13h',\n",
    "       'V_CSI_ghi_13h', 'L_CSI_ghi_13h', 'B_CSI_ghi_14h', 'V_CSI_ghi_14h',\n",
    "       'L_CSI_ghi_14h', 'B_CSI_ghi_15h', 'V_CSI_ghi_15h', 'L_CSI_ghi_15h',\n",
    "       'B_CSI_ghi_16h', 'V_CSI_ghi_16h', 'L_CSI_ghi_16h', 'B_CSI_ghi_17h',\n",
    "       'V_CSI_ghi_17h', 'L_CSI_ghi_17h', 'B_CSI_ghi_18h', 'V_CSI_ghi_18h',\n",
    "       'L_CSI_ghi_18h', 'B_CSI_ghi_19h', 'V_CSI_ghi_19h', 'L_CSI_ghi_19h',\n",
    "       'nam_ghi', 'nam_cc','80_cloud_cover',  '56_cloud_cover',\n",
    "        '20_cloud_cover', '88_cloud_cover']]\n",
    "\n",
    "df_phase6 = df_interpolated[['solar_zenith', 'CSI_ghi','time_gap_hours',\n",
    "       'time_gap_norm', 'day_boundary_flag', 'hour_progression',\n",
    "       'absolute_hour',\n",
    "       'season_flag', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos', 'B_CSI_ghi_8h', 'V_CSI_ghi_8h', 'L_CSI_ghi_8h', 'B_CSI_ghi_9h',\n",
    "       'V_CSI_ghi_9h', 'L_CSI_ghi_9h', 'B_CSI_ghi_10h', 'V_CSI_ghi_10h',\n",
    "       'L_CSI_ghi_10h', 'B_CSI_ghi_11h', 'V_CSI_ghi_11h', 'L_CSI_ghi_11h',\n",
    "       'B_CSI_ghi_12h', 'V_CSI_ghi_12h', 'L_CSI_ghi_12h', 'B_CSI_ghi_13h',\n",
    "       'V_CSI_ghi_13h', 'L_CSI_ghi_13h', 'B_CSI_ghi_14h', 'V_CSI_ghi_14h',\n",
    "       'L_CSI_ghi_14h', 'B_CSI_ghi_15h', 'V_CSI_ghi_15h', 'L_CSI_ghi_15h',\n",
    "       'B_CSI_ghi_16h', 'V_CSI_ghi_16h', 'L_CSI_ghi_16h', 'B_CSI_ghi_17h',\n",
    "       'V_CSI_ghi_17h', 'L_CSI_ghi_17h', 'B_CSI_ghi_18h', 'V_CSI_ghi_18h',\n",
    "       'L_CSI_ghi_18h', 'B_CSI_ghi_19h', 'V_CSI_ghi_19h', 'L_CSI_ghi_19h',\n",
    "       'nam_ghi', 'nam_cc', '80_dwsw', 'valtime', '80_cloud_cover', '56_dwsw', '56_cloud_cover',\n",
    "       '20_dwsw', '20_cloud_cover', '88_dwsw', '88_cloud_cover'\n",
    "      ]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abe0f33",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe8b05e",
   "metadata": {},
   "source": [
    "## Testing and Validating the Data Preparation for traning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fac779e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocessing import build_model_arrays, to_fixedgrid_multiindex\n",
    "\n",
    "TARGET_COL = \"CSI_ghi\"\n",
    "timestamp_col=\"measurement_time\"\n",
    "\n",
    "df_phase1 = df_interpolated[['solar_zenith', 'CSI_ghi',\n",
    "       'absolute_hour',\n",
    "       'season_flag' ]]\n",
    "\n",
    "\n",
    "feature_cols = [c for c in df_phase1.columns.tolist() if c != TARGET_COL]\n",
    "# fixed_df = to_fixedgrid_multiindex(df_phase1, timestamp_col=\"measurement_time\", expected_T=None) \n",
    "test_df = df_phase1.copy(deep=True)\n",
    "\n",
    "test_df.reset_index(inplace=True)\n",
    "test_df[\"UTC_date\"] = test_df[timestamp_col].dt.tz_convert(\"UTC\").dt.date\n",
    "test_df[\"PST_date\"] = (\n",
    "    test_df[timestamp_col]\n",
    "    .dt.tz_convert(\"US/Pacific\")  # Converts UTC to Pacific Time (handles DST automatically)\n",
    "    .dt.date\n",
    ")\n",
    "test_df[\"bin_id\"] = test_df.groupby(\"PST_date\").cumcount()\n",
    "test_df = test_df.set_index([\"PST_date\", \"bin_id\"]).sort_index()\n",
    "\n",
    "\n",
    "# ph1_X, ph1_Y, ph1_labels_list = build_model_arrays(\n",
    "#         test_df,\n",
    "#         feature_cols=feature_cols,  # <-- This is correct!\n",
    "#         target_col=TARGET_COL,\n",
    "#         history_days=2,\n",
    "#         horizon_days=1,\n",
    "#     )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "434725dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>measurement_time</th>\n",
       "      <th>solar_zenith</th>\n",
       "      <th>CSI_ghi</th>\n",
       "      <th>absolute_hour</th>\n",
       "      <th>season_flag</th>\n",
       "      <th>UTC_date</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PST_date</th>\n",
       "      <th>bin_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2014-01-03</th>\n",
       "      <th>0</th>\n",
       "      <td>2014-01-03 14:00:00+00:00</td>\n",
       "      <td>100.276046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.491667</td>\n",
       "      <td>2</td>\n",
       "      <td>2014-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-01-03 15:00:00+00:00</td>\n",
       "      <td>89.724162</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>25.491667</td>\n",
       "      <td>2</td>\n",
       "      <td>2014-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-01-03 16:00:00+00:00</td>\n",
       "      <td>80.146724</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>26.491667</td>\n",
       "      <td>2</td>\n",
       "      <td>2014-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-01-03 17:00:00+00:00</td>\n",
       "      <td>71.988814</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>27.491667</td>\n",
       "      <td>2</td>\n",
       "      <td>2014-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-01-03 18:00:00+00:00</td>\n",
       "      <td>65.816158</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>28.491667</td>\n",
       "      <td>2</td>\n",
       "      <td>2014-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2016-12-30</th>\n",
       "      <th>5</th>\n",
       "      <td>2016-12-30 19:00:00+00:00</td>\n",
       "      <td>62.502377</td>\n",
       "      <td>1.051822</td>\n",
       "      <td>26237.491667</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-12-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2016-12-30 20:00:00+00:00</td>\n",
       "      <td>62.079248</td>\n",
       "      <td>1.001250</td>\n",
       "      <td>26238.491667</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-12-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2016-12-30 21:00:00+00:00</td>\n",
       "      <td>64.765899</td>\n",
       "      <td>0.961864</td>\n",
       "      <td>26239.491667</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-12-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2016-12-30 22:00:00+00:00</td>\n",
       "      <td>70.203098</td>\n",
       "      <td>0.896756</td>\n",
       "      <td>26240.491667</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-12-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2016-12-30 23:00:00+00:00</td>\n",
       "      <td>77.805433</td>\n",
       "      <td>0.804137</td>\n",
       "      <td>26241.491667</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-12-30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11560 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           measurement_time  solar_zenith   CSI_ghi  \\\n",
       "PST_date   bin_id                                                     \n",
       "2014-01-03 0      2014-01-03 14:00:00+00:00    100.276046  0.000000   \n",
       "           1      2014-01-03 15:00:00+00:00     89.724162  1.200000   \n",
       "           2      2014-01-03 16:00:00+00:00     80.146724  1.200000   \n",
       "           3      2014-01-03 17:00:00+00:00     71.988814  1.200000   \n",
       "           4      2014-01-03 18:00:00+00:00     65.816158  1.200000   \n",
       "...                                     ...           ...       ...   \n",
       "2016-12-30 5      2016-12-30 19:00:00+00:00     62.502377  1.051822   \n",
       "           6      2016-12-30 20:00:00+00:00     62.079248  1.001250   \n",
       "           7      2016-12-30 21:00:00+00:00     64.765899  0.961864   \n",
       "           8      2016-12-30 22:00:00+00:00     70.203098  0.896756   \n",
       "           9      2016-12-30 23:00:00+00:00     77.805433  0.804137   \n",
       "\n",
       "                   absolute_hour  season_flag    UTC_date  \n",
       "PST_date   bin_id                                          \n",
       "2014-01-03 0           24.491667            2  2014-01-03  \n",
       "           1           25.491667            2  2014-01-03  \n",
       "           2           26.491667            2  2014-01-03  \n",
       "           3           27.491667            2  2014-01-03  \n",
       "           4           28.491667            2  2014-01-03  \n",
       "...                          ...          ...         ...  \n",
       "2016-12-30 5        26237.491667            2  2016-12-30  \n",
       "           6        26238.491667            2  2016-12-30  \n",
       "           7        26239.491667            2  2016-12-30  \n",
       "           8        26240.491667            2  2016-12-30  \n",
       "           9        26241.491667            2  2016-12-30  \n",
       "\n",
       "[11560 rows x 6 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5cf57b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = list(test_df.index.get_level_values(\"PST_date\").unique())\n",
    "K = int(test_df.index.get_level_values(\"bin_id\").max()) + 1\n",
    "F = len(feature_cols)\n",
    "X = np.full((len(dates), K, F), np.nan, dtype=float)\n",
    "\n",
    "for j, col in enumerate(feature_cols):\n",
    "    if col not in test_df.columns:\n",
    "        continue\n",
    "\n",
    "    \n",
    "\n",
    "    x_val = test_df['solar_zenith'].groupby(level=[\"PST_date\", \"bin_id\"]).first().unstack(\"bin_id\").reindex(index=dates, columns=range(K))\n",
    "    X[:, :, j] = x_val.values \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aab31f79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1051, (1051, 11, 3))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dates), X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "822985c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Tensor Building\n",
    "f_cols = ['CSI_ghi']\n",
    "\n",
    "dates = list(test_df.index.get_level_values(\"PST_date\").unique())\n",
    "K = int(test_df.index.get_level_values(\"bin_id\").max()) + 1\n",
    "F = len(f_cols)\n",
    "Y = np.full((len(dates), K, F), np.nan, dtype=float)\n",
    "\n",
    "for j, col in enumerate(f_cols):\n",
    "    if col not in test_df.columns:\n",
    "        continue\n",
    "\n",
    "    \n",
    "\n",
    "    y_val = test_df['solar_zenith'].groupby(level=[\"PST_date\", \"bin_id\"]).first().unstack(\"bin_id\").reindex(index=dates, columns=range(K))\n",
    "    Y[:, :, j] = x_val.values \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1de77b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = Y[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6470a9e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1051, (1051, 11))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dates), Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "71486748",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_days = 3\n",
    "horizon_days = 1\n",
    "num_days = X.shape[0]   # Avaible dates\n",
    "K = X.shape[1]          # Hours records\n",
    "F = X.shape[2]          # Avaible Features\n",
    "\n",
    "samples = max(0, num_days - history_days - horizon_days + 1)\n",
    "X_list, Y_list, labels = [], [], []\n",
    "for i in range(samples):\n",
    "    past = slice(i, i+history_days)\n",
    "    fut  = slice(i+history_days, i+history_days+horizon_days)\n",
    "    X_i = X[past,:,:]\n",
    "    Y_i = Y[fut,:]\n",
    "    X_list.append(X_i)\n",
    "    Y_list.append(Y_i)\n",
    "    labels.append(dates[i+history_days+horizon_days-1])\n",
    "\n",
    "\n",
    " # converts the list of individual samples into the final, single, massive tensor required by the deep learning model.\n",
    "X_tensor = np.stack(X_list, axis=0) if X_list else np.empty((0,history_days,K,F))\n",
    "Y_tensor = np.stack(Y_list, axis=0) if Y_list else np.empty((0,horizon_days,K))\n",
    "# X: A 4D tensor of shape (samples, history_days, K, F)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0015ac7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1048, 3, 11, 3),\n",
       " (1048, 1, 11),\n",
       " 1048,\n",
       " [datetime.date(2014, 2, 11),\n",
       "  datetime.date(2014, 2, 12),\n",
       "  datetime.date(2014, 2, 19),\n",
       "  datetime.date(2014, 2, 20),\n",
       "  datetime.date(2014, 2, 21),\n",
       "  datetime.date(2014, 2, 22)])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tensor.shape, Y_tensor.shape, len(labels), labels[30:36]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e32bfb63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function list.index(value, start=0, stop=9223372036854775807, /)>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5725a6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import DataManager\n",
    "\n",
    "data_manager = DataManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb6a100",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.utils:Saved arrays to data/Test_dataFile_prefix_*.npy\n",
      "INFO:src.utils:X shape: (1048, 3, 11, 3), Y shape: (1048, 1, 11)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_manager.save_arrays(\n",
    "    X_tensor, Y_tensor,\n",
    "    pd.DataFrame(\n",
    "        index=pd.to_datetime(labels, utc=True)),\n",
    "    filename_prefix='Test_dataFile_prefix',  # <-- You are saving as 'phas2_data'\n",
    "    feature_cols=feature_cols,\n",
    "    target_col=TARGET_COL,\n",
    "    metadata={\n",
    "        \"input_csv\":       \"data/processed/df_1h_lag_BLV_spatial_images.csv\",\n",
    "        \"timestamp_col\": \"measurement_time\",\n",
    "        \"feature_set\": feature_cols,\n",
    "        \"history_days\": 7,\n",
    "        \"horizon_days\": 1,\n",
    "        \"k_bins\": None,\n",
    "    }\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "faab0a12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['solar_zenith', 'absolute_hour', 'season_flag'],\n",
       " 'CSI_ghi',\n",
       " <function list.index(value, start=0, stop=9223372036854775807, /)>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_cols, TARGET_COL, labels.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b491657b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'target_time'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3811\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mpandas/_libs/index.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/index.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'target_time'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 50\u001b[0m\n\u001b[1;32m     42\u001b[0m site_config \u001b[38;5;241m=\u001b[39m SiteLocation(\n\u001b[1;32m     43\u001b[0m                 latitude\u001b[38;5;241m=\u001b[39mLSTM_CONFIG\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msite_latitude\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     44\u001b[0m                 longitude\u001b[38;5;241m=\u001b[39mLSTM_CONFIG\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msite_longitude\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     45\u001b[0m                 altitude\u001b[38;5;241m=\u001b[39mLSTM_CONFIG\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msite_altitude\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     46\u001b[0m                 timezone\u001b[38;5;241m=\u001b[39mLSTM_CONFIG\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msite_timezone\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     47\u001b[0m             )\n\u001b[1;32m     48\u001b[0m ts_col, csv_path, site_config\n\u001b[0;32m---> 50\u001b[0m reference_df \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_reference_fixedgrid_frame\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m                        \u001b[49m\u001b[43msite_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mtimestamp_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mts_col\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m reference_df\n",
      "File \u001b[0;32m~/App_v02/src/evaluation_utils.py:202\u001b[0m, in \u001b[0;36mbuild_reference_fixedgrid_frame\u001b[0;34m(csv_path, site, timestamp_col)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ref_df\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;66;03m# If 'target_time' isn't a column, the original timestamps are the index\u001b[39;00m\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;66;03m# We need to get the timestamps *from* the index created by to_fixedgrid_multiindex\u001b[39;00m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# This is complex. A simpler way is to re-add target_time from the original index.\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     base_df\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_time\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 202\u001b[0m     ref_df \u001b[38;5;241m=\u001b[39m ref_df\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mbase_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtarget_time\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ref_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_time\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39misnull()\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m    204\u001b[0m          logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not rejoin target_time in fixed-grid reference.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/pandas/core/frame.py:4113\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4113\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4115\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/pandas/core/indexes/base.py:3819\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3815\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3816\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3817\u001b[0m     ):\n\u001b[1;32m   3818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3820\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3821\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3822\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3823\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'target_time'"
     ]
    }
   ],
   "source": [
    "# --- Block 4: Configure and Run ---\n",
    "LSTM_CONFIG = {\n",
    "    \"experiment_name\": \"TESTEXP_UniLSTM_p1_exp01_essentialF\",\n",
    "    \"model_type\": \"LSTM\",\n",
    "    \"model_config\": {\n",
    "        \"hidden_size\": 64,\n",
    "        \"num_layers\": 2,\n",
    "        \"dropout\": 0.35,\n",
    "        \"bidirectional\": True,\n",
    "    },\n",
    "    \"data_prefix\": \"phas1_data\",  # <-- FIX 2: Match the filename_prefix\n",
    "    \"splits_file\": \"exp-004/exp-004rolling_origin_splits.json\",\n",
    "    \"feature_cols\": feature_cols,\n",
    "    \"feature_selection\": feature_cols, # <-- This is correct!\n",
    "    \"target_col\": TARGET_COL,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_epochs\": 50,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"loss_function\": \"Huber\",  # <-- This setting needs the fix below\n",
    "    \"early_stopping_patience\": 20,\n",
    "    \"max_folds\": 3,\n",
    "\n",
    "\n",
    "\n",
    "}\n",
    "metadata={\n",
    "        \"input_csv\": \"data/processed/df_1h_lag_BLV_spatial_images.csv\",\n",
    "        \"timestamp_col\": \"measurement_time\",\n",
    "        \"feature_set\": feature_cols,\n",
    "        \"history_days\": 7,\n",
    "        \"horizon_days\": 1,\n",
    "        \"k_bins\": None,\n",
    "    }\n",
    "\n",
    "expected_features = LSTM_CONFIG.get(\"feature_cols\")\n",
    "\n",
    "from src.evaluation_utils import build_reference_fixedgrid_frame, SiteLocation\n",
    "\n",
    "ts_col = metadata.get(\"timestamp_col\", \"measurement_time\")\n",
    "csv_path = metadata.get(\"input_csv\")\n",
    "\n",
    "site_config = SiteLocation(\n",
    "                latitude=LSTM_CONFIG.get('site_latitude'),\n",
    "                longitude=LSTM_CONFIG.get('site_longitude'),\n",
    "                altitude=LSTM_CONFIG.get('site_altitude'),\n",
    "                timezone=LSTM_CONFIG.get('site_timezone'),\n",
    "            )\n",
    "ts_col, csv_path, site_config\n",
    "\n",
    "reference_df = build_reference_fixedgrid_frame(\n",
    "                        csv_path,\n",
    "                        site_config,\n",
    "                        timestamp_col=ts_col\n",
    "                    )\n",
    "reference_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1f69ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66085cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pipeline import SolarForecastingPipeline\n",
    "\n",
    "pipeline = SolarForecastingPipeline(LSTM_CONFIG)\n",
    "_, summary = pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f128d9",
   "metadata": {},
   "source": [
    "### End of Data Preparation Validation \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a8d1a6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.utils:Saved arrays to data/phas1_data_*.npy\n",
      "INFO:src.utils:X shape: (1055, 7, 11, 11), Y shape: (1055, 1, 11)\n",
      "INFO:src.pipeline:Loading data...\n",
      "INFO:src.utils:Loaded arrays from data/phas1_data_*.npy\n",
      "INFO:src.utils:X shape: (1055, 7, 11, 11), Y shape: (1055, 1, 11)\n",
      "INFO:src.pipeline:Building FIXED-GRID reference frame (K_bins is None)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 2: Building model arrays (X, Y) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:src.pipeline:Failed to construct reference dataframe for NAM comparison: 'target_time'\n",
      "INFO:src.utils:Loaded 35 folds from exp-004/exp-004rolling_origin_splits.json\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 1 ===\n",
      "INFO:src.pipeline:Train samples: 21, Val samples: 18\n",
      "INFO:src.pipeline:Model parameters: 248,715\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.076680, Val Loss: 0.104906, Train MAE: 0.300700, Val MAE: 0.391978, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.063023, Val Loss: 0.103829, Train MAE: 0.274112, Val MAE: 0.386069, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.049382, Val Loss: 0.104260, Train MAE: 0.223215, Val MAE: 0.386043, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.043392, Val Loss: 0.097369, Train MAE: 0.207388, Val MAE: 0.379170, LR: 0.001000\n",
      "INFO:src.engine:Epoch [50/50] - Train Loss: 0.038784, Val Loss: 0.095513, Train MAE: 0.200005, Val MAE: 0.363086, LR: 0.001000\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 1.\n",
      "INFO:src.pipeline:Fold 1 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.191598\n",
      "INFO:src.pipeline:  rmse: 0.437719\n",
      "INFO:src.pipeline:  mae: 0.363086\n",
      "INFO:src.pipeline:  r2: 0.124162\n",
      "INFO:src.pipeline:  mape: 86.415176\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 2 ===\n",
      "INFO:src.pipeline:Train samples: 40, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 248,715\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.087375, Val Loss: 0.060612, Train MAE: 0.355535, Val MAE: 0.314150, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.068541, Val Loss: 0.071322, Train MAE: 0.298738, Val MAE: 0.339383, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.068950, Val Loss: 0.064620, Train MAE: 0.303822, Val MAE: 0.326164, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 31\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 2.\n",
      "INFO:src.pipeline:Fold 2 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.130217\n",
      "INFO:src.pipeline:  rmse: 0.360856\n",
      "INFO:src.pipeline:  mae: 0.327334\n",
      "INFO:src.pipeline:  r2: -0.102162\n",
      "INFO:src.pipeline:  mape: 68.061508\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 3 ===\n",
      "INFO:src.pipeline:Train samples: 71, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 248,715\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.067696, Val Loss: 0.029759, Train MAE: 0.310352, Val MAE: 0.206922, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.054547, Val Loss: 0.026613, Train MAE: 0.269690, Val MAE: 0.188086, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.050097, Val Loss: 0.025073, Train MAE: 0.251569, Val MAE: 0.182180, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.046947, Val Loss: 0.037739, Train MAE: 0.232884, Val MAE: 0.241025, LR: 0.001000\n",
      "INFO:src.engine:Early stopping at epoch 49\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 3.\n",
      "INFO:src.pipeline:Fold 3 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.146604\n",
      "INFO:src.pipeline:  rmse: 0.382890\n",
      "INFO:src.pipeline:  mae: 0.344676\n",
      "INFO:src.pipeline:  r2: -0.766771\n",
      "INFO:src.pipeline:  mape: 44.209103\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 4 ===\n",
      "INFO:src.pipeline:Train samples: 101, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 248,715\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.049396, Val Loss: 0.014425, Train MAE: 0.244883, Val MAE: 0.146736, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.047833, Val Loss: 0.015574, Train MAE: 0.248095, Val MAE: 0.150324, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.039532, Val Loss: 0.015123, Train MAE: 0.212692, Val MAE: 0.108292, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.036966, Val Loss: 0.015419, Train MAE: 0.217667, Val MAE: 0.144642, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 41\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 4.\n",
      "INFO:src.pipeline:Fold 4 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.029147\n",
      "INFO:src.pipeline:  rmse: 0.170726\n",
      "INFO:src.pipeline:  mae: 0.136480\n",
      "INFO:src.pipeline:  r2: 0.374675\n",
      "INFO:src.pipeline:  mape: 18.061934\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 5 ===\n",
      "INFO:src.pipeline:Train samples: 132, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 248,715\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.046485, Val Loss: 0.021402, Train MAE: 0.226365, Val MAE: 0.196333, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.040306, Val Loss: 0.030027, Train MAE: 0.215653, Val MAE: 0.230455, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 21\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 5.\n",
      "INFO:src.pipeline:Fold 5 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.058488\n",
      "INFO:src.pipeline:  rmse: 0.241842\n",
      "INFO:src.pipeline:  mae: 0.228994\n",
      "INFO:src.pipeline:  r2: -1.191341\n",
      "INFO:src.pipeline:  mape: 22.751516\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 6 ===\n",
      "INFO:src.pipeline:Train samples: 162, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 248,715\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.035428, Val Loss: 0.012858, Train MAE: 0.190587, Val MAE: 0.089513, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.044122, Val Loss: 0.012188, Train MAE: 0.215584, Val MAE: 0.095304, LR: 0.000500\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.044102, Val Loss: 0.012273, Train MAE: 0.205717, Val MAE: 0.083632, LR: 0.000500\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.028326, Val Loss: 0.012953, Train MAE: 0.162122, Val MAE: 0.079529, LR: 0.000250\n",
      "INFO:src.engine:Early stopping at epoch 44\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 6.\n",
      "INFO:src.pipeline:Fold 6 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.025226\n",
      "INFO:src.pipeline:  rmse: 0.158827\n",
      "INFO:src.pipeline:  mae: 0.079994\n",
      "INFO:src.pipeline:  r2: 0.367255\n",
      "INFO:src.pipeline:  mape: 15.715520\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 7 ===\n",
      "INFO:src.pipeline:Train samples: 193, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 248,715\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.039852, Val Loss: 0.022915, Train MAE: 0.209039, Val MAE: 0.124089, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.032716, Val Loss: 0.021918, Train MAE: 0.183529, Val MAE: 0.122728, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 22\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 7.\n",
      "INFO:src.pipeline:Fold 7 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.044003\n",
      "INFO:src.pipeline:  rmse: 0.209769\n",
      "INFO:src.pipeline:  mae: 0.121154\n",
      "INFO:src.pipeline:  r2: 0.380402\n",
      "INFO:src.pipeline:  mape: 33.314575\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 8 ===\n",
      "INFO:src.pipeline:Train samples: 224, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 248,715\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.031861, Val Loss: 0.025191, Train MAE: 0.177582, Val MAE: 0.148665, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.028677, Val Loss: 0.025205, Train MAE: 0.167349, Val MAE: 0.145646, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.024632, Val Loss: 0.026829, Train MAE: 0.156029, Val MAE: 0.151029, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 30\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 8.\n",
      "INFO:src.pipeline:Fold 8 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.053668\n",
      "INFO:src.pipeline:  rmse: 0.231664\n",
      "INFO:src.pipeline:  mae: 0.151029\n",
      "INFO:src.pipeline:  r2: 0.338237\n",
      "INFO:src.pipeline:  mape: 32.053757\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 9 ===\n",
      "INFO:src.pipeline:Train samples: 254, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 248,715\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.030403, Val Loss: 0.024246, Train MAE: 0.164842, Val MAE: 0.190350, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.028648, Val Loss: 0.025904, Train MAE: 0.164025, Val MAE: 0.185823, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 26\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 9.\n",
      "INFO:src.pipeline:Fold 9 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.052282\n",
      "INFO:src.pipeline:  rmse: 0.228653\n",
      "INFO:src.pipeline:  mae: 0.194455\n",
      "INFO:src.pipeline:  r2: 0.597011\n",
      "INFO:src.pipeline:  mape: 22.666676\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 10 ===\n",
      "INFO:src.pipeline:Train samples: 285, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 248,715\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.031109, Val Loss: 0.048609, Train MAE: 0.165164, Val MAE: 0.244355, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.026892, Val Loss: 0.048596, Train MAE: 0.157144, Val MAE: 0.244111, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.024875, Val Loss: 0.048967, Train MAE: 0.148536, Val MAE: 0.240329, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 35\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 10.\n",
      "INFO:src.pipeline:Fold 10 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.117787\n",
      "INFO:src.pipeline:  rmse: 0.343200\n",
      "INFO:src.pipeline:  mae: 0.256125\n",
      "INFO:src.pipeline:  r2: 0.344119\n",
      "INFO:src.pipeline:  mape: 71.068840\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 11 ===\n",
      "INFO:src.pipeline:Train samples: 315, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 248,715\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.032144, Val Loss: 0.103113, Train MAE: 0.175167, Val MAE: 0.344044, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.029918, Val Loss: 0.094507, Train MAE: 0.164786, Val MAE: 0.349811, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 22\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 11.\n",
      "INFO:src.pipeline:Fold 11 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.186161\n",
      "INFO:src.pipeline:  rmse: 0.431464\n",
      "INFO:src.pipeline:  mae: 0.334953\n",
      "INFO:src.pipeline:  r2: 0.095860\n",
      "INFO:src.pipeline:  mape: 121.796814\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 12 ===\n",
      "INFO:src.pipeline:Train samples: 346, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 248,715\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.034934, Val Loss: 0.040513, Train MAE: 0.188833, Val MAE: 0.227506, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.033411, Val Loss: 0.042204, Train MAE: 0.180457, Val MAE: 0.232039, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.030405, Val Loss: 0.049567, Train MAE: 0.170064, Val MAE: 0.252908, LR: 0.000500\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.027774, Val Loss: 0.043788, Train MAE: 0.163890, Val MAE: 0.256951, LR: 0.000500\n",
      "INFO:src.engine:Epoch [50/50] - Train Loss: 0.026812, Val Loss: 0.043038, Train MAE: 0.156322, Val MAE: 0.254084, LR: 0.000250\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 12.\n",
      "INFO:src.pipeline:Fold 12 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.086075\n",
      "INFO:src.pipeline:  rmse: 0.293386\n",
      "INFO:src.pipeline:  mae: 0.254084\n",
      "INFO:src.pipeline:  r2: 0.563546\n",
      "INFO:src.pipeline:  mape: 47.360630\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 13 ===\n",
      "INFO:src.pipeline:Train samples: 377, Val samples: 26\n",
      "INFO:src.pipeline:Model parameters: 248,715\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.036144, Val Loss: 0.060072, Train MAE: 0.191592, Val MAE: 0.303909, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.032052, Val Loss: 0.060951, Train MAE: 0.178404, Val MAE: 0.294883, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.029905, Val Loss: 0.064318, Train MAE: 0.171589, Val MAE: 0.309191, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 35\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 13.\n",
      "INFO:src.pipeline:Fold 13 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.132321\n",
      "INFO:src.pipeline:  rmse: 0.363760\n",
      "INFO:src.pipeline:  mae: 0.307927\n",
      "INFO:src.pipeline:  r2: 0.323739\n",
      "INFO:src.pipeline:  mape: 92.666595\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 14 ===\n",
      "INFO:src.pipeline:Train samples: 405, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 248,715\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.036489, Val Loss: 0.027486, Train MAE: 0.199724, Val MAE: 0.166650, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.035687, Val Loss: 0.024970, Train MAE: 0.193551, Val MAE: 0.162173, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.032830, Val Loss: 0.027077, Train MAE: 0.185153, Val MAE: 0.168210, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.029751, Val Loss: 0.027801, Train MAE: 0.175926, Val MAE: 0.177619, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 42\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 14.\n",
      "INFO:src.pipeline:Fold 14 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.060865\n",
      "INFO:src.pipeline:  rmse: 0.246708\n",
      "INFO:src.pipeline:  mae: 0.199013\n",
      "INFO:src.pipeline:  r2: 0.281853\n",
      "INFO:src.pipeline:  mape: 27.295702\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 15 ===\n",
      "INFO:src.pipeline:Train samples: 436, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 248,715\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.036188, Val Loss: 0.021630, Train MAE: 0.197616, Val MAE: 0.140669, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.033235, Val Loss: 0.021136, Train MAE: 0.190313, Val MAE: 0.123237, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.030431, Val Loss: 0.021840, Train MAE: 0.176769, Val MAE: 0.128197, LR: 0.000500\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.028783, Val Loss: 0.021521, Train MAE: 0.172607, Val MAE: 0.127603, LR: 0.000500\n",
      "INFO:src.engine:Epoch [50/50] - Train Loss: 0.028318, Val Loss: 0.021511, Train MAE: 0.165466, Val MAE: 0.123109, LR: 0.000250\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 15.\n",
      "INFO:src.pipeline:Fold 15 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.043036\n",
      "INFO:src.pipeline:  rmse: 0.207452\n",
      "INFO:src.pipeline:  mae: 0.123109\n",
      "INFO:src.pipeline:  r2: 0.488935\n",
      "INFO:src.pipeline:  mape: 35.954674\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 16 ===\n",
      "INFO:src.pipeline:Train samples: 466, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 248,715\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.035727, Val Loss: 0.018841, Train MAE: 0.194664, Val MAE: 0.123573, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.032060, Val Loss: 0.017845, Train MAE: 0.182232, Val MAE: 0.130375, LR: 0.000500\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.031188, Val Loss: 0.018383, Train MAE: 0.177524, Val MAE: 0.124597, LR: 0.000500\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.028380, Val Loss: 0.018389, Train MAE: 0.165779, Val MAE: 0.123132, LR: 0.000250\n",
      "INFO:src.engine:Early stopping at epoch 43\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 16.\n",
      "INFO:src.pipeline:Fold 16 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.037089\n",
      "INFO:src.pipeline:  rmse: 0.192585\n",
      "INFO:src.pipeline:  mae: 0.122638\n",
      "INFO:src.pipeline:  r2: 0.337046\n",
      "INFO:src.pipeline:  mape: 48.155388\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 17 ===\n",
      "INFO:src.pipeline:Train samples: 497, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 248,715\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.033720, Val Loss: 0.018065, Train MAE: 0.187672, Val MAE: 0.144749, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.031292, Val Loss: 0.017351, Train MAE: 0.179725, Val MAE: 0.130667, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.028707, Val Loss: 0.017407, Train MAE: 0.168811, Val MAE: 0.134738, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 31\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 17.\n",
      "INFO:src.pipeline:Fold 17 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.034242\n",
      "INFO:src.pipeline:  rmse: 0.185047\n",
      "INFO:src.pipeline:  mae: 0.125746\n",
      "INFO:src.pipeline:  r2: 0.416579\n",
      "INFO:src.pipeline:  mape: 27.143150\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 18 ===\n",
      "INFO:src.pipeline:Train samples: 527, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 248,715\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.031619, Val Loss: 0.015360, Train MAE: 0.180323, Val MAE: 0.105376, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.030337, Val Loss: 0.015122, Train MAE: 0.174290, Val MAE: 0.105518, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 27\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 18.\n",
      "INFO:src.pipeline:Fold 18 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.030339\n",
      "INFO:src.pipeline:  rmse: 0.174181\n",
      "INFO:src.pipeline:  mae: 0.105211\n",
      "INFO:src.pipeline:  r2: 0.298361\n",
      "INFO:src.pipeline:  mape: 19.571131\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 19 ===\n",
      "INFO:src.pipeline:Train samples: 558, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 248,715\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.033165, Val Loss: 0.025512, Train MAE: 0.182916, Val MAE: 0.128362, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.030082, Val Loss: 0.025543, Train MAE: 0.172608, Val MAE: 0.137166, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 25\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 19.\n",
      "INFO:src.pipeline:Fold 19 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.050332\n",
      "INFO:src.pipeline:  rmse: 0.224347\n",
      "INFO:src.pipeline:  mae: 0.128020\n",
      "INFO:src.pipeline:  r2: 0.361874\n",
      "INFO:src.pipeline:  mape: 13.668256\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 20 ===\n",
      "INFO:src.pipeline:Train samples: 589, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 248,715\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.030941, Val Loss: 0.021340, Train MAE: 0.175834, Val MAE: 0.149413, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.028651, Val Loss: 0.021491, Train MAE: 0.167937, Val MAE: 0.146048, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 27\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 20.\n",
      "INFO:src.pipeline:Fold 20 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.043429\n",
      "INFO:src.pipeline:  rmse: 0.208397\n",
      "INFO:src.pipeline:  mae: 0.146187\n",
      "INFO:src.pipeline:  r2: 0.455448\n",
      "INFO:src.pipeline:  mape: 22.544422\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 21 ===\n",
      "INFO:src.pipeline:Train samples: 619, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 248,715\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.030400, Val Loss: 0.014925, Train MAE: 0.182697, Val MAE: 0.126824, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.029974, Val Loss: 0.018557, Train MAE: 0.170469, Val MAE: 0.168135, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.027476, Val Loss: 0.018363, Train MAE: 0.163268, Val MAE: 0.161611, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 30\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 21.\n",
      "INFO:src.pipeline:Fold 21 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.036725\n",
      "INFO:src.pipeline:  rmse: 0.191638\n",
      "INFO:src.pipeline:  mae: 0.161611\n",
      "INFO:src.pipeline:  r2: 0.712298\n",
      "INFO:src.pipeline:  mape: 20.866888\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 22 ===\n",
      "INFO:src.pipeline:Train samples: 650, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 248,715\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.029972, Val Loss: 0.039061, Train MAE: 0.174006, Val MAE: 0.207992, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.027416, Val Loss: 0.040017, Train MAE: 0.165451, Val MAE: 0.215930, LR: 0.000500\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.027081, Val Loss: 0.038922, Train MAE: 0.157508, Val MAE: 0.199844, LR: 0.000500\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.025471, Val Loss: 0.038187, Train MAE: 0.156549, Val MAE: 0.197176, LR: 0.000250\n",
      "INFO:src.engine:Early stopping at epoch 47\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 22.\n",
      "INFO:src.pipeline:Fold 22 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.074795\n",
      "INFO:src.pipeline:  rmse: 0.273487\n",
      "INFO:src.pipeline:  mae: 0.175674\n",
      "INFO:src.pipeline:  r2: 0.539539\n",
      "INFO:src.pipeline:  mape: 54.830788\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 23 ===\n",
      "INFO:src.pipeline:Train samples: 680, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 248,715\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.032571, Val Loss: 0.053256, Train MAE: 0.177897, Val MAE: 0.269582, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.028960, Val Loss: 0.048000, Train MAE: 0.167367, Val MAE: 0.255656, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.027791, Val Loss: 0.044372, Train MAE: 0.163230, Val MAE: 0.231192, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 37\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 23.\n",
      "INFO:src.pipeline:Fold 23 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.085278\n",
      "INFO:src.pipeline:  rmse: 0.292025\n",
      "INFO:src.pipeline:  mae: 0.224872\n",
      "INFO:src.pipeline:  r2: 0.567974\n",
      "INFO:src.pipeline:  mape: 92.828514\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 24 ===\n",
      "INFO:src.pipeline:Train samples: 711, Val samples: 21\n",
      "INFO:src.pipeline:Model parameters: 248,715\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.031965, Val Loss: 0.069311, Train MAE: 0.181530, Val MAE: 0.303057, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.031530, Val Loss: 0.063039, Train MAE: 0.181954, Val MAE: 0.265795, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.029533, Val Loss: 0.061996, Train MAE: 0.173053, Val MAE: 0.262301, LR: 0.000500\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.027679, Val Loss: 0.060539, Train MAE: 0.164468, Val MAE: 0.268398, LR: 0.000500\n",
      "INFO:src.engine:Epoch [50/50] - Train Loss: 0.026582, Val Loss: 0.057351, Train MAE: 0.161655, Val MAE: 0.258324, LR: 0.000250\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 24.\n",
      "INFO:src.pipeline:Fold 24 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.114830\n",
      "INFO:src.pipeline:  rmse: 0.338866\n",
      "INFO:src.pipeline:  mae: 0.258324\n",
      "INFO:src.pipeline:  r2: 0.450593\n",
      "INFO:src.pipeline:  mape: 104.819557\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 25 ===\n",
      "INFO:src.pipeline:Train samples: 734, Val samples: 23\n",
      "INFO:src.pipeline:Model parameters: 248,715\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.033838, Val Loss: 0.056564, Train MAE: 0.188347, Val MAE: 0.251897, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.031201, Val Loss: 0.060208, Train MAE: 0.175447, Val MAE: 0.228515, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.028971, Val Loss: 0.053738, Train MAE: 0.167080, Val MAE: 0.228831, LR: 0.001000\n",
      "INFO:src.engine:Early stopping at epoch 39\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 25.\n",
      "INFO:src.pipeline:Fold 25 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.118795\n",
      "INFO:src.pipeline:  rmse: 0.344666\n",
      "INFO:src.pipeline:  mae: 0.228726\n",
      "INFO:src.pipeline:  r2: 0.409527\n",
      "INFO:src.pipeline:  mape: 19.733927\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 26 ===\n",
      "INFO:src.pipeline:Train samples: 759, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 248,715\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.034023, Val Loss: 0.061698, Train MAE: 0.189230, Val MAE: 0.246979, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.032306, Val Loss: 0.059714, Train MAE: 0.180006, Val MAE: 0.248397, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 22\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 26.\n",
      "INFO:src.pipeline:Fold 26 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.124290\n",
      "INFO:src.pipeline:  rmse: 0.352547\n",
      "INFO:src.pipeline:  mae: 0.252341\n",
      "INFO:src.pipeline:  r2: 0.147418\n",
      "INFO:src.pipeline:  mape: 155.106964\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 27 ===\n",
      "INFO:src.pipeline:Train samples: 790, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 248,715\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.035064, Val Loss: 0.034343, Train MAE: 0.194410, Val MAE: 0.174226, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.032591, Val Loss: 0.034502, Train MAE: 0.183437, Val MAE: 0.182730, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.030572, Val Loss: 0.035259, Train MAE: 0.174267, Val MAE: 0.169140, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 34\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 27.\n",
      "INFO:src.pipeline:Fold 27 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.070078\n",
      "INFO:src.pipeline:  rmse: 0.264722\n",
      "INFO:src.pipeline:  mae: 0.164773\n",
      "INFO:src.pipeline:  r2: 0.353839\n",
      "INFO:src.pipeline:  mape: 61.622501\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 28 ===\n",
      "INFO:src.pipeline:Train samples: 820, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 248,715\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.034034, Val Loss: 0.031633, Train MAE: 0.189308, Val MAE: 0.177384, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.032684, Val Loss: 0.031051, Train MAE: 0.179990, Val MAE: 0.184197, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.029923, Val Loss: 0.030845, Train MAE: 0.171559, Val MAE: 0.186631, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 33\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 28.\n",
      "INFO:src.pipeline:Fold 28 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.062329\n",
      "INFO:src.pipeline:  rmse: 0.249657\n",
      "INFO:src.pipeline:  mae: 0.182747\n",
      "INFO:src.pipeline:  r2: 0.248713\n",
      "INFO:src.pipeline:  mape: 45.205086\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 29 ===\n",
      "INFO:src.pipeline:Train samples: 851, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 248,715\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.033906, Val Loss: 0.012094, Train MAE: 0.189714, Val MAE: 0.142144, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.031445, Val Loss: 0.009983, Train MAE: 0.180815, Val MAE: 0.124235, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.030320, Val Loss: 0.009475, Train MAE: 0.176804, Val MAE: 0.118523, LR: 0.001000\n",
      "INFO:src.engine:Early stopping at epoch 39\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 29.\n",
      "INFO:src.pipeline:Fold 29 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.017205\n",
      "INFO:src.pipeline:  rmse: 0.131167\n",
      "INFO:src.pipeline:  mae: 0.108071\n",
      "INFO:src.pipeline:  r2: 0.580983\n",
      "INFO:src.pipeline:  mape: 13.644589\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 30 ===\n",
      "INFO:src.pipeline:Train samples: 881, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 248,715\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.033571, Val Loss: 0.021798, Train MAE: 0.186716, Val MAE: 0.123814, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.030572, Val Loss: 0.021551, Train MAE: 0.177661, Val MAE: 0.118353, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.029370, Val Loss: 0.020830, Train MAE: 0.172183, Val MAE: 0.097190, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.027897, Val Loss: 0.020687, Train MAE: 0.163596, Val MAE: 0.104534, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 45\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 30.\n",
      "INFO:src.pipeline:Fold 30 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.042765\n",
      "INFO:src.pipeline:  rmse: 0.206797\n",
      "INFO:src.pipeline:  mae: 0.113048\n",
      "INFO:src.pipeline:  r2: 0.255489\n",
      "INFO:src.pipeline:  mape: 8.062436\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 31 ===\n",
      "INFO:src.pipeline:Train samples: 911, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 248,715\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.032873, Val Loss: 0.005091, Train MAE: 0.181597, Val MAE: 0.071747, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.031048, Val Loss: 0.004987, Train MAE: 0.175728, Val MAE: 0.069248, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.028588, Val Loss: 0.005875, Train MAE: 0.166239, Val MAE: 0.084928, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 35\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 31.\n",
      "INFO:src.pipeline:Fold 31 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.012453\n",
      "INFO:src.pipeline:  rmse: 0.111594\n",
      "INFO:src.pipeline:  mae: 0.090158\n",
      "INFO:src.pipeline:  r2: 0.679369\n",
      "INFO:src.pipeline:  mape: 10.686665\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 32 ===\n",
      "INFO:src.pipeline:Train samples: 942, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 248,715\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.032233, Val Loss: 0.012535, Train MAE: 0.178830, Val MAE: 0.104297, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.029318, Val Loss: 0.013317, Train MAE: 0.166590, Val MAE: 0.117099, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 22\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 32.\n",
      "INFO:src.pipeline:Fold 32 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.029637\n",
      "INFO:src.pipeline:  rmse: 0.172153\n",
      "INFO:src.pipeline:  mae: 0.126089\n",
      "INFO:src.pipeline:  r2: 0.496649\n",
      "INFO:src.pipeline:  mape: 13.727166\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 33 ===\n",
      "INFO:src.pipeline:Train samples: 972, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 248,715\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.031346, Val Loss: 0.060699, Train MAE: 0.173768, Val MAE: 0.243676, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.029257, Val Loss: 0.060954, Train MAE: 0.167118, Val MAE: 0.232458, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 23\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 33.\n",
      "INFO:src.pipeline:Fold 33 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.124672\n",
      "INFO:src.pipeline:  rmse: 0.353089\n",
      "INFO:src.pipeline:  mae: 0.235362\n",
      "INFO:src.pipeline:  r2: 0.332199\n",
      "INFO:src.pipeline:  mape: 114.030144\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 34 ===\n",
      "INFO:src.pipeline:Train samples: 1003, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 248,715\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.032395, Val Loss: 0.039872, Train MAE: 0.179801, Val MAE: 0.199298, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.031160, Val Loss: 0.039008, Train MAE: 0.173298, Val MAE: 0.223434, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.029967, Val Loss: 0.047406, Train MAE: 0.164922, Val MAE: 0.244820, LR: 0.001000\n",
      "INFO:src.engine:Early stopping at epoch 39\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 34.\n",
      "INFO:src.pipeline:Fold 34 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.093151\n",
      "INFO:src.pipeline:  rmse: 0.305207\n",
      "INFO:src.pipeline:  mae: 0.246676\n",
      "INFO:src.pipeline:  r2: 0.463758\n",
      "INFO:src.pipeline:  mape: 70.502151\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 35 ===\n",
      "INFO:src.pipeline:Train samples: 1033, Val samples: 21\n",
      "INFO:src.pipeline:Model parameters: 248,715\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.032552, Val Loss: 0.078200, Train MAE: 0.182504, Val MAE: 0.321202, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.030768, Val Loss: 0.078516, Train MAE: 0.173110, Val MAE: 0.321086, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.029253, Val Loss: 0.097092, Train MAE: 0.167841, Val MAE: 0.360148, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 31\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 35.\n",
      "INFO:src.pipeline:Fold 35 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.173786\n",
      "INFO:src.pipeline:  rmse: 0.416877\n",
      "INFO:src.pipeline:  mae: 0.341343\n",
      "INFO:src.pipeline:  r2: 0.282257\n",
      "INFO:src.pipeline:  mape: 79.404083\n",
      "INFO:src.pipeline:\n",
      "=== Experiment Summary ===\n",
      "INFO:src.pipeline:Average Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.078106 ± 0.049085\n",
      "INFO:src.pipeline:  rmse: 0.265656 ± 0.086790\n",
      "INFO:src.pipeline:  mae: 0.198858 ± 0.081093\n",
      "INFO:src.pipeline:  r2: 0.303127 ± 0.358872\n",
      "INFO:src.pipeline:  mape: 49.587052 ± 36.562833\n"
     ]
    }
   ],
   "source": [
    "from src.preprocessing import KBinConfig, process_splits_to_kbins, build_model_arrays, to_fixedgrid_multiindex\n",
    "import pandas as pd\n",
    "from src.utils import DataManager\n",
    "from src.pipeline import SolarForecastingPipeline\n",
    "\n",
    "# --- Block 1: Define variables ---\n",
    "USE_KBINS = False\n",
    "k_bins = 60\n",
    "TARGET_COL = \"CSI_ghi\"  # <-- FIX 1: Define Target FIRST\n",
    "history_days = 7\n",
    "horizon_days = 1\n",
    "\n",
    "# This list now correctly excludes the target\n",
    "feature_cols = [c for c in df_phase1.columns.tolist() if c != TARGET_COL]\n",
    "\n",
    "# --- Block 2: Build model arrays (Your code was correct here) ---\n",
    "if USE_KBINS:\n",
    "        print(\"\\n--- Step 2: Converting full dataset to K-Bins format ---\")\n",
    "        # ... (rest of your K-Bins code) ...\n",
    "else:\n",
    "    print(\"\\n--- Step 2: Building model arrays (X, Y) ---\")\n",
    "    fixed_df = to_fixedgrid_multiindex(df_phase1, timestamp_col=\"measurement_time\", expected_T=None)  # or set T\n",
    "    \n",
    "    # You correctly passed the filtered 'feature_cols' here, fixing the data leak!\n",
    "    ph1_X, ph1_Y, ph1_labels_list = build_model_arrays(\n",
    "        fixed_df,\n",
    "        feature_cols=feature_cols,  # <-- This is correct!\n",
    "        target_col=TARGET_COL,\n",
    "        history_days=history_days,\n",
    "        horizon_days=horizon_days,\n",
    "    )\n",
    "\n",
    "# --- Block 3: Save arrays ---\n",
    "data_manager = DataManager()\n",
    "data_manager.save_arrays(\n",
    "    ph1_X, ph1_Y,\n",
    "    pd.DataFrame(index=pd.to_datetime(ph1_labels_list, utc=True)),\n",
    "    filename_prefix='phas1_data',  # <-- You are saving as 'phas2_data'\n",
    "    feature_cols=feature_cols,\n",
    "    target_col=TARGET_COL,\n",
    "    metadata={\n",
    "        \"input_csv\": \"data/processed/df_1h_lag_BLV_spatial_images.csv\",\n",
    "        \"timestamp_col\": \"measurement_time\",\n",
    "        \"feature_set\": feature_cols,\n",
    "        \"history_days\": 7,\n",
    "        \"horizon_days\": 1,\n",
    "        \"k_bins\": None,\n",
    "    }\n",
    ")\n",
    "\n",
    "# --- Block 4: Configure and Run ---\n",
    "LSTM_CONFIG = {\n",
    "    \"experiment_name\": \"UniLSTM_p1_exp01_essentialF\",\n",
    "    \"model_type\": \"LSTM\",\n",
    "    \"model_config\": {\n",
    "        \"hidden_size\": 64,\n",
    "        \"num_layers\": 2,\n",
    "        \"dropout\": 0.35,\n",
    "        \"bidirectional\": True,\n",
    "    },\n",
    "    \"data_prefix\": \"phas1_data\",  # <-- FIX 2: Match the filename_prefix\n",
    "    \"splits_file\": \"exp-004/exp-004rolling_origin_splits.json\",\n",
    "    \"feature_cols\": feature_cols,\n",
    "    \"feature_selection\": feature_cols, # <-- This is correct!\n",
    "    \"target_col\": TARGET_COL,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_epochs\": 50,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"loss_function\": \"Huber\",  # <-- This setting needs the fix below\n",
    "    \"early_stopping_patience\": 20,\n",
    "    \"max_folds\": 35,\n",
    "}\n",
    "\n",
    "# This will now work after you apply the fixes below\n",
    "pipeline = SolarForecastingPipeline(LSTM_CONFIG)\n",
    "_, summary = pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f26ed5",
   "metadata": {},
   "source": [
    "The model has learned the easiest possible trick: it's predicting that the value at the next time step will be the same as the value at the current time step.\n",
    "\n",
    "It is a Naive (Persistence Model)\n",
    "\n",
    "\n",
    "\"Lazy learning\" (the effect) is a model's symptom. \"Incorrect sequencing\" (the cause) is the disease. Investigation needed.\n",
    "    - Solution1: Adding time_gap_hours',\n",
    "       'time_gap_norm', 'day_boundary_flag', 'hour_progression',\n",
    "       'absolute_hour', to the model, as well the cyclical time features problem is still. \n",
    "\n",
    "    - Solution 2 adding more statistical features about historical Lag like 'B_CSI_ghi_8h', 'V_CSI_ghi_8h', 'L_CSI_ghi_8h', 'B_CSI_ghi_9h',\n",
    "       'V_CSI_ghi_9h', 'L_CSI_ghi_9h', 'B_CSI_ghi_10h', 'V_CSI_ghi_10h',\n",
    "       'L_CSI_ghi_10h', 'B_CSI_ghi_11h', 'V_CSI_ghi_11h', 'L_CSI_ghi_11h',\n",
    "       'B_CSI_ghi_12h', 'V_CSI_ghi_12h', 'L_CSI_ghi_12h', 'B_CSI_ghi_13h',\n",
    "       'V_CSI_ghi_13h', 'L_CSI_ghi_13h', 'B_CSI_ghi_14h', 'V_CSI_ghi_14h',\n",
    "       'L_CSI_ghi_14h', 'B_CSI_ghi_15h', 'V_CSI_ghi_15h', 'L_CSI_ghi_15h',\n",
    "       'B_CSI_ghi_16h', 'V_CSI_ghi_16h', 'L_CSI_ghi_16h', 'B_CSI_ghi_17h',\n",
    "       'V_CSI_ghi_17h', 'L_CSI_ghi_17h', 'B_CSI_ghi_18h', 'V_CSI_ghi_18h',\n",
    "       'L_CSI_ghi_18h', 'B_CSI_ghi_19h', 'V_CSI_ghi_19h', 'L_CSI_ghi_19h'\n",
    "    - Problem is still so i will move to the other \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a4d784",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9572a8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.utils:Saved arrays to data/phas2_data_*.npy\n",
      "INFO:src.utils:X shape: (1055, 7, 11, 47), Y shape: (1055, 1, 11)\n",
      "INFO:src.pipeline:Loading data...\n",
      "INFO:src.utils:Loaded arrays from data/phas2_data_*.npy\n",
      "INFO:src.utils:X shape: (1055, 7, 11, 47), Y shape: (1055, 1, 11)\n",
      "INFO:src.pipeline:Building FIXED-GRID reference frame (K_bins is None)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 2: Building model arrays (X, Y) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:src.pipeline:Failed to construct reference dataframe for NAM comparison: 'target_time'\n",
      "INFO:src.utils:Loaded 35 folds from exp-004/exp-004rolling_origin_splits.json\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 1 ===\n",
      "INFO:src.pipeline:Train samples: 21, Val samples: 18\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.069708, Val Loss: 0.099907, Train MAE: 0.282255, Val MAE: 0.374032, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.054642, Val Loss: 0.101493, Train MAE: 0.239465, Val MAE: 0.379981, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.054945, Val Loss: 0.100272, Train MAE: 0.248461, Val MAE: 0.386173, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 31\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 1.\n",
      "INFO:src.pipeline:Fold 1 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.200547\n",
      "INFO:src.pipeline:  rmse: 0.447825\n",
      "INFO:src.pipeline:  mae: 0.386084\n",
      "INFO:src.pipeline:  r2: 0.083255\n",
      "INFO:src.pipeline:  mape: 111.452530\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 2 ===\n",
      "INFO:src.pipeline:Train samples: 40, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.071297, Val Loss: 0.060338, Train MAE: 0.329235, Val MAE: 0.309224, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.060832, Val Loss: 0.058418, Train MAE: 0.297003, Val MAE: 0.305215, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.057110, Val Loss: 0.053860, Train MAE: 0.281203, Val MAE: 0.284067, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.035655, Val Loss: 0.072416, Train MAE: 0.203179, Val MAE: 0.348237, LR: 0.001000\n",
      "INFO:src.engine:Epoch [50/50] - Train Loss: 0.046234, Val Loss: 0.142530, Train MAE: 0.210655, Val MAE: 0.475623, LR: 0.000500\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 2.\n",
      "INFO:src.pipeline:Fold 2 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.285060\n",
      "INFO:src.pipeline:  rmse: 0.533910\n",
      "INFO:src.pipeline:  mae: 0.475623\n",
      "INFO:src.pipeline:  r2: -1.412765\n",
      "INFO:src.pipeline:  mape: 63.425835\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 3 ===\n",
      "INFO:src.pipeline:Train samples: 71, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.066946, Val Loss: 0.031758, Train MAE: 0.303205, Val MAE: 0.217697, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.050030, Val Loss: 0.029642, Train MAE: 0.258705, Val MAE: 0.201703, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.039096, Val Loss: 0.027947, Train MAE: 0.212193, Val MAE: 0.187695, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.036369, Val Loss: 0.028375, Train MAE: 0.206798, Val MAE: 0.192656, LR: 0.000500\n",
      "INFO:src.engine:Epoch [50/50] - Train Loss: 0.037343, Val Loss: 0.038362, Train MAE: 0.216984, Val MAE: 0.239442, LR: 0.000500\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 3.\n",
      "INFO:src.pipeline:Fold 3 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.076724\n",
      "INFO:src.pipeline:  rmse: 0.276992\n",
      "INFO:src.pipeline:  mae: 0.239442\n",
      "INFO:src.pipeline:  r2: 0.075373\n",
      "INFO:src.pipeline:  mape: 36.028419\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 4 ===\n",
      "INFO:src.pipeline:Train samples: 101, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.060483, Val Loss: 0.020286, Train MAE: 0.279674, Val MAE: 0.186462, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.053720, Val Loss: 0.016987, Train MAE: 0.257586, Val MAE: 0.164015, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.038307, Val Loss: 0.014079, Train MAE: 0.210791, Val MAE: 0.127659, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.037617, Val Loss: 0.015268, Train MAE: 0.206786, Val MAE: 0.110029, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 46\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 4.\n",
      "INFO:src.pipeline:Fold 4 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.039052\n",
      "INFO:src.pipeline:  rmse: 0.197617\n",
      "INFO:src.pipeline:  mae: 0.130020\n",
      "INFO:src.pipeline:  r2: 0.162173\n",
      "INFO:src.pipeline:  mape: 18.426487\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 5 ===\n",
      "INFO:src.pipeline:Train samples: 132, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.040414, Val Loss: 0.012189, Train MAE: 0.216376, Val MAE: 0.133552, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.033239, Val Loss: 0.012342, Train MAE: 0.193066, Val MAE: 0.138385, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.028102, Val Loss: 0.018214, Train MAE: 0.175395, Val MAE: 0.169374, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 38\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 5.\n",
      "INFO:src.pipeline:Fold 5 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.046919\n",
      "INFO:src.pipeline:  rmse: 0.216608\n",
      "INFO:src.pipeline:  mae: 0.191900\n",
      "INFO:src.pipeline:  r2: -0.757909\n",
      "INFO:src.pipeline:  mape: 20.081072\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 6 ===\n",
      "INFO:src.pipeline:Train samples: 162, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.035106, Val Loss: 0.012256, Train MAE: 0.182668, Val MAE: 0.089284, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.030579, Val Loss: 0.012442, Train MAE: 0.176014, Val MAE: 0.083532, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 24\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 6.\n",
      "INFO:src.pipeline:Fold 6 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.025981\n",
      "INFO:src.pipeline:  rmse: 0.161185\n",
      "INFO:src.pipeline:  mae: 0.083808\n",
      "INFO:src.pipeline:  r2: 0.348323\n",
      "INFO:src.pipeline:  mape: 15.781154\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 7 ===\n",
      "INFO:src.pipeline:Train samples: 193, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.047292, Val Loss: 0.021580, Train MAE: 0.216231, Val MAE: 0.118821, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.023386, Val Loss: 0.022284, Train MAE: 0.144005, Val MAE: 0.122197, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 24\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 7.\n",
      "INFO:src.pipeline:Fold 7 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.044398\n",
      "INFO:src.pipeline:  rmse: 0.210709\n",
      "INFO:src.pipeline:  mae: 0.109533\n",
      "INFO:src.pipeline:  r2: 0.374838\n",
      "INFO:src.pipeline:  mape: 32.507805\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 8 ===\n",
      "INFO:src.pipeline:Train samples: 224, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.029068, Val Loss: 0.025458, Train MAE: 0.166871, Val MAE: 0.127317, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.024175, Val Loss: 0.027832, Train MAE: 0.143792, Val MAE: 0.147388, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 27\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 8.\n",
      "INFO:src.pipeline:Fold 8 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.055535\n",
      "INFO:src.pipeline:  rmse: 0.235658\n",
      "INFO:src.pipeline:  mae: 0.150315\n",
      "INFO:src.pipeline:  r2: 0.315218\n",
      "INFO:src.pipeline:  mape: 31.190931\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 9 ===\n",
      "INFO:src.pipeline:Train samples: 254, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.026953, Val Loss: 0.029423, Train MAE: 0.158668, Val MAE: 0.157048, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.023769, Val Loss: 0.026277, Train MAE: 0.144941, Val MAE: 0.168545, LR: 0.000500\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.021863, Val Loss: 0.027338, Train MAE: 0.139020, Val MAE: 0.176919, LR: 0.000250\n",
      "INFO:src.engine:Early stopping at epoch 35\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 9.\n",
      "INFO:src.pipeline:Fold 9 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.060271\n",
      "INFO:src.pipeline:  rmse: 0.245502\n",
      "INFO:src.pipeline:  mae: 0.187911\n",
      "INFO:src.pipeline:  r2: 0.535430\n",
      "INFO:src.pipeline:  mape: 21.007057\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 10 ===\n",
      "INFO:src.pipeline:Train samples: 285, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.027996, Val Loss: 0.043053, Train MAE: 0.160688, Val MAE: 0.223395, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.023068, Val Loss: 0.045004, Train MAE: 0.137409, Val MAE: 0.215032, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 28\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 10.\n",
      "INFO:src.pipeline:Fold 10 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.088650\n",
      "INFO:src.pipeline:  rmse: 0.297742\n",
      "INFO:src.pipeline:  mae: 0.231621\n",
      "INFO:src.pipeline:  r2: 0.506362\n",
      "INFO:src.pipeline:  mape: 62.391060\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 11 ===\n",
      "INFO:src.pipeline:Train samples: 315, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.028458, Val Loss: 0.059267, Train MAE: 0.161197, Val MAE: 0.283681, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.024290, Val Loss: 0.054534, Train MAE: 0.149296, Val MAE: 0.276157, LR: 0.001000\n",
      "INFO:src.engine:Early stopping at epoch 29\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 11.\n",
      "INFO:src.pipeline:Fold 11 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.127757\n",
      "INFO:src.pipeline:  rmse: 0.357431\n",
      "INFO:src.pipeline:  mae: 0.283234\n",
      "INFO:src.pipeline:  r2: 0.379513\n",
      "INFO:src.pipeline:  mape: 76.641579\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 12 ===\n",
      "INFO:src.pipeline:Train samples: 346, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.031632, Val Loss: 0.036493, Train MAE: 0.174705, Val MAE: 0.234742, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.026559, Val Loss: 0.032512, Train MAE: 0.157724, Val MAE: 0.206402, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.024062, Val Loss: 0.035140, Train MAE: 0.152777, Val MAE: 0.217228, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.021298, Val Loss: 0.038435, Train MAE: 0.140071, Val MAE: 0.204533, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 40\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 12.\n",
      "INFO:src.pipeline:Fold 12 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.076870\n",
      "INFO:src.pipeline:  rmse: 0.277254\n",
      "INFO:src.pipeline:  mae: 0.204533\n",
      "INFO:src.pipeline:  r2: 0.610223\n",
      "INFO:src.pipeline:  mape: 47.403477\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 13 ===\n",
      "INFO:src.pipeline:Train samples: 377, Val samples: 26\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.033713, Val Loss: 0.052261, Train MAE: 0.188196, Val MAE: 0.247929, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.026715, Val Loss: 0.040148, Train MAE: 0.162534, Val MAE: 0.222808, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.025226, Val Loss: 0.042282, Train MAE: 0.159336, Val MAE: 0.203717, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.021183, Val Loss: 0.040458, Train MAE: 0.140921, Val MAE: 0.206910, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 43\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 13.\n",
      "INFO:src.pipeline:Fold 13 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.082360\n",
      "INFO:src.pipeline:  rmse: 0.286984\n",
      "INFO:src.pipeline:  mae: 0.202866\n",
      "INFO:src.pipeline:  r2: 0.579078\n",
      "INFO:src.pipeline:  mape: 71.825966\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 14 ===\n",
      "INFO:src.pipeline:Train samples: 405, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.033351, Val Loss: 0.033548, Train MAE: 0.187002, Val MAE: 0.189299, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.028748, Val Loss: 0.044076, Train MAE: 0.169841, Val MAE: 0.219650, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 21\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 14.\n",
      "INFO:src.pipeline:Fold 14 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.083442\n",
      "INFO:src.pipeline:  rmse: 0.288863\n",
      "INFO:src.pipeline:  mae: 0.205615\n",
      "INFO:src.pipeline:  r2: 0.015472\n",
      "INFO:src.pipeline:  mape: 27.462873\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 15 ===\n",
      "INFO:src.pipeline:Train samples: 436, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.032753, Val Loss: 0.022789, Train MAE: 0.183818, Val MAE: 0.153451, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.025641, Val Loss: 0.022630, Train MAE: 0.159034, Val MAE: 0.141895, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 26\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 15.\n",
      "INFO:src.pipeline:Fold 15 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.046192\n",
      "INFO:src.pipeline:  rmse: 0.214923\n",
      "INFO:src.pipeline:  mae: 0.143499\n",
      "INFO:src.pipeline:  r2: 0.451462\n",
      "INFO:src.pipeline:  mape: 37.901726\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 16 ===\n",
      "INFO:src.pipeline:Train samples: 466, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.033065, Val Loss: 0.017829, Train MAE: 0.185445, Val MAE: 0.143260, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.029379, Val Loss: 0.018719, Train MAE: 0.173973, Val MAE: 0.119846, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.024462, Val Loss: 0.018884, Train MAE: 0.155659, Val MAE: 0.131763, LR: 0.000500\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.021107, Val Loss: 0.018524, Train MAE: 0.145147, Val MAE: 0.121873, LR: 0.000250\n",
      "INFO:src.engine:Early stopping at epoch 46\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 16.\n",
      "INFO:src.pipeline:Fold 16 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.038067\n",
      "INFO:src.pipeline:  rmse: 0.195108\n",
      "INFO:src.pipeline:  mae: 0.123596\n",
      "INFO:src.pipeline:  r2: 0.319556\n",
      "INFO:src.pipeline:  mape: 42.778938\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 17 ===\n",
      "INFO:src.pipeline:Train samples: 497, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.032051, Val Loss: 0.016688, Train MAE: 0.183026, Val MAE: 0.119073, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.026796, Val Loss: 0.016329, Train MAE: 0.161395, Val MAE: 0.113219, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.023147, Val Loss: 0.015676, Train MAE: 0.150882, Val MAE: 0.114732, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.020238, Val Loss: 0.017061, Train MAE: 0.141801, Val MAE: 0.147692, LR: 0.001000\n",
      "INFO:src.engine:Epoch [50/50] - Train Loss: 0.019224, Val Loss: 0.022044, Train MAE: 0.133959, Val MAE: 0.185099, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 50\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 17.\n",
      "INFO:src.pipeline:Fold 17 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.044087\n",
      "INFO:src.pipeline:  rmse: 0.209970\n",
      "INFO:src.pipeline:  mae: 0.185099\n",
      "INFO:src.pipeline:  r2: 0.248843\n",
      "INFO:src.pipeline:  mape: 27.380783\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 18 ===\n",
      "INFO:src.pipeline:Train samples: 527, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.030642, Val Loss: 0.015118, Train MAE: 0.175178, Val MAE: 0.113069, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.027484, Val Loss: 0.015010, Train MAE: 0.166736, Val MAE: 0.092975, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.024977, Val Loss: 0.014785, Train MAE: 0.153384, Val MAE: 0.106731, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.018870, Val Loss: 0.015703, Train MAE: 0.132451, Val MAE: 0.120641, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 47\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 18.\n",
      "INFO:src.pipeline:Fold 18 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.030888\n",
      "INFO:src.pipeline:  rmse: 0.175751\n",
      "INFO:src.pipeline:  mae: 0.101933\n",
      "INFO:src.pipeline:  r2: 0.285650\n",
      "INFO:src.pipeline:  mape: 19.591917\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 19 ===\n",
      "INFO:src.pipeline:Train samples: 558, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.028810, Val Loss: 0.025356, Train MAE: 0.170262, Val MAE: 0.127450, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.023884, Val Loss: 0.025723, Train MAE: 0.153779, Val MAE: 0.124933, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 22\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 19.\n",
      "INFO:src.pipeline:Fold 19 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.051770\n",
      "INFO:src.pipeline:  rmse: 0.227531\n",
      "INFO:src.pipeline:  mae: 0.128492\n",
      "INFO:src.pipeline:  r2: 0.343632\n",
      "INFO:src.pipeline:  mape: 13.546510\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 20 ===\n",
      "INFO:src.pipeline:Train samples: 589, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.029679, Val Loss: 0.019289, Train MAE: 0.170801, Val MAE: 0.128000, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.026448, Val Loss: 0.022518, Train MAE: 0.158644, Val MAE: 0.152143, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 28\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 20.\n",
      "INFO:src.pipeline:Fold 20 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.047519\n",
      "INFO:src.pipeline:  rmse: 0.217989\n",
      "INFO:src.pipeline:  mae: 0.140707\n",
      "INFO:src.pipeline:  r2: 0.404164\n",
      "INFO:src.pipeline:  mape: 21.565231\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 21 ===\n",
      "INFO:src.pipeline:Train samples: 619, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.028063, Val Loss: 0.016777, Train MAE: 0.166865, Val MAE: 0.144713, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.025840, Val Loss: 0.018147, Train MAE: 0.157901, Val MAE: 0.147700, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.023751, Val Loss: 0.023259, Train MAE: 0.146647, Val MAE: 0.169856, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 32\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 21.\n",
      "INFO:src.pipeline:Fold 21 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.042274\n",
      "INFO:src.pipeline:  rmse: 0.205607\n",
      "INFO:src.pipeline:  mae: 0.151696\n",
      "INFO:src.pipeline:  r2: 0.668827\n",
      "INFO:src.pipeline:  mape: 19.009689\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 22 ===\n",
      "INFO:src.pipeline:Train samples: 650, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.030323, Val Loss: 0.039374, Train MAE: 0.171777, Val MAE: 0.190860, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.026058, Val Loss: 0.036152, Train MAE: 0.159686, Val MAE: 0.180274, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.022740, Val Loss: 0.036187, Train MAE: 0.147360, Val MAE: 0.162720, LR: 0.001000\n",
      "INFO:src.engine:Early stopping at epoch 39\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 22.\n",
      "INFO:src.pipeline:Fold 22 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.074200\n",
      "INFO:src.pipeline:  rmse: 0.272397\n",
      "INFO:src.pipeline:  mae: 0.153505\n",
      "INFO:src.pipeline:  r2: 0.543201\n",
      "INFO:src.pipeline:  mape: 54.145229\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 23 ===\n",
      "INFO:src.pipeline:Train samples: 680, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.029602, Val Loss: 0.038943, Train MAE: 0.169441, Val MAE: 0.207298, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.024076, Val Loss: 0.039555, Train MAE: 0.153239, Val MAE: 0.215409, LR: 0.000500\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.022878, Val Loss: 0.043065, Train MAE: 0.144620, Val MAE: 0.225480, LR: 0.000500\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.019319, Val Loss: 0.035660, Train MAE: 0.134846, Val MAE: 0.200742, LR: 0.000250\n",
      "INFO:src.engine:Epoch [50/50] - Train Loss: 0.018620, Val Loss: 0.038225, Train MAE: 0.131635, Val MAE: 0.206380, LR: 0.000250\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 23.\n",
      "INFO:src.pipeline:Fold 23 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.076451\n",
      "INFO:src.pipeline:  rmse: 0.276498\n",
      "INFO:src.pipeline:  mae: 0.206380\n",
      "INFO:src.pipeline:  r2: 0.612695\n",
      "INFO:src.pipeline:  mape: 60.132450\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 24 ===\n",
      "INFO:src.pipeline:Train samples: 711, Val samples: 21\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.029184, Val Loss: 0.059321, Train MAE: 0.171558, Val MAE: 0.258159, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.025504, Val Loss: 0.068156, Train MAE: 0.156088, Val MAE: 0.275530, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.023049, Val Loss: 0.073914, Train MAE: 0.147411, Val MAE: 0.287933, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 30\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 24.\n",
      "INFO:src.pipeline:Fold 24 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.148162\n",
      "INFO:src.pipeline:  rmse: 0.384918\n",
      "INFO:src.pipeline:  mae: 0.287933\n",
      "INFO:src.pipeline:  r2: 0.291115\n",
      "INFO:src.pipeline:  mape: 116.461517\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 25 ===\n",
      "INFO:src.pipeline:Train samples: 734, Val samples: 23\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.030355, Val Loss: 0.056458, Train MAE: 0.178783, Val MAE: 0.223382, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.026950, Val Loss: 0.059195, Train MAE: 0.161871, Val MAE: 0.228541, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 22\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 25.\n",
      "INFO:src.pipeline:Fold 25 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.120882\n",
      "INFO:src.pipeline:  rmse: 0.347681\n",
      "INFO:src.pipeline:  mae: 0.242033\n",
      "INFO:src.pipeline:  r2: 0.399151\n",
      "INFO:src.pipeline:  mape: 22.916613\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 26 ===\n",
      "INFO:src.pipeline:Train samples: 759, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.031259, Val Loss: 0.047343, Train MAE: 0.178107, Val MAE: 0.228298, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.027730, Val Loss: 0.054746, Train MAE: 0.162503, Val MAE: 0.227166, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.024748, Val Loss: 0.071817, Train MAE: 0.151450, Val MAE: 0.250043, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 30\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 26.\n",
      "INFO:src.pipeline:Fold 26 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.143860\n",
      "INFO:src.pipeline:  rmse: 0.379289\n",
      "INFO:src.pipeline:  mae: 0.250043\n",
      "INFO:src.pipeline:  r2: 0.013170\n",
      "INFO:src.pipeline:  mape: 106.275436\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 27 ===\n",
      "INFO:src.pipeline:Train samples: 790, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.032212, Val Loss: 0.034497, Train MAE: 0.179972, Val MAE: 0.156680, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.028789, Val Loss: 0.032400, Train MAE: 0.169032, Val MAE: 0.151099, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.025620, Val Loss: 0.033257, Train MAE: 0.156461, Val MAE: 0.158897, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 38\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 27.\n",
      "INFO:src.pipeline:Fold 27 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.070488\n",
      "INFO:src.pipeline:  rmse: 0.265495\n",
      "INFO:src.pipeline:  mae: 0.159138\n",
      "INFO:src.pipeline:  r2: 0.350060\n",
      "INFO:src.pipeline:  mape: 60.672714\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 28 ===\n",
      "INFO:src.pipeline:Train samples: 820, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.032517, Val Loss: 0.026590, Train MAE: 0.182505, Val MAE: 0.152828, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.030257, Val Loss: 0.026406, Train MAE: 0.172240, Val MAE: 0.167247, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.026451, Val Loss: 0.028435, Train MAE: 0.160754, Val MAE: 0.158159, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.022329, Val Loss: 0.031425, Train MAE: 0.144578, Val MAE: 0.152379, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 48\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 28.\n",
      "INFO:src.pipeline:Fold 28 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.065999\n",
      "INFO:src.pipeline:  rmse: 0.256903\n",
      "INFO:src.pipeline:  mae: 0.162492\n",
      "INFO:src.pipeline:  r2: 0.204466\n",
      "INFO:src.pipeline:  mape: 42.454018\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 29 ===\n",
      "INFO:src.pipeline:Train samples: 851, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.031448, Val Loss: 0.011600, Train MAE: 0.180403, Val MAE: 0.135821, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.027904, Val Loss: 0.014283, Train MAE: 0.167740, Val MAE: 0.145141, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 28\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 29.\n",
      "INFO:src.pipeline:Fold 29 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.022882\n",
      "INFO:src.pipeline:  rmse: 0.151269\n",
      "INFO:src.pipeline:  mae: 0.126509\n",
      "INFO:src.pipeline:  r2: 0.442710\n",
      "INFO:src.pipeline:  mape: 15.262622\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 30 ===\n",
      "INFO:src.pipeline:Train samples: 881, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.030212, Val Loss: 0.021741, Train MAE: 0.174979, Val MAE: 0.109310, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.026775, Val Loss: 0.019812, Train MAE: 0.163410, Val MAE: 0.091064, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.025486, Val Loss: 0.019844, Train MAE: 0.154771, Val MAE: 0.080703, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.023286, Val Loss: 0.019858, Train MAE: 0.149189, Val MAE: 0.079436, LR: 0.001000\n",
      "INFO:src.engine:Epoch [50/50] - Train Loss: 0.021812, Val Loss: 0.019857, Train MAE: 0.141233, Val MAE: 0.076493, LR: 0.001000\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 30.\n",
      "INFO:src.pipeline:Fold 30 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.040011\n",
      "INFO:src.pipeline:  rmse: 0.200029\n",
      "INFO:src.pipeline:  mae: 0.076493\n",
      "INFO:src.pipeline:  r2: 0.303424\n",
      "INFO:src.pipeline:  mape: 4.290060\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 31 ===\n",
      "INFO:src.pipeline:Train samples: 911, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.030223, Val Loss: 0.004239, Train MAE: 0.172812, Val MAE: 0.047787, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.027424, Val Loss: 0.006397, Train MAE: 0.161207, Val MAE: 0.084725, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.023318, Val Loss: 0.004449, Train MAE: 0.147149, Val MAE: 0.058556, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 38\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 31.\n",
      "INFO:src.pipeline:Fold 31 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.010317\n",
      "INFO:src.pipeline:  rmse: 0.101575\n",
      "INFO:src.pipeline:  mae: 0.072256\n",
      "INFO:src.pipeline:  r2: 0.734357\n",
      "INFO:src.pipeline:  mape: 9.135936\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 32 ===\n",
      "INFO:src.pipeline:Train samples: 942, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.029568, Val Loss: 0.016371, Train MAE: 0.169942, Val MAE: 0.139858, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.027192, Val Loss: 0.010563, Train MAE: 0.159530, Val MAE: 0.060020, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.025255, Val Loss: 0.014457, Train MAE: 0.152642, Val MAE: 0.099677, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.020575, Val Loss: 0.016036, Train MAE: 0.134416, Val MAE: 0.099288, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 44\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 32.\n",
      "INFO:src.pipeline:Fold 32 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.032984\n",
      "INFO:src.pipeline:  rmse: 0.181614\n",
      "INFO:src.pipeline:  mae: 0.109172\n",
      "INFO:src.pipeline:  r2: 0.439807\n",
      "INFO:src.pipeline:  mape: 13.453928\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 33 ===\n",
      "INFO:src.pipeline:Train samples: 972, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.029277, Val Loss: 0.051016, Train MAE: 0.166797, Val MAE: 0.231493, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.027548, Val Loss: 0.063681, Train MAE: 0.160065, Val MAE: 0.219237, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.023772, Val Loss: 0.065148, Train MAE: 0.144731, Val MAE: 0.230129, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 31\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 33.\n",
      "INFO:src.pipeline:Fold 33 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.143002\n",
      "INFO:src.pipeline:  rmse: 0.378156\n",
      "INFO:src.pipeline:  mae: 0.236933\n",
      "INFO:src.pipeline:  r2: 0.234017\n",
      "INFO:src.pipeline:  mape: 120.922127\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 34 ===\n",
      "INFO:src.pipeline:Train samples: 1003, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.029629, Val Loss: 0.035004, Train MAE: 0.168593, Val MAE: 0.197775, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.025972, Val Loss: 0.035033, Train MAE: 0.153805, Val MAE: 0.188470, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.023952, Val Loss: 0.037328, Train MAE: 0.145870, Val MAE: 0.209493, LR: 0.000500\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.022434, Val Loss: 0.037220, Train MAE: 0.141654, Val MAE: 0.194459, LR: 0.000500\n",
      "INFO:src.engine:Epoch [50/50] - Train Loss: 0.019390, Val Loss: 0.044755, Train MAE: 0.130324, Val MAE: 0.216068, LR: 0.000250\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 34.\n",
      "INFO:src.pipeline:Fold 34 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.089777\n",
      "INFO:src.pipeline:  rmse: 0.299629\n",
      "INFO:src.pipeline:  mae: 0.216068\n",
      "INFO:src.pipeline:  r2: 0.483180\n",
      "INFO:src.pipeline:  mape: 52.878044\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 35 ===\n",
      "INFO:src.pipeline:Train samples: 1033, Val samples: 21\n",
      "INFO:src.pipeline:Model parameters: 274,059\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.029735, Val Loss: 0.068609, Train MAE: 0.170992, Val MAE: 0.295495, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.027835, Val Loss: 0.059811, Train MAE: 0.160628, Val MAE: 0.274515, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.024753, Val Loss: 0.067594, Train MAE: 0.150997, Val MAE: 0.307640, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.023041, Val Loss: 0.078587, Train MAE: 0.141076, Val MAE: 0.319353, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 41\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 35.\n",
      "INFO:src.pipeline:Fold 35 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.145530\n",
      "INFO:src.pipeline:  rmse: 0.381483\n",
      "INFO:src.pipeline:  mae: 0.284352\n",
      "INFO:src.pipeline:  r2: 0.398958\n",
      "INFO:src.pipeline:  mape: 69.868317\n",
      "INFO:src.pipeline:\n",
      "=== Experiment Summary ===\n",
      "INFO:src.pipeline:Average Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.079397 ± 0.055498\n",
      "INFO:src.pipeline:  rmse: 0.267374 ± 0.088930\n",
      "INFO:src.pipeline:  mae: 0.189738 ± 0.083971\n",
      "INFO:src.pipeline:  r2: 0.285344 ± 0.386973\n",
      "INFO:src.pipeline:  mape: 44.750573 ± 31.478089\n"
     ]
    }
   ],
   "source": [
    "from src.preprocessing import KBinConfig, process_splits_to_kbins, build_model_arrays, to_fixedgrid_multiindex\n",
    "import pandas as pd\n",
    "from src.utils import DataManager\n",
    "from src.pipeline import SolarForecastingPipeline\n",
    "\n",
    "# --- Block 1: Define variables ---\n",
    "USE_KBINS = False\n",
    "k_bins = 60\n",
    "TARGET_COL = \"CSI_ghi\"  \n",
    "history_days = 7\n",
    "horizon_days = 1\n",
    "\n",
    "# This list now correctly excludes the target\n",
    "feature_cols = [c for c in df_phase2.columns.tolist() if c != TARGET_COL]\n",
    "\n",
    "# --- Block 2: Build model arrays (Your code was correct here) ---\n",
    "if USE_KBINS:\n",
    "        print(\"\\n--- Step 2: Converting full dataset to K-Bins format ---\")\n",
    "        # ... (rest of your K-Bins code) ...\n",
    "else:\n",
    "    print(\"\\n--- Step 2: Building model arrays (X, Y) ---\")\n",
    "    fixed_df = to_fixedgrid_multiindex(df_phase2, timestamp_col=\"measurement_time\", expected_T=None)  # or set T\n",
    "    \n",
    "    # You correctly passed the filtered 'feature_cols' here, fixing the data leak!\n",
    "    ph2_X, ph2_Y, ph2_labels_list = build_model_arrays(\n",
    "        fixed_df,\n",
    "        feature_cols=feature_cols,  # <-- This is correct!\n",
    "        target_col=TARGET_COL,\n",
    "        history_days=history_days,\n",
    "        horizon_days=horizon_days,\n",
    "    )\n",
    "\n",
    "# --- Block 3: Save arrays ---\n",
    "data_manager = DataManager()\n",
    "data_manager.save_arrays(\n",
    "    ph2_X, ph2_Y,\n",
    "    pd.DataFrame(index=pd.to_datetime(ph2_labels_list, utc=True)),\n",
    "    filename_prefix='phas2_data',  # <-- You are saving as 'phas2_data'\n",
    "    feature_cols=feature_cols,\n",
    "    target_col=TARGET_COL,\n",
    "    metadata={\n",
    "        \"input_csv\": \"data/processed/df_1h_lag_BLV_spatial_images.csv\",\n",
    "        \"timestamp_col\": \"measurement_time\",\n",
    "        \"feature_set\": feature_cols,\n",
    "        \"history_days\": 7,\n",
    "        \"horizon_days\": 1,\n",
    "        \"k_bins\": None,\n",
    "    }\n",
    ")\n",
    "\n",
    "# --- Block 4: Configure and Run ---\n",
    "LSTM_CONFIG = {\n",
    "    \"experiment_name\": \"UniLSTM_p2_exp01_BLVF\",\n",
    "    \"model_type\": \"LSTM\",\n",
    "    \"model_config\": {\n",
    "        \"hidden_size\": 64,\n",
    "        \"num_layers\": 2,\n",
    "        \"dropout\": 0.35,\n",
    "        \"bidirectional\": True,\n",
    "    },\n",
    "    \"data_prefix\": \"phas2_data\",  # <-- FIX 2: Match the filename_prefix\n",
    "    \"splits_file\": \"exp-004/exp-004rolling_origin_splits.json\",\n",
    "    \"feature_cols\": feature_cols,\n",
    "    \"feature_selection\": feature_cols, # <-- This is correct!\n",
    "    \"target_col\": TARGET_COL,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_epochs\": 50,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"loss_function\": \"Huber\",  # <-- This setting needs the fix below\n",
    "    \"early_stopping_patience\": 20,\n",
    "    \"max_folds\": 35,\n",
    "}\n",
    "\n",
    "# This will now work after you apply the fixes below\n",
    "pipeline = SolarForecastingPipeline(LSTM_CONFIG)\n",
    "_, summary = pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf41c240",
   "metadata": {},
   "source": [
    "Increasing the Hisory days "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb883c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocessing import KBinConfig, process_splits_to_kbins, build_model_arrays, to_fixedgrid_multiindex\n",
    "import pandas as pd\n",
    "from src.utils import DataManager\n",
    "from src.pipeline import SolarForecastingPipeline\n",
    "\n",
    "# --- Block 1: Define variables ---\n",
    "USE_KBINS = False\n",
    "k_bins = 60\n",
    "TARGET_COL = \"CSI_ghi\"  \n",
    "history_days = 7\n",
    "horizon_days = 1\n",
    "\n",
    "# This list now correctly excludes the target\n",
    "feature_cols = [c for c in df_phase2.columns.tolist() if c != TARGET_COL]\n",
    "\n",
    "# --- Block 2: Build model arrays (Your code was correct here) ---\n",
    "if USE_KBINS:\n",
    "        print(\"\\n--- Step 2: Converting full dataset to K-Bins format ---\")\n",
    "        # ... (rest of your K-Bins code) ...\n",
    "else:\n",
    "    print(\"\\n--- Step 2: Building model arrays (X, Y) ---\")\n",
    "    fixed_df = to_fixedgrid_multiindex(df_phase2, timestamp_col=\"measurement_time\", expected_T=None)  # or set T\n",
    "    \n",
    "    # You correctly passed the filtered 'feature_cols' here, fixing the data leak!\n",
    "    ph2_X, ph2_Y, ph2_labels_list = build_model_arrays(\n",
    "        fixed_df,\n",
    "        feature_cols=feature_cols,  # <-- This is correct!\n",
    "        target_col=TARGET_COL,\n",
    "        history_days=history_days,\n",
    "        horizon_days=horizon_days,\n",
    "    )\n",
    "\n",
    "# --- Block 3: Save arrays ---\n",
    "data_manager = DataManager()\n",
    "data_manager.save_arrays(\n",
    "    ph2_X, ph2_Y,\n",
    "    pd.DataFrame(index=pd.to_datetime(ph2_labels_list, utc=True)),\n",
    "    filename_prefix='phas2_data',  # <-- You are saving as 'phas2_data'\n",
    "    feature_cols=feature_cols,\n",
    "    target_col=TARGET_COL,\n",
    "    metadata={\n",
    "        \"input_csv\": \"data/processed/df_1h_lag_BLV_spatial_images.csv\",\n",
    "        \"timestamp_col\": \"measurement_time\",\n",
    "        \"feature_set\": feature_cols,\n",
    "        \"history_days\": 7,\n",
    "        \"horizon_days\": 1,\n",
    "        \"k_bins\": None,\n",
    "    }\n",
    ")\n",
    "\n",
    "# --- Block 4: Configure and Run ---\n",
    "LSTM_CONFIG = {\n",
    "    \"experiment_name\": \"UniLSTM_p2_exp01_BLVF\",\n",
    "    \"model_type\": \"LSTM\",\n",
    "    \"model_config\": {\n",
    "        \"hidden_size\": 64,\n",
    "        \"num_layers\": 2,\n",
    "        \"dropout\": 0.35,\n",
    "        \"bidirectional\": True,\n",
    "    },\n",
    "    \"data_prefix\": \"phas2_data\",  # <-- FIX 2: Match the filename_prefix\n",
    "    \"splits_file\": \"exp-004/exp-004rolling_origin_splits.json\",\n",
    "    \"feature_cols\": feature_cols,\n",
    "    \"feature_selection\": feature_cols, # <-- This is correct!\n",
    "    \"target_col\": TARGET_COL,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_epochs\": 50,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"loss_function\": \"Huber\",  # <-- This setting needs the fix below\n",
    "    \"early_stopping_patience\": 20,\n",
    "    \"max_folds\": 35,\n",
    "}\n",
    "\n",
    "# This will now work after you apply the fixes below\n",
    "pipeline = SolarForecastingPipeline(LSTM_CONFIG)\n",
    "_, summary = pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e87f38",
   "metadata": {},
   "source": [
    "•\tDuring training, the model shows decreasing loss and MAE on the training set, which is expected as the model learns the underlying patterns. However, the validation loss and MAE do not always decrease at the same rate, indicating some fluctuation in generalization.\n",
    "•\tEarly Stopping: Many of the folds use early stopping, indicating that the model reached a point where further training did not result in significant improvements or even caused overfitting.\n",
    "\n",
    "the model’s performance can fluctuate depending on the fold. This suggests potential instability in model predictions for some subsets of the data.\n",
    "\n",
    "CSI metrics show that the model consistently outperforms the NAM model.\n",
    "\n",
    "The performance drops on validation while the training loss improves, which suggests that the model is memorizing the training data instead of generalizing well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b087680",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d755b25b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71736538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 11,561 records\n",
      "Date range: 2014-01-03 14:00:00+00:00 to 2016-12-31 02:00:00+00:00\n",
      "Timezone: UTC\n"
     ]
    }
   ],
   "source": [
    "from src.data_preparation import load_data\n",
    "PROCESSED_DATA_PATH = \"data/processed/dayTime_NAM_spatial_5locations_dayahead_features_processed.csv\"\n",
    "df = load_data(PROCESSED_DATA_PATH, date_col=\"measurement_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fbd69e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'ghi', 'dni', 'dhi', 'solar_zenith', 'time_gap_hours',\n",
       "       'time_gap_norm', 'day_boundary_flag', 'hour_progression',\n",
       "       'absolute_hour', 'GHI_cs', 'DNI_cs', 'CSI_ghi', 'CSI_dni',\n",
       "       'season_flag', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos',\n",
       "       'issue_time', 'target_time', 'horizon', 'nam_ghi', 'nam_dni', 'nam_cc',\n",
       "       'nam_target_time', '80_dwsw', 'valtime', '80_cloud_cover', '56_dwsw',\n",
       "       '56_cloud_cover', '20_dwsw', '20_cloud_cover', '88_dwsw',\n",
       "       '88_cloud_cover'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91c68df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ROLLING ORIGIN EVALUATION\n",
      "============================================================\n",
      "Total folds: 35\n",
      "Frequency: MS\n",
      "Data range: 2014-01-03 to 2016-12-31\n",
      "\n",
      "Fold Summary:\n",
      "  Fold 1: Train [2014-01-03 to 2014-02-01] (297 records) → Val [2014-02-02 to 2014-02-28] (186 records)\n",
      "  Fold 2: Train [2014-01-03 to 2014-03-01] (494 records) → Val [2014-03-02 to 2014-03-31] (319 records)\n",
      "  Fold 3: Train [2014-01-03 to 2014-04-01] (835 records) → Val [2014-04-02 to 2014-04-30] (308 records)\n",
      "  Fold 4: Train [2014-01-03 to 2014-05-01] (1,165 records) → Val [2014-05-02 to 2014-05-31] (319 records)\n",
      "  Fold 5: Train [2014-01-03 to 2014-06-01] (1,506 records) → Val [2014-06-02 to 2014-06-30] (308 records)\n",
      "  Fold 6: Train [2014-01-03 to 2014-07-01] (1,836 records) → Val [2014-07-02 to 2014-07-31] (319 records)\n",
      "  Fold 7: Train [2014-01-03 to 2014-08-01] (2,177 records) → Val [2014-08-02 to 2014-08-31] (319 records)\n",
      "  Fold 8: Train [2014-01-03 to 2014-09-01] (2,518 records) → Val [2014-09-02 to 2014-09-30] (308 records)\n",
      "  Fold 9: Train [2014-01-03 to 2014-10-01] (2,848 records) → Val [2014-10-02 to 2014-10-31] (319 records)\n",
      "  Fold 10: Train [2014-01-03 to 2014-11-01] (3,189 records) → Val [2014-11-02 to 2014-11-30] (308 records)\n",
      "  Fold 11: Train [2014-01-03 to 2014-12-01] (3,519 records) → Val [2014-12-02 to 2014-12-31] (319 records)\n",
      "  Fold 12: Train [2014-01-03 to 2015-01-01] (3,860 records) → Val [2015-01-02 to 2015-01-31] (319 records)\n",
      "  Fold 13: Train [2014-01-03 to 2015-02-01] (4,201 records) → Val [2015-02-02 to 2015-02-28] (286 records)\n",
      "  Fold 14: Train [2014-01-03 to 2015-03-01] (4,509 records) → Val [2015-03-02 to 2015-03-31] (319 records)\n",
      "  Fold 15: Train [2014-01-03 to 2015-04-01] (4,850 records) → Val [2015-04-02 to 2015-04-30] (308 records)\n",
      "  Fold 16: Train [2014-01-03 to 2015-05-01] (5,180 records) → Val [2015-05-02 to 2015-05-31] (319 records)\n",
      "  Fold 17: Train [2014-01-03 to 2015-06-01] (5,521 records) → Val [2015-06-02 to 2015-06-30] (308 records)\n",
      "  Fold 18: Train [2014-01-03 to 2015-07-01] (5,851 records) → Val [2015-07-02 to 2015-07-31] (319 records)\n",
      "  Fold 19: Train [2014-01-03 to 2015-08-01] (6,192 records) → Val [2015-08-02 to 2015-08-31] (308 records)\n",
      "  Fold 20: Train [2014-01-03 to 2015-09-01] (6,522 records) → Val [2015-09-02 to 2015-09-30] (308 records)\n",
      "  Fold 21: Train [2014-01-03 to 2015-10-01] (6,852 records) → Val [2015-10-02 to 2015-10-31] (319 records)\n",
      "  Fold 22: Train [2014-01-03 to 2015-11-01] (7,193 records) → Val [2015-11-02 to 2015-11-30] (308 records)\n",
      "  Fold 23: Train [2014-01-03 to 2015-12-01] (7,523 records) → Val [2015-12-02 to 2015-12-31] (319 records)\n",
      "  Fold 24: Train [2014-01-03 to 2016-01-01] (7,854 records) → Val [2016-01-02 to 2016-01-31] (220 records)\n",
      "  Fold 25: Train [2014-01-03 to 2016-02-01] (8,095 records) → Val [2016-02-02 to 2016-02-29] (231 records)\n",
      "  Fold 26: Train [2014-01-03 to 2016-03-01] (8,348 records) → Val [2016-03-02 to 2016-03-31] (319 records)\n",
      "  Fold 27: Train [2014-01-03 to 2016-04-01] (8,689 records) → Val [2016-04-02 to 2016-04-30] (308 records)\n",
      "  Fold 28: Train [2014-01-03 to 2016-05-01] (9,019 records) → Val [2016-05-02 to 2016-05-31] (319 records)\n",
      "  Fold 29: Train [2014-01-03 to 2016-06-01] (9,360 records) → Val [2016-06-02 to 2016-06-30] (308 records)\n",
      "  Fold 30: Train [2014-01-03 to 2016-07-01] (9,690 records) → Val [2016-07-02 to 2016-07-31] (297 records)\n",
      "  Fold 31: Train [2014-01-03 to 2016-08-01] (10,009 records) → Val [2016-08-02 to 2016-08-31] (319 records)\n",
      "  Fold 32: Train [2014-01-03 to 2016-09-01] (10,350 records) → Val [2016-09-02 to 2016-09-30] (308 records)\n",
      "  Fold 33: Train [2014-01-03 to 2016-10-01] (10,680 records) → Val [2016-10-02 to 2016-10-31] (319 records)\n",
      "  Fold 34: Train [2014-01-03 to 2016-11-01] (11,021 records) → Val [2016-11-02 to 2016-11-30] (308 records)\n",
      "  Fold 35: Train [2014-01-03 to 2016-12-01] (11,351 records) → Val [2016-12-02 to 2016-12-31] (208 records)\n",
      "============================================================\n",
      "\n",
      "Split metadata saved to 'splits/exp-003/' directory\n"
     ]
    }
   ],
   "source": [
    "# 2. Rolling Origin Split (More Suitable in TimeSeries) like K-Folds\n",
    "from src.data_preparation import rolling_origin_evaluation,save_splits_info\n",
    "\n",
    "rollingSplits_df_5locations = rolling_origin_evaluation(df=df, start_train = '2014-01-2',\n",
    "    end_train = '2016-12-31')\n",
    "save_splits_info({}, rollingSplits_df_5locations, experiment_name=\"exp-003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "513156ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does NAM records have a 1-hour lag ahead from measurements? Yes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_noIndex = df.reset_index()\n",
    "df_noIndex['nam_target_time'] = pd.to_datetime(df_noIndex['nam_target_time'])\n",
    "df_noIndex['measurement_time'] = pd.to_datetime(df_noIndex['measurement_time'])\n",
    "df_noIndex['time_diff'] = (df_noIndex['nam_target_time'] - df_noIndex['measurement_time']).dt.total_seconds() / 3600  # Calculate the time difference in hours\n",
    "    \n",
    "# Check if the time difference is exactly 1 hour\n",
    "is_nam_lag_correct = np.allclose(df_noIndex['time_diff'], 1)  # All time differences should be 1 hour\n",
    "print(f\"Does NAM records have a 1-hour lag ahead from measurements? {'Yes' if is_nam_lag_correct else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "349153c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do we have NaN values in features? Yes\n",
      "--- Features With NaN Values ---\n",
      "The following features have NaN values, with the total count for each:\n",
      "CSI_dni    788\n",
      "CSI_ghi    639\n",
      "dtype: int64\n",
      "\n",
      "========================================\n",
      "\n",
      "--- Distribution of NaN-Value Records by Hour ---\n",
      "Distribution of records (rows) containing at least one NaN, by hour:\n",
      "measurement_time\n",
      "2     449\n",
      "14    313\n",
      "15     26\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total number of rows with at least one NaN: 788\n"
     ]
    }
   ],
   "source": [
    "# 3. Check if there are zero values in any of the features\n",
    "zero_values_in_features = (df[['ghi', 'dni', 'solar_zenith', 'GHI_cs', 'DNI_cs', 'CSI_ghi', \n",
    "                                   'CSI_dni',\n",
    "                                   'nam_ghi', 'nam_dni', 'nam_cc']] == 0).sum().sum()\n",
    "print(f\"Do we have NaN values in features? {'Yes' if zero_values_in_features > 0 else 'No'}\")\n",
    "\n",
    "\n",
    "features_to_check = ['ghi', 'dni', 'solar_zenith', 'GHI_cs', 'DNI_cs', 'CSI_ghi', \n",
    "                                   'CSI_dni',\n",
    "                                   'nam_ghi', 'nam_dni', 'nam_cc']\n",
    "\n",
    "# 2. Calculate the number of NaN values for each feature\n",
    "# We use .isna() instead of == 0\n",
    "nans_per_feature = df[features_to_check].isna().sum()\n",
    "\n",
    "# 3. Filter to get only features that actually have NaN values\n",
    "features_with_nans = nans_per_feature[nans_per_feature > 0]\n",
    "\n",
    "# 4. Report the findings for which features have NaNs\n",
    "if features_with_nans.empty:\n",
    "    print(\"No NaN (missing) values found in any of the specified features.\")\n",
    "else:\n",
    "    print(\"--- Features With NaN Values ---\")\n",
    "    print(\"The following features have NaN values, with the total count for each:\")\n",
    "    # Sort for clearer output\n",
    "    print(features_with_nans.sort_values(ascending=False))\n",
    "    print(\"\\n\" + \"=\"*40 + \"\\n\")\n",
    "\n",
    "    # 5. Analyze the distribution of hours for rows containing NaNs\n",
    "    print(\"--- Distribution of NaN-Value Records by Hour ---\")\n",
    "    \n",
    "    # Create a boolean mask for rows that contain *at least one* NaN\n",
    "    # in the specified columns\n",
    "    rows_with_any_nan = df[features_to_check].isna().any(axis=1)\n",
    "    \n",
    "    if rows_with_any_nan.sum() > 0:\n",
    "        # Get the index for these rows\n",
    "        nan_rows_index = df.index[rows_with_any_nan]\n",
    "        \n",
    "        # Extract the hour from the DatetimeIndex and get the value counts\n",
    "        hour_distribution = nan_rows_index.hour.value_counts().sort_index()\n",
    "        \n",
    "        print(\"Distribution of records (rows) containing at least one NaN, by hour:\")\n",
    "        print(hour_distribution)\n",
    "        \n",
    "        # Optional: Print total number of affected rows\n",
    "        print(f\"\\nTotal number of rows with at least one NaN: {rows_with_any_nan.sum()}\")\n",
    "    else:\n",
    "        # This case shouldn't be hit if features_with_nans was not empty,\n",
    "        # but it's good practice to include.\n",
    "        print(\"No rows found with NaN values (this is unexpected, check logic).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b082d6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1694326/931910069.py:1: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df_interpolated = df.interpolate(method='linear')\n"
     ]
    }
   ],
   "source": [
    "df_interpolated = df.interpolate(method='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b859051c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do we have NaN values in features? Yes\n",
      "--- Features With NaN Values ---\n",
      "The following features have NaN values, with the total count for each:\n",
      "CSI_ghi    1\n",
      "CSI_dni    1\n",
      "dtype: int64\n",
      "\n",
      "========================================\n",
      "\n",
      "--- Distribution of NaN-Value Records by Hour ---\n",
      "Distribution of records (rows) containing at least one NaN, by hour:\n",
      "measurement_time\n",
      "14    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total number of rows with at least one NaN: 1\n"
     ]
    }
   ],
   "source": [
    "# 3. Check if there are zero values in any of the features\n",
    "zero_values_in_features = (df_interpolated[['ghi', 'dni', 'solar_zenith', 'GHI_cs', 'DNI_cs', 'CSI_ghi', \n",
    "                                   'CSI_dni',\n",
    "                                   'nam_ghi', 'nam_dni', 'nam_cc']] == 0).sum().sum()\n",
    "print(f\"Do we have NaN values in features? {'Yes' if zero_values_in_features > 0 else 'No'}\")\n",
    "\n",
    "\n",
    "features_to_check = ['ghi', 'dni', 'solar_zenith', 'GHI_cs', 'DNI_cs', 'CSI_ghi', \n",
    "                                   'CSI_dni',\n",
    "                                   'nam_ghi', 'nam_dni', 'nam_cc']\n",
    "\n",
    "# 2. Calculate the number of NaN values for each feature\n",
    "# We use .isna() instead of == 0\n",
    "nans_per_feature = df_interpolated[features_to_check].isna().sum()\n",
    "\n",
    "# 3. Filter to get only features that actually have NaN values\n",
    "features_with_nans = nans_per_feature[nans_per_feature > 0]\n",
    "\n",
    "# 4. Report the findings for which features have NaNs\n",
    "if features_with_nans.empty:\n",
    "    print(\"No NaN (missing) values found in any of the specified features.\")\n",
    "else:\n",
    "    print(\"--- Features With NaN Values ---\")\n",
    "    print(\"The following features have NaN values, with the total count for each:\")\n",
    "    # Sort for clearer output\n",
    "    print(features_with_nans.sort_values(ascending=False))\n",
    "    print(\"\\n\" + \"=\"*40 + \"\\n\")\n",
    "\n",
    "    # 5. Analyze the distribution of hours for rows containing NaNs\n",
    "    print(\"--- Distribution of NaN-Value Records by Hour ---\")\n",
    "    \n",
    "    # Create a boolean mask for rows that contain *at least one* NaN\n",
    "    # in the specified columns\n",
    "    rows_with_any_nan = df_interpolated[features_to_check].isna().any(axis=1)\n",
    "    \n",
    "    if rows_with_any_nan.sum() > 0:\n",
    "        # Get the index for these rows\n",
    "        nan_rows_index = df_interpolated.index[rows_with_any_nan]\n",
    "        \n",
    "        # Extract the hour from the DatetimeIndex and get the value counts\n",
    "        hour_distribution = nan_rows_index.hour.value_counts().sort_index()\n",
    "        \n",
    "        print(\"Distribution of records (rows) containing at least one NaN, by hour:\")\n",
    "        print(hour_distribution)\n",
    "        \n",
    "        # Optional: Print total number of affected rows\n",
    "        print(f\"\\nTotal number of rows with at least one NaN: {rows_with_any_nan.sum()}\")\n",
    "    else:\n",
    "        # This case shouldn't be hit if features_with_nans was not empty,\n",
    "        # but it's good practice to include.\n",
    "        print(\"No rows found with NaN values (this is unexpected, check logic).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88ecccde",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_interpolated.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efacfc8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'ghi', 'dni', 'dhi', 'solar_zenith', 'time_gap_hours',\n",
       "       'time_gap_norm', 'day_boundary_flag', 'hour_progression',\n",
       "       'absolute_hour', 'GHI_cs', 'DNI_cs', 'CSI_ghi', 'CSI_dni',\n",
       "       'season_flag', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos',\n",
       "       'issue_time', 'target_time', 'horizon', 'nam_ghi', 'nam_dni', 'nam_cc',\n",
       "       'nam_target_time', '80_dwsw', 'valtime', '80_cloud_cover', '56_dwsw',\n",
       "       '56_cloud_cover', '20_dwsw', '20_cloud_cover', '88_dwsw',\n",
       "       '88_cloud_cover'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_interpolated.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7904cd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_phase3 = df_interpolated[['CSI_ghi', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos',\n",
    "    'nam_ghi', 'nam_dni', 'nam_cc',\n",
    "        '80_dwsw', '80_cloud_cover', '56_dwsw',\n",
    "       '56_cloud_cover', '20_dwsw', '20_cloud_cover', '88_dwsw',\n",
    "       '88_cloud_cover']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c5bb7ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CSI_ghi</th>\n",
       "      <th>hour_sin</th>\n",
       "      <th>hour_cos</th>\n",
       "      <th>month_sin</th>\n",
       "      <th>month_cos</th>\n",
       "      <th>nam_ghi</th>\n",
       "      <th>nam_dni</th>\n",
       "      <th>nam_cc</th>\n",
       "      <th>80_dwsw</th>\n",
       "      <th>80_cloud_cover</th>\n",
       "      <th>56_dwsw</th>\n",
       "      <th>56_cloud_cover</th>\n",
       "      <th>20_dwsw</th>\n",
       "      <th>20_cloud_cover</th>\n",
       "      <th>88_dwsw</th>\n",
       "      <th>88_cloud_cover</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>measurement_time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-01-03 15:00:00+00:00</th>\n",
       "      <td>1.200000</td>\n",
       "      <td>-0.707107</td>\n",
       "      <td>-7.071068e-01</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>99.625</td>\n",
       "      <td>497.069277</td>\n",
       "      <td>2.0</td>\n",
       "      <td>99.625</td>\n",
       "      <td>2.0</td>\n",
       "      <td>97.375</td>\n",
       "      <td>2.0</td>\n",
       "      <td>95.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>102.000</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-03 16:00:00+00:00</th>\n",
       "      <td>1.200000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-5.000000e-01</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>274.750</td>\n",
       "      <td>760.162689</td>\n",
       "      <td>0.0</td>\n",
       "      <td>274.750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>273.375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>274.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>279.000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-03 17:00:00+00:00</th>\n",
       "      <td>1.200000</td>\n",
       "      <td>-0.965926</td>\n",
       "      <td>-2.588190e-01</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>416.375</td>\n",
       "      <td>871.248807</td>\n",
       "      <td>2.0</td>\n",
       "      <td>416.375</td>\n",
       "      <td>2.0</td>\n",
       "      <td>411.500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>417.500</td>\n",
       "      <td>4.0</td>\n",
       "      <td>410.375</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-03 18:00:00+00:00</th>\n",
       "      <td>1.200000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.836970e-16</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>510.875</td>\n",
       "      <td>919.980698</td>\n",
       "      <td>8.0</td>\n",
       "      <td>510.875</td>\n",
       "      <td>8.0</td>\n",
       "      <td>508.125</td>\n",
       "      <td>4.0</td>\n",
       "      <td>509.875</td>\n",
       "      <td>12.0</td>\n",
       "      <td>508.750</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-03 19:00:00+00:00</th>\n",
       "      <td>1.126874</td>\n",
       "      <td>-0.965926</td>\n",
       "      <td>2.588190e-01</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>547.375</td>\n",
       "      <td>935.728503</td>\n",
       "      <td>20.0</td>\n",
       "      <td>547.375</td>\n",
       "      <td>20.0</td>\n",
       "      <td>545.375</td>\n",
       "      <td>18.0</td>\n",
       "      <td>546.750</td>\n",
       "      <td>22.0</td>\n",
       "      <td>546.000</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            CSI_ghi  hour_sin      hour_cos  month_sin  \\\n",
       "measurement_time                                                         \n",
       "2014-01-03 15:00:00+00:00  1.200000 -0.707107 -7.071068e-01        0.5   \n",
       "2014-01-03 16:00:00+00:00  1.200000 -0.866025 -5.000000e-01        0.5   \n",
       "2014-01-03 17:00:00+00:00  1.200000 -0.965926 -2.588190e-01        0.5   \n",
       "2014-01-03 18:00:00+00:00  1.200000 -1.000000 -1.836970e-16        0.5   \n",
       "2014-01-03 19:00:00+00:00  1.126874 -0.965926  2.588190e-01        0.5   \n",
       "\n",
       "                           month_cos  nam_ghi     nam_dni  nam_cc  80_dwsw  \\\n",
       "measurement_time                                                             \n",
       "2014-01-03 15:00:00+00:00   0.866025   99.625  497.069277     2.0   99.625   \n",
       "2014-01-03 16:00:00+00:00   0.866025  274.750  760.162689     0.0  274.750   \n",
       "2014-01-03 17:00:00+00:00   0.866025  416.375  871.248807     2.0  416.375   \n",
       "2014-01-03 18:00:00+00:00   0.866025  510.875  919.980698     8.0  510.875   \n",
       "2014-01-03 19:00:00+00:00   0.866025  547.375  935.728503    20.0  547.375   \n",
       "\n",
       "                           80_cloud_cover  56_dwsw  56_cloud_cover  20_dwsw  \\\n",
       "measurement_time                                                              \n",
       "2014-01-03 15:00:00+00:00             2.0   97.375             2.0   95.000   \n",
       "2014-01-03 16:00:00+00:00             0.0  273.375             0.0  274.125   \n",
       "2014-01-03 17:00:00+00:00             2.0  411.500             0.0  417.500   \n",
       "2014-01-03 18:00:00+00:00             8.0  508.125             4.0  509.875   \n",
       "2014-01-03 19:00:00+00:00            20.0  545.375            18.0  546.750   \n",
       "\n",
       "                           20_cloud_cover  88_dwsw  88_cloud_cover  \n",
       "measurement_time                                                    \n",
       "2014-01-03 15:00:00+00:00             0.0  102.000             6.0  \n",
       "2014-01-03 16:00:00+00:00             0.0  279.000             2.0  \n",
       "2014-01-03 17:00:00+00:00             4.0  410.375             0.0  \n",
       "2014-01-03 18:00:00+00:00            12.0  508.750             2.0  \n",
       "2014-01-03 19:00:00+00:00            22.0  546.000            16.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_phase3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b85e67c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.utils:Saved arrays to data/phas3_data_*.npy\n",
      "INFO:src.utils:X shape: (1056, 7, 11, 15), Y shape: (1056, 1, 11)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 2: Building model arrays (X, Y) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.pipeline:Loading data...\n",
      "INFO:src.utils:Loaded arrays from data/phas3_data_*.npy\n",
      "INFO:src.utils:X shape: (1056, 7, 11, 15), Y shape: (1056, 1, 11)\n",
      "INFO:src.pipeline:Building FIXED-GRID reference frame (K_bins is None)...\n",
      "WARNING:src.pipeline:Failed to construct reference dataframe for NAM comparison: unsupported operand type(s) for -: 'str' and 'int'\n",
      "INFO:src.utils:Loaded 35 folds from exp-003/exp-003rolling_origin_splits.json\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 1 ===\n",
      "INFO:src.pipeline:Train samples: 21, Val samples: 18\n",
      "INFO:src.pipeline:Model parameters: 251,531\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.048390, Val Loss: 0.080466, Train MAE: 0.222599, Val MAE: 0.313158, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.032025, Val Loss: 0.061464, Train MAE: 0.179580, Val MAE: 0.289278, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.026566, Val Loss: 0.061203, Train MAE: 0.152212, Val MAE: 0.273124, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.021560, Val Loss: 0.089690, Train MAE: 0.133791, Val MAE: 0.317289, LR: 0.001000\n",
      "INFO:src.engine:Epoch [50/50] - Train Loss: 0.018941, Val Loss: 0.070745, Train MAE: 0.141067, Val MAE: 0.294176, LR: 0.000500\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 1.\n",
      "INFO:src.pipeline:Fold 1 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.141709\n",
      "INFO:src.pipeline:  rmse: 0.376442\n",
      "INFO:src.pipeline:  mae: 0.294176\n",
      "INFO:src.pipeline:  r2: 0.143852\n",
      "INFO:src.pipeline:  mape: 68.729362\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 2 ===\n",
      "INFO:src.pipeline:Train samples: 40, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 251,531\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.050087, Val Loss: 0.064349, Train MAE: 0.245055, Val MAE: 0.325290, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.027376, Val Loss: 0.056567, Train MAE: 0.166803, Val MAE: 0.297149, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 24\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 2.\n",
      "INFO:src.pipeline:Fold 2 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.189303\n",
      "INFO:src.pipeline:  rmse: 0.435090\n",
      "INFO:src.pipeline:  mae: 0.381988\n",
      "INFO:src.pipeline:  r2: -0.701853\n",
      "INFO:src.pipeline:  mape: 59.862755\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 3 ===\n",
      "INFO:src.pipeline:Train samples: 71, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 251,531\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.043147, Val Loss: 0.028591, Train MAE: 0.229222, Val MAE: 0.196501, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.037269, Val Loss: 0.026207, Train MAE: 0.196465, Val MAE: 0.172200, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.027142, Val Loss: 0.022451, Train MAE: 0.165013, Val MAE: 0.150626, LR: 0.000500\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.025728, Val Loss: 0.023518, Train MAE: 0.160141, Val MAE: 0.156707, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 49\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 3.\n",
      "INFO:src.pipeline:Fold 3 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.047856\n",
      "INFO:src.pipeline:  rmse: 0.218761\n",
      "INFO:src.pipeline:  mae: 0.159614\n",
      "INFO:src.pipeline:  r2: 0.423269\n",
      "INFO:src.pipeline:  mape: 33.589500\n",
      "INFO:src.pipeline:\n",
      "=== Experiment Summary ===\n",
      "INFO:src.pipeline:Average Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.126289 ± 0.058766\n",
      "INFO:src.pipeline:  rmse: 0.343431 ± 0.091349\n",
      "INFO:src.pipeline:  mae: 0.278593 ± 0.091450\n",
      "INFO:src.pipeline:  r2: -0.044911 ± 0.478329\n",
      "INFO:src.pipeline:  mape: 54.060539 ± 14.920942\n"
     ]
    }
   ],
   "source": [
    "from src.preprocessing import KBinConfig, process_splits_to_kbins, build_model_arrays, to_fixedgrid_multiindex\n",
    "import pandas as pd\n",
    "from src.utils import DataManager\n",
    "from src.pipeline import SolarForecastingPipeline\n",
    "\n",
    "# --- Block 1: Define variables ---\n",
    "USE_KBINS = False\n",
    "k_bins = 60\n",
    "TARGET_COL = \"CSI_ghi\"  # <-- FIX 1: Define Target FIRST\n",
    "history_days = 7\n",
    "horizon_days = 1\n",
    "\n",
    "# This list now correctly excludes the target\n",
    "feature_cols = [c for c in df_phase3.columns.tolist() if c != TARGET_COL]\n",
    "\n",
    "# --- Block 2: Build model arrays (Your code was correct here) ---\n",
    "if USE_KBINS:\n",
    "        print(\"\\n--- Step 2: Converting full dataset to K-Bins format ---\")\n",
    "        # ... (rest of your K-Bins code) ...\n",
    "else:\n",
    "    print(\"\\n--- Step 2: Building model arrays (X, Y) ---\")\n",
    "    fixed_df = to_fixedgrid_multiindex(df_phase3, timestamp_col=\"measurement_time\", expected_T=None)  # or set T\n",
    "    \n",
    "    # You correctly passed the filtered 'feature_cols' here, fixing the data leak!\n",
    "    ph3_X, ph3_Y, ph3_labels_list = build_model_arrays(\n",
    "        fixed_df,\n",
    "        feature_cols=feature_cols,  # <-- This is correct!\n",
    "        target_col=TARGET_COL,\n",
    "        history_days=history_days,\n",
    "        horizon_days=horizon_days,\n",
    "    )\n",
    "\n",
    "# --- Block 3: Save arrays ---\n",
    "data_manager = DataManager()\n",
    "data_manager.save_arrays(\n",
    "    ph3_X, ph3_Y,\n",
    "    pd.DataFrame(index=pd.to_datetime(ph3_labels_list, utc=True)),\n",
    "    filename_prefix='phas3_data',  # <-- You are saving as 'phas2_data'\n",
    "    feature_cols=feature_cols,\n",
    "    target_col=TARGET_COL,\n",
    "    metadata={\n",
    "        \"input_csv\": \"data/processed/dayTime_NAM_spatial_5locations_dayahead_features_processed.csv\",\n",
    "        \"timestamp_col\": \"measurement_time\",\n",
    "        \"feature_set\": feature_cols,\n",
    "        \"history_days\": 7,\n",
    "        \"horizon_days\": 1,\n",
    "        \"k_bins\": None,\n",
    "    }\n",
    ")\n",
    "\n",
    "# --- Block 4: Configure and Run ---\n",
    "LSTM_CONFIG = {\n",
    "    \"experiment_name\": \"LSTM_phase3_exp02\",\n",
    "    \"model_type\": \"LSTM\",\n",
    "    \"model_config\": {\n",
    "        \"hidden_size\": 64,\n",
    "        \"num_layers\": 2,\n",
    "        \"dropout\": 0.25,\n",
    "        \"bidirectional\": True,\n",
    "    },\n",
    "    \"data_prefix\": \"phas3_data\",  # <-- FIX 2: Match the filename_prefix\n",
    "    \"splits_file\": \"exp-003/exp-003rolling_origin_splits.json\",\n",
    "    \"feature_cols\": feature_cols,\n",
    "    \"feature_selection\": feature_cols, # <-- This is correct!\n",
    "    \"target_col\": TARGET_COL,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_epochs\": 50,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"loss_function\": \"Huber\",  # <-- This setting needs the fix below\n",
    "    \"early_stopping_patience\": 20,\n",
    "    \"max_folds\": 3,\n",
    "}\n",
    "\n",
    "# This will now work after you apply the fixes below\n",
    "pipeline = SolarForecastingPipeline(LSTM_CONFIG)\n",
    "phas3_fold_results, summary = pipeline.run()\n",
    "\n",
    "# To get the model from the LAST fold:\n",
    "phas3_last_model = phas3_fold_results[-1]['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a97f96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.utils:Saved arrays to data/phas4_data_*.npy\n",
      "INFO:src.utils:X shape: (1056, 7, 11, 6), Y shape: (1056, 1, 11)\n",
      "INFO:src.pipeline:Loading data...\n",
      "INFO:src.utils:Loaded arrays from data/phas4_data_*.npy\n",
      "INFO:src.utils:X shape: (1056, 7, 11, 6), Y shape: (1056, 1, 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:src.pipeline:Failed to construct reference dataframe for NAM comparison: could not convert string to float: '2014-01-03 14:00:00+00:00'\n",
      "INFO:src.utils:Loaded 35 folds from exp-003/exp-003rolling_origin_splits.json\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 1 ===\n",
      "INFO:src.pipeline:Train samples: 21, Val samples: 18\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 2: Building model arrays (X, Y) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.051697, Val Loss: 0.087358, Train MAE: 0.227353, Val MAE: 0.320367, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.044961, Val Loss: 0.076549, Train MAE: 0.212394, Val MAE: 0.301476, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.024929, Val Loss: 0.077833, Train MAE: 0.152070, Val MAE: 0.294396, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.023551, Val Loss: 0.081485, Train MAE: 0.134832, Val MAE: 0.297576, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 45\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 1.\n",
      "INFO:src.pipeline:Fold 1 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.161004\n",
      "INFO:src.pipeline:  rmse: 0.401253\n",
      "INFO:src.pipeline:  mae: 0.295937\n",
      "INFO:src.pipeline:  r2: 0.027275\n",
      "INFO:src.pipeline:  mape: 78.607788\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 2 ===\n",
      "INFO:src.pipeline:Train samples: 40, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.049543, Val Loss: 0.065256, Train MAE: 0.252557, Val MAE: 0.325191, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.042641, Val Loss: 0.045536, Train MAE: 0.220476, Val MAE: 0.267640, LR: 0.000500\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.046473, Val Loss: 0.056028, Train MAE: 0.227792, Val MAE: 0.300442, LR: 0.000500\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.028526, Val Loss: 0.041509, Train MAE: 0.154594, Val MAE: 0.248156, LR: 0.000250\n",
      "INFO:src.engine:Early stopping at epoch 46\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 2.\n",
      "INFO:src.pipeline:Fold 2 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.101845\n",
      "INFO:src.pipeline:  rmse: 0.319132\n",
      "INFO:src.pipeline:  mae: 0.284106\n",
      "INFO:src.pipeline:  r2: 0.084405\n",
      "INFO:src.pipeline:  mape: 65.261124\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 3 ===\n",
      "INFO:src.pipeline:Train samples: 71, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.053629, Val Loss: 0.025698, Train MAE: 0.257343, Val MAE: 0.158472, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.032691, Val Loss: 0.031230, Train MAE: 0.177133, Val MAE: 0.149641, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.031981, Val Loss: 0.023046, Train MAE: 0.178528, Val MAE: 0.127980, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.035552, Val Loss: 0.021529, Train MAE: 0.191864, Val MAE: 0.135020, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 48\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 3.\n",
      "INFO:src.pipeline:Fold 3 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.043600\n",
      "INFO:src.pipeline:  rmse: 0.208806\n",
      "INFO:src.pipeline:  mae: 0.121423\n",
      "INFO:src.pipeline:  r2: 0.474566\n",
      "INFO:src.pipeline:  mape: 33.536869\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 4 ===\n",
      "INFO:src.pipeline:Train samples: 101, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.036342, Val Loss: 0.011919, Train MAE: 0.199115, Val MAE: 0.107658, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.028691, Val Loss: 0.012329, Train MAE: 0.177563, Val MAE: 0.094597, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.030949, Val Loss: 0.012048, Train MAE: 0.173703, Val MAE: 0.100862, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.025279, Val Loss: 0.011656, Train MAE: 0.151898, Val MAE: 0.099173, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 44\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 4.\n",
      "INFO:src.pipeline:Fold 4 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.023625\n",
      "INFO:src.pipeline:  rmse: 0.153705\n",
      "INFO:src.pipeline:  mae: 0.106405\n",
      "INFO:src.pipeline:  r2: 0.493144\n",
      "INFO:src.pipeline:  mape: 15.734255\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 5 ===\n",
      "INFO:src.pipeline:Train samples: 132, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.031278, Val Loss: 0.005768, Train MAE: 0.181544, Val MAE: 0.071452, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.043119, Val Loss: 0.007236, Train MAE: 0.199604, Val MAE: 0.094568, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.022582, Val Loss: 0.004885, Train MAE: 0.148767, Val MAE: 0.055250, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 32\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 5.\n",
      "INFO:src.pipeline:Fold 5 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.010741\n",
      "INFO:src.pipeline:  rmse: 0.103638\n",
      "INFO:src.pipeline:  mae: 0.065713\n",
      "INFO:src.pipeline:  r2: 0.597576\n",
      "INFO:src.pipeline:  mape: 8.141768\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 6 ===\n",
      "INFO:src.pipeline:Train samples: 162, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.024610, Val Loss: 0.012791, Train MAE: 0.149149, Val MAE: 0.095847, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.020657, Val Loss: 0.014148, Train MAE: 0.135303, Val MAE: 0.087318, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 26\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 6.\n",
      "INFO:src.pipeline:Fold 6 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.029220\n",
      "INFO:src.pipeline:  rmse: 0.170939\n",
      "INFO:src.pipeline:  mae: 0.086687\n",
      "INFO:src.pipeline:  r2: 0.267065\n",
      "INFO:src.pipeline:  mape: 17.265215\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 7 ===\n",
      "INFO:src.pipeline:Train samples: 193, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.020646, Val Loss: 0.023925, Train MAE: 0.133516, Val MAE: 0.123788, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.018602, Val Loss: 0.023175, Train MAE: 0.118500, Val MAE: 0.111214, LR: 0.000500\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.024201, Val Loss: 0.022005, Train MAE: 0.145641, Val MAE: 0.114229, LR: 0.000500\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.022725, Val Loss: 0.021406, Train MAE: 0.145836, Val MAE: 0.109784, LR: 0.000500\n",
      "INFO:src.engine:Epoch [50/50] - Train Loss: 0.019765, Val Loss: 0.020219, Train MAE: 0.136289, Val MAE: 0.121249, LR: 0.000500\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 7.\n",
      "INFO:src.pipeline:Fold 7 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.040437\n",
      "INFO:src.pipeline:  rmse: 0.201090\n",
      "INFO:src.pipeline:  mae: 0.121249\n",
      "INFO:src.pipeline:  r2: 0.430613\n",
      "INFO:src.pipeline:  mape: 31.897800\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 8 ===\n",
      "INFO:src.pipeline:Train samples: 224, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.021490, Val Loss: 0.026256, Train MAE: 0.135738, Val MAE: 0.134270, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.018836, Val Loss: 0.026033, Train MAE: 0.122723, Val MAE: 0.113387, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 23\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 8.\n",
      "INFO:src.pipeline:Fold 8 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.051649\n",
      "INFO:src.pipeline:  rmse: 0.227263\n",
      "INFO:src.pipeline:  mae: 0.111550\n",
      "INFO:src.pipeline:  r2: 0.227525\n",
      "INFO:src.pipeline:  mape: 31.877350\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 9 ===\n",
      "INFO:src.pipeline:Train samples: 254, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.022433, Val Loss: 0.020467, Train MAE: 0.140273, Val MAE: 0.139614, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.021594, Val Loss: 0.023301, Train MAE: 0.126856, Val MAE: 0.185459, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.017051, Val Loss: 0.016487, Train MAE: 0.117584, Val MAE: 0.145969, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.014329, Val Loss: 0.017930, Train MAE: 0.105559, Val MAE: 0.154312, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 47\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 9.\n",
      "INFO:src.pipeline:Fold 9 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.037507\n",
      "INFO:src.pipeline:  rmse: 0.193667\n",
      "INFO:src.pipeline:  mae: 0.161545\n",
      "INFO:src.pipeline:  r2: 0.211292\n",
      "INFO:src.pipeline:  mape: 21.366646\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 10 ===\n",
      "INFO:src.pipeline:Train samples: 285, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.021016, Val Loss: 0.041618, Train MAE: 0.134526, Val MAE: 0.183992, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.017528, Val Loss: 0.037540, Train MAE: 0.121849, Val MAE: 0.192569, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.015386, Val Loss: 0.039251, Train MAE: 0.112750, Val MAE: 0.183382, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 32\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 10.\n",
      "INFO:src.pipeline:Fold 10 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.077057\n",
      "INFO:src.pipeline:  rmse: 0.277591\n",
      "INFO:src.pipeline:  mae: 0.184923\n",
      "INFO:src.pipeline:  r2: 0.307627\n",
      "INFO:src.pipeline:  mape: 59.012234\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 11 ===\n",
      "INFO:src.pipeline:Train samples: 315, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.023235, Val Loss: 0.046103, Train MAE: 0.142308, Val MAE: 0.245083, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.018433, Val Loss: 0.053574, Train MAE: 0.124943, Val MAE: 0.246271, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 25\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 11.\n",
      "INFO:src.pipeline:Fold 11 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.117912\n",
      "INFO:src.pipeline:  rmse: 0.343384\n",
      "INFO:src.pipeline:  mae: 0.252153\n",
      "INFO:src.pipeline:  r2: 0.172615\n",
      "INFO:src.pipeline:  mape: 88.300545\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 12 ===\n",
      "INFO:src.pipeline:Train samples: 346, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.025902, Val Loss: 0.037375, Train MAE: 0.155823, Val MAE: 0.211431, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.021295, Val Loss: 0.043704, Train MAE: 0.137489, Val MAE: 0.205821, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 26\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 12.\n",
      "INFO:src.pipeline:Fold 12 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.088723\n",
      "INFO:src.pipeline:  rmse: 0.297865\n",
      "INFO:src.pipeline:  mae: 0.203376\n",
      "INFO:src.pipeline:  r2: 0.026719\n",
      "INFO:src.pipeline:  mape: 46.589241\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 13 ===\n",
      "INFO:src.pipeline:Train samples: 377, Val samples: 26\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.025978, Val Loss: 0.036930, Train MAE: 0.154948, Val MAE: 0.213589, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.023506, Val Loss: 0.036699, Train MAE: 0.145102, Val MAE: 0.206079, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.021263, Val Loss: 0.038685, Train MAE: 0.135402, Val MAE: 0.200521, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 34\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 13.\n",
      "INFO:src.pipeline:Fold 13 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.083406\n",
      "INFO:src.pipeline:  rmse: 0.288801\n",
      "INFO:src.pipeline:  mae: 0.206141\n",
      "INFO:src.pipeline:  r2: 0.288351\n",
      "INFO:src.pipeline:  mape: 68.734001\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 14 ===\n",
      "INFO:src.pipeline:Train samples: 405, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.026300, Val Loss: 0.024623, Train MAE: 0.158884, Val MAE: 0.161754, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.024650, Val Loss: 0.030321, Train MAE: 0.149785, Val MAE: 0.176814, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.021957, Val Loss: 0.032102, Train MAE: 0.141114, Val MAE: 0.175110, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 30\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 14.\n",
      "INFO:src.pipeline:Fold 14 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.064203\n",
      "INFO:src.pipeline:  rmse: 0.253383\n",
      "INFO:src.pipeline:  mae: 0.175110\n",
      "INFO:src.pipeline:  r2: 0.192162\n",
      "INFO:src.pipeline:  mape: 28.988800\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 15 ===\n",
      "INFO:src.pipeline:Train samples: 436, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.025755, Val Loss: 0.019721, Train MAE: 0.155893, Val MAE: 0.129985, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.023669, Val Loss: 0.020706, Train MAE: 0.145085, Val MAE: 0.126720, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.021596, Val Loss: 0.020982, Train MAE: 0.137599, Val MAE: 0.121036, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 38\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 15.\n",
      "INFO:src.pipeline:Fold 15 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.038845\n",
      "INFO:src.pipeline:  rmse: 0.197092\n",
      "INFO:src.pipeline:  mae: 0.115606\n",
      "INFO:src.pipeline:  r2: 0.538708\n",
      "INFO:src.pipeline:  mape: 31.901279\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 16 ===\n",
      "INFO:src.pipeline:Train samples: 466, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.026031, Val Loss: 0.019296, Train MAE: 0.157283, Val MAE: 0.113416, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.022464, Val Loss: 0.018958, Train MAE: 0.141219, Val MAE: 0.114566, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 25\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 16.\n",
      "INFO:src.pipeline:Fold 16 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.037115\n",
      "INFO:src.pipeline:  rmse: 0.192654\n",
      "INFO:src.pipeline:  mae: 0.117598\n",
      "INFO:src.pipeline:  r2: 0.336571\n",
      "INFO:src.pipeline:  mape: 41.817493\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 17 ===\n",
      "INFO:src.pipeline:Train samples: 497, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.025696, Val Loss: 0.018056, Train MAE: 0.153827, Val MAE: 0.097955, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.022375, Val Loss: 0.018085, Train MAE: 0.142316, Val MAE: 0.104165, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 24\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 17.\n",
      "INFO:src.pipeline:Fold 17 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.036015\n",
      "INFO:src.pipeline:  rmse: 0.189776\n",
      "INFO:src.pipeline:  mae: 0.109041\n",
      "INFO:src.pipeline:  r2: 0.386378\n",
      "INFO:src.pipeline:  mape: 26.224894\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 18 ===\n",
      "INFO:src.pipeline:Train samples: 527, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.024988, Val Loss: 0.014959, Train MAE: 0.154037, Val MAE: 0.130499, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.021922, Val Loss: 0.012865, Train MAE: 0.142295, Val MAE: 0.109029, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.020638, Val Loss: 0.013391, Train MAE: 0.138876, Val MAE: 0.091342, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 37\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 18.\n",
      "INFO:src.pipeline:Fold 18 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.028200\n",
      "INFO:src.pipeline:  rmse: 0.167928\n",
      "INFO:src.pipeline:  mae: 0.122653\n",
      "INFO:src.pipeline:  r2: 0.347829\n",
      "INFO:src.pipeline:  mape: 19.578266\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 19 ===\n",
      "INFO:src.pipeline:Train samples: 558, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.025098, Val Loss: 0.025595, Train MAE: 0.155186, Val MAE: 0.127670, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.022708, Val Loss: 0.026428, Train MAE: 0.144254, Val MAE: 0.139375, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.020392, Val Loss: 0.029477, Train MAE: 0.134667, Val MAE: 0.132748, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 35\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 19.\n",
      "INFO:src.pipeline:Fold 19 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.053554\n",
      "INFO:src.pipeline:  rmse: 0.231418\n",
      "INFO:src.pipeline:  mae: 0.107282\n",
      "INFO:src.pipeline:  r2: 0.321015\n",
      "INFO:src.pipeline:  mape: 11.408997\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 20 ===\n",
      "INFO:src.pipeline:Train samples: 589, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.024307, Val Loss: 0.018209, Train MAE: 0.149095, Val MAE: 0.135877, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.022938, Val Loss: 0.017063, Train MAE: 0.141518, Val MAE: 0.132465, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.020451, Val Loss: 0.017932, Train MAE: 0.130738, Val MAE: 0.123219, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 36\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 20.\n",
      "INFO:src.pipeline:Fold 20 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.043560\n",
      "INFO:src.pipeline:  rmse: 0.208711\n",
      "INFO:src.pipeline:  mae: 0.146932\n",
      "INFO:src.pipeline:  r2: 0.337031\n",
      "INFO:src.pipeline:  mape: 25.877800\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 21 ===\n",
      "INFO:src.pipeline:Train samples: 619, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.023521, Val Loss: 0.018728, Train MAE: 0.147029, Val MAE: 0.132867, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.022755, Val Loss: 0.016854, Train MAE: 0.142127, Val MAE: 0.133175, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.019992, Val Loss: 0.016697, Train MAE: 0.132262, Val MAE: 0.123199, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.017442, Val Loss: 0.017434, Train MAE: 0.122750, Val MAE: 0.121912, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 43\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 21.\n",
      "INFO:src.pipeline:Fold 21 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.047102\n",
      "INFO:src.pipeline:  rmse: 0.217030\n",
      "INFO:src.pipeline:  mae: 0.137486\n",
      "INFO:src.pipeline:  r2: 0.023994\n",
      "INFO:src.pipeline:  mape: 19.676910\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 22 ===\n",
      "INFO:src.pipeline:Train samples: 650, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.025240, Val Loss: 0.035450, Train MAE: 0.148716, Val MAE: 0.200312, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.021886, Val Loss: 0.035945, Train MAE: 0.140069, Val MAE: 0.170391, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.020044, Val Loss: 0.039812, Train MAE: 0.133410, Val MAE: 0.182858, LR: 0.001000\n",
      "INFO:src.engine:Early stopping at epoch 39\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 22.\n",
      "INFO:src.pipeline:Fold 22 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.079428\n",
      "INFO:src.pipeline:  rmse: 0.281830\n",
      "INFO:src.pipeline:  mae: 0.193277\n",
      "INFO:src.pipeline:  r2: 0.054252\n",
      "INFO:src.pipeline:  mape: 48.903648\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 23 ===\n",
      "INFO:src.pipeline:Train samples: 680, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.025731, Val Loss: 0.044269, Train MAE: 0.156311, Val MAE: 0.234123, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.022448, Val Loss: 0.043102, Train MAE: 0.141852, Val MAE: 0.234187, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 26\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 23.\n",
      "INFO:src.pipeline:Fold 23 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.098198\n",
      "INFO:src.pipeline:  rmse: 0.313365\n",
      "INFO:src.pipeline:  mae: 0.242602\n",
      "INFO:src.pipeline:  r2: 0.130230\n",
      "INFO:src.pipeline:  mape: 63.711494\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 24 ===\n",
      "INFO:src.pipeline:Train samples: 711, Val samples: 21\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.025549, Val Loss: 0.061818, Train MAE: 0.159307, Val MAE: 0.284757, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.023442, Val Loss: 0.064684, Train MAE: 0.147908, Val MAE: 0.277631, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 24\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 24.\n",
      "INFO:src.pipeline:Fold 24 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.125638\n",
      "INFO:src.pipeline:  rmse: 0.354454\n",
      "INFO:src.pipeline:  mae: 0.272778\n",
      "INFO:src.pipeline:  r2: 0.185251\n",
      "INFO:src.pipeline:  mape: 87.684196\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 25 ===\n",
      "INFO:src.pipeline:Train samples: 734, Val samples: 23\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.026933, Val Loss: 0.052002, Train MAE: 0.162428, Val MAE: 0.209671, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.025067, Val Loss: 0.056742, Train MAE: 0.150539, Val MAE: 0.230183, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 23\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 25.\n",
      "INFO:src.pipeline:Fold 25 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.113314\n",
      "INFO:src.pipeline:  rmse: 0.336622\n",
      "INFO:src.pipeline:  mae: 0.231334\n",
      "INFO:src.pipeline:  r2: 0.061978\n",
      "INFO:src.pipeline:  mape: 20.752678\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 26 ===\n",
      "INFO:src.pipeline:Train samples: 759, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.027646, Val Loss: 0.040843, Train MAE: 0.160730, Val MAE: 0.216293, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.026732, Val Loss: 0.037792, Train MAE: 0.155479, Val MAE: 0.206226, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.023590, Val Loss: 0.052009, Train MAE: 0.145209, Val MAE: 0.234818, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.019117, Val Loss: 0.054727, Train MAE: 0.128630, Val MAE: 0.228009, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 40\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 26.\n",
      "INFO:src.pipeline:Fold 26 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.109454\n",
      "INFO:src.pipeline:  rmse: 0.330838\n",
      "INFO:src.pipeline:  mae: 0.228009\n",
      "INFO:src.pipeline:  r2: 0.229650\n",
      "INFO:src.pipeline:  mape: 133.067581\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 27 ===\n",
      "INFO:src.pipeline:Train samples: 790, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.028779, Val Loss: 0.034747, Train MAE: 0.168027, Val MAE: 0.158063, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.026724, Val Loss: 0.028783, Train MAE: 0.157654, Val MAE: 0.165977, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.023839, Val Loss: 0.029462, Train MAE: 0.145874, Val MAE: 0.152161, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 38\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 27.\n",
      "INFO:src.pipeline:Fold 27 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.062848\n",
      "INFO:src.pipeline:  rmse: 0.250695\n",
      "INFO:src.pipeline:  mae: 0.166412\n",
      "INFO:src.pipeline:  r2: 0.420505\n",
      "INFO:src.pipeline:  mape: 56.794231\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 28 ===\n",
      "INFO:src.pipeline:Train samples: 820, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.028450, Val Loss: 0.024342, Train MAE: 0.166562, Val MAE: 0.138788, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.026977, Val Loss: 0.021258, Train MAE: 0.159112, Val MAE: 0.134474, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.023964, Val Loss: 0.028896, Train MAE: 0.144906, Val MAE: 0.152281, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 38\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 28.\n",
      "INFO:src.pipeline:Fold 28 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.062902\n",
      "INFO:src.pipeline:  rmse: 0.250802\n",
      "INFO:src.pipeline:  mae: 0.159929\n",
      "INFO:src.pipeline:  r2: 0.241806\n",
      "INFO:src.pipeline:  mape: 42.360706\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 29 ===\n",
      "INFO:src.pipeline:Train samples: 851, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.027306, Val Loss: 0.008428, Train MAE: 0.162724, Val MAE: 0.095889, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.026377, Val Loss: 0.008847, Train MAE: 0.154214, Val MAE: 0.110162, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.024110, Val Loss: 0.009409, Train MAE: 0.146722, Val MAE: 0.110830, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 32\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 29.\n",
      "INFO:src.pipeline:Fold 29 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.017750\n",
      "INFO:src.pipeline:  rmse: 0.133231\n",
      "INFO:src.pipeline:  mae: 0.093265\n",
      "INFO:src.pipeline:  r2: 0.567696\n",
      "INFO:src.pipeline:  mape: 12.190336\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 30 ===\n",
      "INFO:src.pipeline:Train samples: 881, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.027379, Val Loss: 0.020375, Train MAE: 0.161499, Val MAE: 0.096838, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.025430, Val Loss: 0.020227, Train MAE: 0.153577, Val MAE: 0.094071, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.023048, Val Loss: 0.020289, Train MAE: 0.144109, Val MAE: 0.099981, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.021182, Val Loss: 0.020102, Train MAE: 0.138240, Val MAE: 0.089413, LR: 0.001000\n",
      "INFO:src.engine:Epoch [50/50] - Train Loss: 0.014287, Val Loss: 0.020407, Train MAE: 0.112134, Val MAE: 0.071468, LR: 0.000500\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 30.\n",
      "INFO:src.pipeline:Fold 30 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.041133\n",
      "INFO:src.pipeline:  rmse: 0.202812\n",
      "INFO:src.pipeline:  mae: 0.071468\n",
      "INFO:src.pipeline:  r2: 0.283906\n",
      "INFO:src.pipeline:  mape: 3.725259\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 31 ===\n",
      "INFO:src.pipeline:Train samples: 911, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.027241, Val Loss: 0.004731, Train MAE: 0.157798, Val MAE: 0.064420, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.025299, Val Loss: 0.003976, Train MAE: 0.151908, Val MAE: 0.040780, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.024220, Val Loss: 0.004649, Train MAE: 0.146260, Val MAE: 0.062394, LR: 0.001000\n",
      "INFO:src.engine:Early stopping at epoch 39\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 31.\n",
      "INFO:src.pipeline:Fold 31 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.008273\n",
      "INFO:src.pipeline:  rmse: 0.090953\n",
      "INFO:src.pipeline:  mae: 0.049101\n",
      "INFO:src.pipeline:  r2: 0.787008\n",
      "INFO:src.pipeline:  mape: 6.733099\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 32 ===\n",
      "INFO:src.pipeline:Train samples: 942, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.025762, Val Loss: 0.007503, Train MAE: 0.153349, Val MAE: 0.072169, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.025328, Val Loss: 0.006744, Train MAE: 0.148937, Val MAE: 0.065085, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.022814, Val Loss: 0.005989, Train MAE: 0.137797, Val MAE: 0.057619, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.019439, Val Loss: 0.005523, Train MAE: 0.127130, Val MAE: 0.044307, LR: 0.000500\n",
      "INFO:src.engine:Epoch [50/50] - Train Loss: 0.016106, Val Loss: 0.005049, Train MAE: 0.115874, Val MAE: 0.047503, LR: 0.000500\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 32.\n",
      "INFO:src.pipeline:Fold 32 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.010099\n",
      "INFO:src.pipeline:  rmse: 0.100493\n",
      "INFO:src.pipeline:  mae: 0.047503\n",
      "INFO:src.pipeline:  r2: 0.744573\n",
      "INFO:src.pipeline:  mape: 6.831277\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 33 ===\n",
      "INFO:src.pipeline:Train samples: 972, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.025383, Val Loss: 0.048333, Train MAE: 0.151313, Val MAE: 0.210315, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.024246, Val Loss: 0.043372, Train MAE: 0.142927, Val MAE: 0.219479, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.022970, Val Loss: 0.050011, Train MAE: 0.137914, Val MAE: 0.207201, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.020043, Val Loss: 0.059244, Train MAE: 0.126987, Val MAE: 0.208531, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 40\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 33.\n",
      "INFO:src.pipeline:Fold 33 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.118512\n",
      "INFO:src.pipeline:  rmse: 0.344255\n",
      "INFO:src.pipeline:  mae: 0.208531\n",
      "INFO:src.pipeline:  r2: 0.089055\n",
      "INFO:src.pipeline:  mape: 100.944374\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 34 ===\n",
      "INFO:src.pipeline:Train samples: 1003, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.025444, Val Loss: 0.033160, Train MAE: 0.151195, Val MAE: 0.193632, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.024006, Val Loss: 0.032679, Train MAE: 0.146429, Val MAE: 0.196598, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.022674, Val Loss: 0.034618, Train MAE: 0.137188, Val MAE: 0.200304, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.020294, Val Loss: 0.039926, Train MAE: 0.128247, Val MAE: 0.215727, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 46\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 34.\n",
      "INFO:src.pipeline:Fold 34 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.099326\n",
      "INFO:src.pipeline:  rmse: 0.315160\n",
      "INFO:src.pipeline:  mae: 0.232802\n",
      "INFO:src.pipeline:  r2: 0.017104\n",
      "INFO:src.pipeline:  mape: 43.812309\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 35 ===\n",
      "INFO:src.pipeline:Train samples: 1033, Val samples: 21\n",
      "INFO:src.pipeline:Model parameters: 245,195\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.027288, Val Loss: 0.078180, Train MAE: 0.155568, Val MAE: 0.282623, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.024558, Val Loss: 0.078398, Train MAE: 0.147611, Val MAE: 0.281962, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.021986, Val Loss: 0.082337, Train MAE: 0.138717, Val MAE: 0.330081, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 37\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 35.\n",
      "INFO:src.pipeline:Fold 35 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.176798\n",
      "INFO:src.pipeline:  rmse: 0.420474\n",
      "INFO:src.pipeline:  mae: 0.339765\n",
      "INFO:src.pipeline:  r2: -0.108294\n",
      "INFO:src.pipeline:  mape: 53.972595\n",
      "INFO:src.pipeline:\n",
      "=== Experiment Summary ===\n",
      "INFO:src.pipeline:Average Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.066828 ± 0.041981\n",
      "INFO:src.pipeline:  rmse: 0.244889 ± 0.082812\n",
      "INFO:src.pipeline:  mae: 0.164848 ± 0.073236\n",
      "INFO:src.pipeline:  r2: 0.279919 ± 0.208601\n",
      "INFO:src.pipeline:  mape: 41.522373 ± 29.729323\n"
     ]
    }
   ],
   "source": [
    "from src.preprocessing import KBinConfig, process_splits_to_kbins, build_model_arrays, to_fixedgrid_multiindex\n",
    "import pandas as pd\n",
    "from src.utils import DataManager\n",
    "from src.pipeline import SolarForecastingPipeline\n",
    "\n",
    "\n",
    "df_phase4 = df_interpolated[['CSI_ghi', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos',\n",
    "    'nam_ghi', 'nam_cc']]\n",
    "\n",
    "\n",
    "# --- Block 1: Define variables ---\n",
    "USE_KBINS = False\n",
    "k_bins = 60\n",
    "TARGET_COL = \"CSI_ghi\"  # <-- FIX 1: Define Target FIRST\n",
    "history_days = 7\n",
    "horizon_days = 1\n",
    "\n",
    "# This list now correctly excludes the target\n",
    "feature_cols = [c for c in df_phase4.columns.tolist() if c != TARGET_COL]\n",
    "\n",
    "# --- Block 2: Build model arrays (Your code was correct here) ---\n",
    "if USE_KBINS:\n",
    "        print(\"\\n--- Step 2: Converting full dataset to K-Bins format ---\")\n",
    "        # ... (rest of your K-Bins code) ...\n",
    "else:\n",
    "    print(\"\\n--- Step 2: Building model arrays (X, Y) ---\")\n",
    "    fixed_df = to_fixedgrid_multiindex(df_phase4, timestamp_col=\"measurement_time\", expected_T=None)  # or set T\n",
    "    \n",
    "    # You correctly passed the filtered 'feature_cols' here, fixing the data leak!\n",
    "    ph4_X, ph4_Y, ph4_labels_list = build_model_arrays(\n",
    "        fixed_df,\n",
    "        feature_cols=feature_cols,  # <-- This is correct!\n",
    "        target_col=TARGET_COL,\n",
    "        history_days=history_days,\n",
    "        horizon_days=horizon_days,\n",
    "    )\n",
    "\n",
    "# --- Block 3: Save arrays ---\n",
    "data_manager = DataManager()\n",
    "data_manager.save_arrays(\n",
    "    ph4_X, ph4_Y,\n",
    "    pd.DataFrame(index=pd.to_datetime(ph4_labels_list, utc=True)),\n",
    "    filename_prefix='phas4_data',  # <-- You are saving as 'phas2_data'\n",
    "    feature_cols=feature_cols,\n",
    "    target_col=TARGET_COL,\n",
    "    metadata={\n",
    "        \"input_csv\": \"data/processed/dayTime_NAM_spatial_5locations_dayahead_features_processed.csv\",\n",
    "        \"timestamp_col\": \"measurement_time\",\n",
    "        \"feature_set\": feature_cols,\n",
    "        \"history_days\": 7,\n",
    "        \"horizon_days\": 1,\n",
    "        \"k_bins\": None,\n",
    "    }\n",
    ")\n",
    "\n",
    "# --- Block 4: Configure and Run ---\n",
    "LSTM_CONFIG = {\n",
    "    \"experiment_name\": \"LSTM_phase4_exp02\",\n",
    "    \"model_type\": \"LSTM\",\n",
    "    \"model_config\": {\n",
    "        \"hidden_size\": 64,\n",
    "        \"num_layers\": 2,\n",
    "        \"dropout\": 0.25,\n",
    "        \"bidirectional\": True,\n",
    "    },\n",
    "    \"data_prefix\": \"phas4_data\",  # <-- FIX 2: Match the filename_prefix\n",
    "    \"splits_file\": \"exp-003/exp-003rolling_origin_splits.json\",\n",
    "    \"feature_cols\": feature_cols,\n",
    "    \"feature_selection\": feature_cols, # <-- This is correct!\n",
    "    \"target_col\": TARGET_COL,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_epochs\": 50,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"loss_function\": \"Huber\",  # <-- This setting needs the fix below\n",
    "    \"early_stopping_patience\": 20,\n",
    "    \"max_folds\": 35,\n",
    "}\n",
    "\n",
    "# This will now work after you apply the fixes below\n",
    "pipeline = SolarForecastingPipeline(LSTM_CONFIG)\n",
    "fold_results, summary = pipeline.run()\n",
    "\n",
    "# To get the model from the LAST fold:\n",
    "last_model = fold_results[-1]['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da4bc29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.utils:Saved arrays to data/phas5_data_*.npy\n",
      "INFO:src.utils:X shape: (1056, 7, 11, 9), Y shape: (1056, 1, 11)\n",
      "INFO:src.pipeline:Loading data...\n",
      "INFO:src.utils:Loaded arrays from data/phas5_data_*.npy\n",
      "INFO:src.utils:X shape: (1056, 7, 11, 9), Y shape: (1056, 1, 11)\n",
      "WARNING:src.pipeline:Failed to construct reference dataframe for NAM comparison: could not convert string to float: '2014-01-03 14:00:00+00:00'\n",
      "INFO:src.utils:Loaded 35 folds from exp-003/exp-003rolling_origin_splits.json\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 1 ===\n",
      "INFO:src.pipeline:Train samples: 21, Val samples: 18\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 2: Building model arrays (X, Y) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.049739, Val Loss: 0.088166, Train MAE: 0.221810, Val MAE: 0.325361, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.025353, Val Loss: 0.074557, Train MAE: 0.148397, Val MAE: 0.294710, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.022186, Val Loss: 0.080162, Train MAE: 0.141149, Val MAE: 0.305926, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 38\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 1.\n",
      "INFO:src.pipeline:Fold 1 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.167472\n",
      "INFO:src.pipeline:  rmse: 0.409233\n",
      "INFO:src.pipeline:  mae: 0.301700\n",
      "INFO:src.pipeline:  r2: -0.011800\n",
      "INFO:src.pipeline:  mape: 70.875938\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 2 ===\n",
      "INFO:src.pipeline:Train samples: 40, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.055114, Val Loss: 0.061075, Train MAE: 0.258907, Val MAE: 0.291307, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.033499, Val Loss: 0.059160, Train MAE: 0.195160, Val MAE: 0.313426, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.026593, Val Loss: 0.048143, Train MAE: 0.169916, Val MAE: 0.273401, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.030522, Val Loss: 0.068122, Train MAE: 0.179059, Val MAE: 0.298328, LR: 0.001000\n",
      "INFO:src.engine:Epoch [50/50] - Train Loss: 0.019333, Val Loss: 0.063462, Train MAE: 0.139043, Val MAE: 0.300646, LR: 0.000500\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 2.\n",
      "INFO:src.pipeline:Fold 2 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.126925\n",
      "INFO:src.pipeline:  rmse: 0.356265\n",
      "INFO:src.pipeline:  mae: 0.300646\n",
      "INFO:src.pipeline:  r2: -0.141062\n",
      "INFO:src.pipeline:  mape: 66.571030\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 3 ===\n",
      "INFO:src.pipeline:Train samples: 71, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.048149, Val Loss: 0.024014, Train MAE: 0.232928, Val MAE: 0.147684, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.038094, Val Loss: 0.025987, Train MAE: 0.199845, Val MAE: 0.162763, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.036815, Val Loss: 0.024543, Train MAE: 0.190239, Val MAE: 0.138014, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 35\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 3.\n",
      "INFO:src.pipeline:Fold 3 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.051933\n",
      "INFO:src.pipeline:  rmse: 0.227889\n",
      "INFO:src.pipeline:  mae: 0.147245\n",
      "INFO:src.pipeline:  r2: 0.374139\n",
      "INFO:src.pipeline:  mape: 37.968739\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 4 ===\n",
      "INFO:src.pipeline:Train samples: 101, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.041340, Val Loss: 0.012185, Train MAE: 0.213012, Val MAE: 0.118589, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.034761, Val Loss: 0.017536, Train MAE: 0.182338, Val MAE: 0.151904, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.027251, Val Loss: 0.013135, Train MAE: 0.164802, Val MAE: 0.106954, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 30\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 4.\n",
      "INFO:src.pipeline:Fold 4 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.026269\n",
      "INFO:src.pipeline:  rmse: 0.162078\n",
      "INFO:src.pipeline:  mae: 0.106954\n",
      "INFO:src.pipeline:  r2: 0.436417\n",
      "INFO:src.pipeline:  mape: 15.510724\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 5 ===\n",
      "INFO:src.pipeline:Train samples: 132, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.032011, Val Loss: 0.005425, Train MAE: 0.171786, Val MAE: 0.071881, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.028441, Val Loss: 0.007665, Train MAE: 0.166018, Val MAE: 0.087199, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.022992, Val Loss: 0.005190, Train MAE: 0.145422, Val MAE: 0.064185, LR: 0.000500\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.021471, Val Loss: 0.007628, Train MAE: 0.148713, Val MAE: 0.095986, LR: 0.000500\n",
      "INFO:src.engine:Epoch [50/50] - Train Loss: 0.024071, Val Loss: 0.004941, Train MAE: 0.150342, Val MAE: 0.061780, LR: 0.000250\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 5.\n",
      "INFO:src.pipeline:Fold 5 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.009882\n",
      "INFO:src.pipeline:  rmse: 0.099408\n",
      "INFO:src.pipeline:  mae: 0.061780\n",
      "INFO:src.pipeline:  r2: 0.629755\n",
      "INFO:src.pipeline:  mape: 7.719372\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 6 ===\n",
      "INFO:src.pipeline:Train samples: 162, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.024543, Val Loss: 0.012756, Train MAE: 0.144394, Val MAE: 0.089856, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.019932, Val Loss: 0.013583, Train MAE: 0.123598, Val MAE: 0.081918, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 27\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 6.\n",
      "INFO:src.pipeline:Fold 6 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.027370\n",
      "INFO:src.pipeline:  rmse: 0.165438\n",
      "INFO:src.pipeline:  mae: 0.083476\n",
      "INFO:src.pipeline:  r2: 0.313480\n",
      "INFO:src.pipeline:  mape: 16.761002\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 7 ===\n",
      "INFO:src.pipeline:Train samples: 193, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.024784, Val Loss: 0.023211, Train MAE: 0.153931, Val MAE: 0.119144, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.021530, Val Loss: 0.023971, Train MAE: 0.139480, Val MAE: 0.125381, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 22\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 7.\n",
      "INFO:src.pipeline:Fold 7 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.048810\n",
      "INFO:src.pipeline:  rmse: 0.220929\n",
      "INFO:src.pipeline:  mae: 0.125643\n",
      "INFO:src.pipeline:  r2: 0.312723\n",
      "INFO:src.pipeline:  mape: 35.053028\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 8 ===\n",
      "INFO:src.pipeline:Train samples: 224, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.023238, Val Loss: 0.024657, Train MAE: 0.144381, Val MAE: 0.123621, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.018565, Val Loss: 0.025388, Train MAE: 0.124855, Val MAE: 0.114486, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 25\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 8.\n",
      "INFO:src.pipeline:Fold 8 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.051172\n",
      "INFO:src.pipeline:  rmse: 0.226213\n",
      "INFO:src.pipeline:  mae: 0.122225\n",
      "INFO:src.pipeline:  r2: 0.234647\n",
      "INFO:src.pipeline:  mape: 30.586529\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 9 ===\n",
      "INFO:src.pipeline:Train samples: 254, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.022170, Val Loss: 0.019227, Train MAE: 0.138999, Val MAE: 0.138570, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.018453, Val Loss: 0.015843, Train MAE: 0.122052, Val MAE: 0.128152, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.015057, Val Loss: 0.015702, Train MAE: 0.112460, Val MAE: 0.117966, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.013170, Val Loss: 0.022639, Train MAE: 0.107431, Val MAE: 0.156920, LR: 0.001000\n",
      "INFO:src.engine:Epoch [50/50] - Train Loss: 0.010256, Val Loss: 0.018894, Train MAE: 0.091944, Val MAE: 0.125287, LR: 0.001000\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 9.\n",
      "INFO:src.pipeline:Fold 9 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.037787\n",
      "INFO:src.pipeline:  rmse: 0.194389\n",
      "INFO:src.pipeline:  mae: 0.125287\n",
      "INFO:src.pipeline:  r2: 0.205400\n",
      "INFO:src.pipeline:  mape: 18.512054\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 10 ===\n",
      "INFO:src.pipeline:Train samples: 285, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.020911, Val Loss: 0.035733, Train MAE: 0.134251, Val MAE: 0.192622, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.017886, Val Loss: 0.034550, Train MAE: 0.120741, Val MAE: 0.180675, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.013540, Val Loss: 0.036096, Train MAE: 0.107384, Val MAE: 0.169945, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 34\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 10.\n",
      "INFO:src.pipeline:Fold 10 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.079962\n",
      "INFO:src.pipeline:  rmse: 0.282776\n",
      "INFO:src.pipeline:  mae: 0.171494\n",
      "INFO:src.pipeline:  r2: 0.281518\n",
      "INFO:src.pipeline:  mape: 59.414673\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 11 ===\n",
      "INFO:src.pipeline:Train samples: 315, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.022112, Val Loss: 0.062412, Train MAE: 0.139714, Val MAE: 0.279819, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.018422, Val Loss: 0.066737, Train MAE: 0.119876, Val MAE: 0.278156, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 24\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 11.\n",
      "INFO:src.pipeline:Fold 11 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.162995\n",
      "INFO:src.pipeline:  rmse: 0.403726\n",
      "INFO:src.pipeline:  mae: 0.300785\n",
      "INFO:src.pipeline:  r2: -0.143724\n",
      "INFO:src.pipeline:  mape: 145.011246\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 12 ===\n",
      "INFO:src.pipeline:Train samples: 346, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.026062, Val Loss: 0.035817, Train MAE: 0.160097, Val MAE: 0.198975, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.021081, Val Loss: 0.041863, Train MAE: 0.134085, Val MAE: 0.198278, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 25\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 12.\n",
      "INFO:src.pipeline:Fold 12 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.092260\n",
      "INFO:src.pipeline:  rmse: 0.303744\n",
      "INFO:src.pipeline:  mae: 0.208193\n",
      "INFO:src.pipeline:  r2: -0.012083\n",
      "INFO:src.pipeline:  mape: 40.277283\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 13 ===\n",
      "INFO:src.pipeline:Train samples: 377, Val samples: 26\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.025071, Val Loss: 0.038043, Train MAE: 0.151148, Val MAE: 0.203937, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.020696, Val Loss: 0.044347, Train MAE: 0.135808, Val MAE: 0.221291, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.017169, Val Loss: 0.053981, Train MAE: 0.121402, Val MAE: 0.245385, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 33\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 13.\n",
      "INFO:src.pipeline:Fold 13 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.115924\n",
      "INFO:src.pipeline:  rmse: 0.340476\n",
      "INFO:src.pipeline:  mae: 0.254056\n",
      "INFO:src.pipeline:  r2: 0.010899\n",
      "INFO:src.pipeline:  mape: 86.344238\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 14 ===\n",
      "INFO:src.pipeline:Train samples: 405, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.027117, Val Loss: 0.031416, Train MAE: 0.164126, Val MAE: 0.194538, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.021599, Val Loss: 0.032403, Train MAE: 0.140746, Val MAE: 0.175428, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.017112, Val Loss: 0.035667, Train MAE: 0.126409, Val MAE: 0.184144, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 33\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 14.\n",
      "INFO:src.pipeline:Fold 14 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.084125\n",
      "INFO:src.pipeline:  rmse: 0.290043\n",
      "INFO:src.pipeline:  mae: 0.194729\n",
      "INFO:src.pipeline:  r2: -0.058510\n",
      "INFO:src.pipeline:  mape: 28.850922\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 15 ===\n",
      "INFO:src.pipeline:Train samples: 436, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.025066, Val Loss: 0.022932, Train MAE: 0.155259, Val MAE: 0.119922, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.021058, Val Loss: 0.021219, Train MAE: 0.139992, Val MAE: 0.112089, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.019038, Val Loss: 0.023027, Train MAE: 0.131557, Val MAE: 0.111390, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.013739, Val Loss: 0.024297, Train MAE: 0.107168, Val MAE: 0.112210, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 46\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 15.\n",
      "INFO:src.pipeline:Fold 15 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.049185\n",
      "INFO:src.pipeline:  rmse: 0.221776\n",
      "INFO:src.pipeline:  mae: 0.113555\n",
      "INFO:src.pipeline:  r2: 0.415924\n",
      "INFO:src.pipeline:  mape: 36.367908\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 16 ===\n",
      "INFO:src.pipeline:Train samples: 466, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.025440, Val Loss: 0.018988, Train MAE: 0.157199, Val MAE: 0.125256, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.021348, Val Loss: 0.018732, Train MAE: 0.137328, Val MAE: 0.128543, LR: 0.001000\n",
      "INFO:src.engine:Early stopping at epoch 29\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 16.\n",
      "INFO:src.pipeline:Fold 16 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.041313\n",
      "INFO:src.pipeline:  rmse: 0.203256\n",
      "INFO:src.pipeline:  mae: 0.118935\n",
      "INFO:src.pipeline:  r2: 0.261540\n",
      "INFO:src.pipeline:  mape: 46.266155\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 17 ===\n",
      "INFO:src.pipeline:Train samples: 497, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.024693, Val Loss: 0.018129, Train MAE: 0.150920, Val MAE: 0.099087, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.020585, Val Loss: 0.017854, Train MAE: 0.137614, Val MAE: 0.108257, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 25\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 17.\n",
      "INFO:src.pipeline:Fold 17 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.035963\n",
      "INFO:src.pipeline:  rmse: 0.189638\n",
      "INFO:src.pipeline:  mae: 0.107473\n",
      "INFO:src.pipeline:  r2: 0.387269\n",
      "INFO:src.pipeline:  mape: 26.542166\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 18 ===\n",
      "INFO:src.pipeline:Train samples: 527, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.023795, Val Loss: 0.013677, Train MAE: 0.151435, Val MAE: 0.096766, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.020869, Val Loss: 0.014007, Train MAE: 0.139290, Val MAE: 0.115967, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.016759, Val Loss: 0.021018, Train MAE: 0.119996, Val MAE: 0.141515, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.012789, Val Loss: 0.022286, Train MAE: 0.105171, Val MAE: 0.125612, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 41\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 18.\n",
      "INFO:src.pipeline:Fold 18 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.041552\n",
      "INFO:src.pipeline:  rmse: 0.203844\n",
      "INFO:src.pipeline:  mae: 0.123599\n",
      "INFO:src.pipeline:  r2: 0.039030\n",
      "INFO:src.pipeline:  mape: 19.685352\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 19 ===\n",
      "INFO:src.pipeline:Train samples: 558, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.023898, Val Loss: 0.026374, Train MAE: 0.149526, Val MAE: 0.125620, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.020730, Val Loss: 0.026389, Train MAE: 0.137913, Val MAE: 0.121267, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 25\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 19.\n",
      "INFO:src.pipeline:Fold 19 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.055387\n",
      "INFO:src.pipeline:  rmse: 0.235343\n",
      "INFO:src.pipeline:  mae: 0.117671\n",
      "INFO:src.pipeline:  r2: 0.297785\n",
      "INFO:src.pipeline:  mape: 12.635724\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 20 ===\n",
      "INFO:src.pipeline:Train samples: 589, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.023054, Val Loss: 0.018885, Train MAE: 0.143848, Val MAE: 0.138909, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.020041, Val Loss: 0.020721, Train MAE: 0.129904, Val MAE: 0.154245, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 27\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 20.\n",
      "INFO:src.pipeline:Fold 20 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.048630\n",
      "INFO:src.pipeline:  rmse: 0.220521\n",
      "INFO:src.pipeline:  mae: 0.152749\n",
      "INFO:src.pipeline:  r2: 0.259879\n",
      "INFO:src.pipeline:  mape: 25.736025\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 21 ===\n",
      "INFO:src.pipeline:Train samples: 619, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.023482, Val Loss: 0.018673, Train MAE: 0.148472, Val MAE: 0.128078, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.019580, Val Loss: 0.022576, Train MAE: 0.129625, Val MAE: 0.149954, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 28\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 21.\n",
      "INFO:src.pipeline:Fold 21 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.041040\n",
      "INFO:src.pipeline:  rmse: 0.202582\n",
      "INFO:src.pipeline:  mae: 0.121127\n",
      "INFO:src.pipeline:  r2: 0.149610\n",
      "INFO:src.pipeline:  mape: 18.595581\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 22 ===\n",
      "INFO:src.pipeline:Train samples: 650, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.022920, Val Loss: 0.038257, Train MAE: 0.144673, Val MAE: 0.186678, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.020057, Val Loss: 0.035428, Train MAE: 0.135215, Val MAE: 0.140749, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.016928, Val Loss: 0.040013, Train MAE: 0.121133, Val MAE: 0.177582, LR: 0.001000\n",
      "INFO:src.engine:Early stopping at epoch 39\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 22.\n",
      "INFO:src.pipeline:Fold 22 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.077485\n",
      "INFO:src.pipeline:  rmse: 0.278362\n",
      "INFO:src.pipeline:  mae: 0.174660\n",
      "INFO:src.pipeline:  r2: 0.077387\n",
      "INFO:src.pipeline:  mape: 48.758240\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 23 ===\n",
      "INFO:src.pipeline:Train samples: 680, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.024596, Val Loss: 0.050256, Train MAE: 0.147254, Val MAE: 0.243004, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.020791, Val Loss: 0.050167, Train MAE: 0.135936, Val MAE: 0.239642, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.016149, Val Loss: 0.048597, Train MAE: 0.117114, Val MAE: 0.228210, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 31\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 23.\n",
      "INFO:src.pipeline:Fold 23 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.106905\n",
      "INFO:src.pipeline:  rmse: 0.326964\n",
      "INFO:src.pipeline:  mae: 0.240022\n",
      "INFO:src.pipeline:  r2: 0.053106\n",
      "INFO:src.pipeline:  mape: 83.487206\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 24 ===\n",
      "INFO:src.pipeline:Train samples: 711, Val samples: 21\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.025610, Val Loss: 0.066743, Train MAE: 0.155347, Val MAE: 0.283975, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.022021, Val Loss: 0.065962, Train MAE: 0.140452, Val MAE: 0.275898, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 22\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 24.\n",
      "INFO:src.pipeline:Fold 24 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.139415\n",
      "INFO:src.pipeline:  rmse: 0.373384\n",
      "INFO:src.pipeline:  mae: 0.278496\n",
      "INFO:src.pipeline:  r2: 0.095904\n",
      "INFO:src.pipeline:  mape: 83.882530\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 25 ===\n",
      "INFO:src.pipeline:Train samples: 734, Val samples: 23\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.026429, Val Loss: 0.053366, Train MAE: 0.158225, Val MAE: 0.227215, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.023021, Val Loss: 0.062348, Train MAE: 0.142145, Val MAE: 0.240848, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 23\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 25.\n",
      "INFO:src.pipeline:Fold 25 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.140074\n",
      "INFO:src.pipeline:  rmse: 0.374265\n",
      "INFO:src.pipeline:  mae: 0.259929\n",
      "INFO:src.pipeline:  r2: -0.159540\n",
      "INFO:src.pipeline:  mape: 24.705645\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 26 ===\n",
      "INFO:src.pipeline:Train samples: 759, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.027090, Val Loss: 0.039704, Train MAE: 0.159943, Val MAE: 0.218598, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.023477, Val Loss: 0.042245, Train MAE: 0.146917, Val MAE: 0.201930, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.018832, Val Loss: 0.037276, Train MAE: 0.127910, Val MAE: 0.203150, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.012980, Val Loss: 0.043894, Train MAE: 0.109903, Val MAE: 0.208270, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 44\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 26.\n",
      "INFO:src.pipeline:Fold 26 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.090278\n",
      "INFO:src.pipeline:  rmse: 0.300462\n",
      "INFO:src.pipeline:  mae: 0.208887\n",
      "INFO:src.pipeline:  r2: 0.364613\n",
      "INFO:src.pipeline:  mape: 116.385391\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 27 ===\n",
      "INFO:src.pipeline:Train samples: 790, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.027537, Val Loss: 0.031697, Train MAE: 0.162990, Val MAE: 0.163306, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.025005, Val Loss: 0.033719, Train MAE: 0.151787, Val MAE: 0.169065, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.019546, Val Loss: 0.035290, Train MAE: 0.132061, Val MAE: 0.169893, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 31\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 27.\n",
      "INFO:src.pipeline:Fold 27 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.075868\n",
      "INFO:src.pipeline:  rmse: 0.275441\n",
      "INFO:src.pipeline:  mae: 0.184951\n",
      "INFO:src.pipeline:  r2: 0.300455\n",
      "INFO:src.pipeline:  mape: 58.884914\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 28 ===\n",
      "INFO:src.pipeline:Train samples: 820, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.028162, Val Loss: 0.022614, Train MAE: 0.167958, Val MAE: 0.144288, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.025330, Val Loss: 0.021524, Train MAE: 0.152232, Val MAE: 0.134927, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.021057, Val Loss: 0.029954, Train MAE: 0.136602, Val MAE: 0.151922, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.015160, Val Loss: 0.034983, Train MAE: 0.116106, Val MAE: 0.163015, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 42\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 28.\n",
      "INFO:src.pipeline:Fold 28 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.068335\n",
      "INFO:src.pipeline:  rmse: 0.261409\n",
      "INFO:src.pipeline:  mae: 0.165170\n",
      "INFO:src.pipeline:  r2: 0.176319\n",
      "INFO:src.pipeline:  mape: 45.339687\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 29 ===\n",
      "INFO:src.pipeline:Train samples: 851, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.026917, Val Loss: 0.008809, Train MAE: 0.160681, Val MAE: 0.105148, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.023459, Val Loss: 0.007609, Train MAE: 0.146850, Val MAE: 0.089162, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.018754, Val Loss: 0.008308, Train MAE: 0.130391, Val MAE: 0.101885, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.014711, Val Loss: 0.007193, Train MAE: 0.113867, Val MAE: 0.075597, LR: 0.001000\n",
      "INFO:src.engine:Epoch [50/50] - Train Loss: 0.009984, Val Loss: 0.007337, Train MAE: 0.094417, Val MAE: 0.072147, LR: 0.000500\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 29.\n",
      "INFO:src.pipeline:Fold 29 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.014674\n",
      "INFO:src.pipeline:  rmse: 0.121136\n",
      "INFO:src.pipeline:  mae: 0.072147\n",
      "INFO:src.pipeline:  r2: 0.642620\n",
      "INFO:src.pipeline:  mape: 10.886256\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 30 ===\n",
      "INFO:src.pipeline:Train samples: 881, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.027031, Val Loss: 0.020538, Train MAE: 0.161018, Val MAE: 0.102005, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.022975, Val Loss: 0.020714, Train MAE: 0.144655, Val MAE: 0.083796, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.017573, Val Loss: 0.020244, Train MAE: 0.125764, Val MAE: 0.083106, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 38\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 30.\n",
      "INFO:src.pipeline:Fold 30 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.040924\n",
      "INFO:src.pipeline:  rmse: 0.202296\n",
      "INFO:src.pipeline:  mae: 0.084907\n",
      "INFO:src.pipeline:  r2: 0.287544\n",
      "INFO:src.pipeline:  mape: 5.047194\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 31 ===\n",
      "INFO:src.pipeline:Train samples: 911, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.026799, Val Loss: 0.004248, Train MAE: 0.158686, Val MAE: 0.042042, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.024198, Val Loss: 0.004084, Train MAE: 0.144086, Val MAE: 0.050532, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.018119, Val Loss: 0.004454, Train MAE: 0.123523, Val MAE: 0.053664, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.013398, Val Loss: 0.003947, Train MAE: 0.108609, Val MAE: 0.044633, LR: 0.000500\n",
      "INFO:src.engine:Epoch [50/50] - Train Loss: 0.011809, Val Loss: 0.003892, Train MAE: 0.099917, Val MAE: 0.043993, LR: 0.000500\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 31.\n",
      "INFO:src.pipeline:Fold 31 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.007784\n",
      "INFO:src.pipeline:  rmse: 0.088227\n",
      "INFO:src.pipeline:  mae: 0.043993\n",
      "INFO:src.pipeline:  r2: 0.799588\n",
      "INFO:src.pipeline:  mape: 6.177649\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 32 ===\n",
      "INFO:src.pipeline:Train samples: 942, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.025809, Val Loss: 0.010248, Train MAE: 0.152514, Val MAE: 0.060743, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.023822, Val Loss: 0.010146, Train MAE: 0.144593, Val MAE: 0.087108, LR: 0.001000\n",
      "INFO:src.engine:Early stopping at epoch 29\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 32.\n",
      "INFO:src.pipeline:Fold 32 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.021876\n",
      "INFO:src.pipeline:  rmse: 0.147905\n",
      "INFO:src.pipeline:  mae: 0.075413\n",
      "INFO:src.pipeline:  r2: 0.446691\n",
      "INFO:src.pipeline:  mape: 10.177034\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 33 ===\n",
      "INFO:src.pipeline:Train samples: 972, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.024814, Val Loss: 0.051907, Train MAE: 0.146493, Val MAE: 0.228969, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.021571, Val Loss: 0.056872, Train MAE: 0.132775, Val MAE: 0.234370, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 27\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 33.\n",
      "INFO:src.pipeline:Fold 33 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.132702\n",
      "INFO:src.pipeline:  rmse: 0.364283\n",
      "INFO:src.pipeline:  mae: 0.231969\n",
      "INFO:src.pipeline:  r2: -0.020023\n",
      "INFO:src.pipeline:  mape: 107.300316\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 34 ===\n",
      "INFO:src.pipeline:Train samples: 1003, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.026175, Val Loss: 0.036864, Train MAE: 0.155132, Val MAE: 0.217704, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.023404, Val Loss: 0.039472, Train MAE: 0.142540, Val MAE: 0.186458, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.018804, Val Loss: 0.051317, Train MAE: 0.124569, Val MAE: 0.238508, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 31\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 34.\n",
      "INFO:src.pipeline:Fold 34 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.116114\n",
      "INFO:src.pipeline:  rmse: 0.340756\n",
      "INFO:src.pipeline:  mae: 0.252805\n",
      "INFO:src.pipeline:  r2: -0.149029\n",
      "INFO:src.pipeline:  mape: 69.719948\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 35 ===\n",
      "INFO:src.pipeline:Train samples: 1033, Val samples: 21\n",
      "INFO:src.pipeline:Model parameters: 247,307\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.026418, Val Loss: 0.084932, Train MAE: 0.154451, Val MAE: 0.309301, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.023207, Val Loss: 0.094938, Train MAE: 0.142987, Val MAE: 0.326713, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 21\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 35.\n",
      "INFO:src.pipeline:Fold 35 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.190529\n",
      "INFO:src.pipeline:  rmse: 0.436497\n",
      "INFO:src.pipeline:  mae: 0.343321\n",
      "INFO:src.pipeline:  r2: -0.194370\n",
      "INFO:src.pipeline:  mape: 63.040531\n",
      "INFO:src.pipeline:\n",
      "=== Experiment Summary ===\n",
      "INFO:src.pipeline:Average Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.074826 ± 0.047808\n",
      "INFO:src.pipeline:  rmse: 0.258599 ± 0.089179\n",
      "INFO:src.pipeline:  mae: 0.170743 ± 0.078700\n",
      "INFO:src.pipeline:  r2: 0.198974 ± 0.240384\n",
      "INFO:src.pipeline:  mape: 45.116521 ± 33.649637\n"
     ]
    }
   ],
   "source": [
    "from src.preprocessing import KBinConfig, process_splits_to_kbins, build_model_arrays, to_fixedgrid_multiindex\n",
    "import pandas as pd\n",
    "from src.utils import DataManager\n",
    "from src.pipeline import SolarForecastingPipeline\n",
    "\n",
    "\n",
    "df_phase5 = df_interpolated[['CSI_ghi', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos',\n",
    "     'nam_cc',\n",
    "        '80_cloud_cover',\n",
    "       '56_cloud_cover', '20_cloud_cover',\n",
    "       '88_cloud_cover']]\n",
    "\n",
    "\n",
    "# --- Block 1: Define variables ---\n",
    "USE_KBINS = False\n",
    "k_bins = 60\n",
    "TARGET_COL = \"CSI_ghi\"  # <-- FIX 1: Define Target FIRST\n",
    "history_days = 7\n",
    "horizon_days = 1\n",
    "\n",
    "# This list now correctly excludes the target\n",
    "feature_cols = [c for c in df_phase5.columns.tolist() if c != TARGET_COL]\n",
    "\n",
    "# --- Block 2: Build model arrays (Your code was correct here) ---\n",
    "if USE_KBINS:\n",
    "        print(\"\\n--- Step 2: Converting full dataset to K-Bins format ---\")\n",
    "        # ... (rest of your K-Bins code) ...\n",
    "else:\n",
    "    print(\"\\n--- Step 2: Building model arrays (X, Y) ---\")\n",
    "    fixed_df = to_fixedgrid_multiindex(df_phase5, timestamp_col=\"measurement_time\", expected_T=None)  # or set T\n",
    "    \n",
    "    # You correctly passed the filtered 'feature_cols' here, fixing the data leak!\n",
    "    ph5_X, ph5_Y, ph5_labels_list = build_model_arrays(\n",
    "        fixed_df,\n",
    "        feature_cols=feature_cols,  # <-- This is correct!\n",
    "        target_col=TARGET_COL,\n",
    "        history_days=history_days,\n",
    "        horizon_days=horizon_days,\n",
    "    )\n",
    "\n",
    "# --- Block 3: Save arrays ---\n",
    "data_manager = DataManager()\n",
    "data_manager.save_arrays(\n",
    "    ph5_X, ph5_Y,\n",
    "    pd.DataFrame(index=pd.to_datetime(ph5_labels_list, utc=True)),\n",
    "    filename_prefix='phas5_data',  # <-- You are saving as 'phas2_data'\n",
    "    feature_cols=feature_cols,\n",
    "    target_col=TARGET_COL,\n",
    "    metadata={\n",
    "        \"input_csv\": \"data/processed/dayTime_NAM_spatial_5locations_dayahead_features_processed.csv\",\n",
    "        \"timestamp_col\": \"measurement_time\",\n",
    "        \"feature_set\": feature_cols,\n",
    "        \"history_days\": 7,\n",
    "        \"horizon_days\": 1,\n",
    "        \"k_bins\": None,\n",
    "    }\n",
    ")\n",
    "\n",
    "# --- Block 4: Configure and Run ---\n",
    "LSTM_CONFIG = {\n",
    "    \"experiment_name\": \"LSTM_phase5_exp02\",\n",
    "    \"model_type\": \"LSTM\",\n",
    "    \"model_config\": {\n",
    "        \"hidden_size\": 64,\n",
    "        \"num_layers\": 2,\n",
    "        \"dropout\": 0.25,\n",
    "        \"bidirectional\": True,\n",
    "    },\n",
    "    \"data_prefix\": \"phas5_data\",  # <-- FIX 2: Match the filename_prefix\n",
    "    \"splits_file\": \"exp-003/exp-003rolling_origin_splits.json\",\n",
    "    \"feature_cols\": feature_cols,\n",
    "    \"feature_selection\": feature_cols, # <-- This is correct!\n",
    "    \"target_col\": TARGET_COL,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_epochs\": 50,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"loss_function\": \"Huber\",  # <-- This setting needs the fix below\n",
    "    \"early_stopping_patience\": 20,\n",
    "    \"max_folds\": 35,\n",
    "}\n",
    "\n",
    "# This will now work after you apply the fixes below\n",
    "pipeline = SolarForecastingPipeline(LSTM_CONFIG)\n",
    "phas5_fold_results, summary = pipeline.run()\n",
    "\n",
    "# To get the model from the LAST fold:\n",
    "phas5_fold_results = phas3_fold_results[-1]['model']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a65f76",
   "metadata": {},
   "source": [
    "Inconsistent Results Across Folds: When adding the spatial-temporal features, the model shows considerable variation in performance across the folds. Some folds perform worse (e.g., Fold 3: RMSE 0.2124 to 0.466), while others perform better (e.g., Fold 31: RMSE 0.0918). This suggests that the added features may be contributing noise or irrelevant complexity, which the model is struggling to handle uniformly across different subsets of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc804f53",
   "metadata": {},
   "source": [
    "The higher variance in metrics across folds, especially in terms of RMSE and MAE, indicates that the model is not stable with the added features. This suggests that while the added features may hold some information, their inclusion doesn’t consistently improve predictive accuracy across all training and validation splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1978bd01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'history': {'train_loss': [0.06003684177994728,\n",
       "    0.059081628918647766,\n",
       "    0.05886007845401764,\n",
       "    0.05694744363427162,\n",
       "    0.0553533211350441,\n",
       "    0.053611330687999725,\n",
       "    0.05284370481967926,\n",
       "    0.052881281822919846,\n",
       "    0.05291888490319252,\n",
       "    0.05297527462244034,\n",
       "    0.04997491464018822,\n",
       "    0.053134702146053314,\n",
       "    0.04904967173933983,\n",
       "    0.0478777177631855,\n",
       "    0.047167930752038956,\n",
       "    0.046798259019851685,\n",
       "    0.04694787412881851,\n",
       "    0.04371234029531479,\n",
       "    0.044653840363025665,\n",
       "    0.0406917929649353,\n",
       "    0.04292237013578415,\n",
       "    0.03734507039189339,\n",
       "    0.03371132165193558,\n",
       "    0.030639274045825005,\n",
       "    0.03304049372673035,\n",
       "    0.029997609555721283,\n",
       "    0.02925606444478035,\n",
       "    0.02697451412677765,\n",
       "    0.02603142336010933,\n",
       "    0.026743434369564056,\n",
       "    0.024257680401206017,\n",
       "    0.02271345816552639,\n",
       "    0.02418750710785389,\n",
       "    0.028082501143217087,\n",
       "    0.02268541231751442,\n",
       "    0.018612494692206383,\n",
       "    0.022291239351034164,\n",
       "    0.023941392078995705,\n",
       "    0.020655987784266472,\n",
       "    0.020717311650514603,\n",
       "    0.02025049552321434,\n",
       "    0.017783040180802345,\n",
       "    0.017522064968943596,\n",
       "    0.016095982864499092],\n",
       "   'val_loss': [0.09866582602262497,\n",
       "    0.09783302992582321,\n",
       "    0.09704198688268661,\n",
       "    0.09622205793857574,\n",
       "    0.09525701403617859,\n",
       "    0.09400943666696548,\n",
       "    0.09247295558452606,\n",
       "    0.09080058336257935,\n",
       "    0.09004609286785126,\n",
       "    0.08985322713851929,\n",
       "    0.08958656340837479,\n",
       "    0.08893394470214844,\n",
       "    0.0876365453004837,\n",
       "    0.08585827052593231,\n",
       "    0.08383392542600632,\n",
       "    0.08183281123638153,\n",
       "    0.07995512336492538,\n",
       "    0.07813270390033722,\n",
       "    0.07615675032138824,\n",
       "    0.07389504462480545,\n",
       "    0.07243393361568451,\n",
       "    0.0716053918004036,\n",
       "    0.07131533324718475,\n",
       "    0.0707559660077095,\n",
       "    0.07136493921279907,\n",
       "    0.07293448597192764,\n",
       "    0.07492406666278839,\n",
       "    0.07554207742214203,\n",
       "    0.07624770700931549,\n",
       "    0.0768333226442337,\n",
       "    0.07742251455783844,\n",
       "    0.07785854488611221,\n",
       "    0.0777415931224823,\n",
       "    0.07727716118097305,\n",
       "    0.07747605443000793,\n",
       "    0.0775834396481514,\n",
       "    0.07723426073789597,\n",
       "    0.076830193400383,\n",
       "    0.07651925086975098,\n",
       "    0.07616252452135086,\n",
       "    0.07594573497772217,\n",
       "    0.07584536075592041,\n",
       "    0.07581766694784164,\n",
       "    0.07584173232316971],\n",
       "   'train_mae': [0.25549373030662537,\n",
       "    0.2526325583457947,\n",
       "    0.2500351369380951,\n",
       "    0.24755486845970154,\n",
       "    0.24418604373931885,\n",
       "    0.2399939000606537,\n",
       "    0.23473094403743744,\n",
       "    0.2322060614824295,\n",
       "    0.22859464585781097,\n",
       "    0.22573912143707275,\n",
       "    0.21825933456420898,\n",
       "    0.21635816991329193,\n",
       "    0.20472830533981323,\n",
       "    0.20761503279209137,\n",
       "    0.2087285965681076,\n",
       "    0.207489475607872,\n",
       "    0.21232958137989044,\n",
       "    0.2051735371351242,\n",
       "    0.20266088843345642,\n",
       "    0.1884879469871521,\n",
       "    0.19794081151485443,\n",
       "    0.1818520724773407,\n",
       "    0.17304964363574982,\n",
       "    0.16448339819908142,\n",
       "    0.17378948628902435,\n",
       "    0.16322527825832367,\n",
       "    0.15957869589328766,\n",
       "    0.14493748545646667,\n",
       "    0.14037805795669556,\n",
       "    0.155052050948143,\n",
       "    0.1530487984418869,\n",
       "    0.13856366276741028,\n",
       "    0.13783572614192963,\n",
       "    0.1517500877380371,\n",
       "    0.12785358726978302,\n",
       "    0.12316443771123886,\n",
       "    0.14382076263427734,\n",
       "    0.15341205894947052,\n",
       "    0.1411556452512741,\n",
       "    0.14014917612075806,\n",
       "    0.13562089204788208,\n",
       "    0.13312149047851562,\n",
       "    0.12745091319084167,\n",
       "    0.12759055197238922],\n",
       "   'val_mae': [0.34586161375045776,\n",
       "    0.3436753749847412,\n",
       "    0.341677188873291,\n",
       "    0.3396679162979126,\n",
       "    0.3373381793498993,\n",
       "    0.33454081416130066,\n",
       "    0.33103838562965393,\n",
       "    0.32730862498283386,\n",
       "    0.32510969042778015,\n",
       "    0.32424503564834595,\n",
       "    0.32335856556892395,\n",
       "    0.32256025075912476,\n",
       "    0.32122644782066345,\n",
       "    0.3197256028652191,\n",
       "    0.3179199993610382,\n",
       "    0.3158930242061615,\n",
       "    0.31352531909942627,\n",
       "    0.31072312593460083,\n",
       "    0.3075943887233734,\n",
       "    0.3034736216068268,\n",
       "    0.3003136217594147,\n",
       "    0.2984689474105835,\n",
       "    0.2979101240634918,\n",
       "    0.29738444089889526,\n",
       "    0.298634797334671,\n",
       "    0.3008671700954437,\n",
       "    0.303356409072876,\n",
       "    0.30548664927482605,\n",
       "    0.30866101384162903,\n",
       "    0.31142929196357727,\n",
       "    0.31362059712409973,\n",
       "    0.3148425817489624,\n",
       "    0.3154776990413666,\n",
       "    0.3161035478115082,\n",
       "    0.31795090436935425,\n",
       "    0.318742573261261,\n",
       "    0.3192324638366699,\n",
       "    0.3193384110927582,\n",
       "    0.3192938566207886,\n",
       "    0.31937769055366516,\n",
       "    0.3196190595626831,\n",
       "    0.319476842880249,\n",
       "    0.3195214569568634,\n",
       "    0.31963685154914856],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005]},\n",
       "  'metrics': {'mse': 0.15170642733573914,\n",
       "   'rmse': 0.3894950928262629,\n",
       "   'mae': 0.31963688135147095,\n",
       "   'r2': 0.08344894647598267,\n",
       "   'mape': 81.203369140625},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.06416858918964863,\n",
       "    0.07969227805733681,\n",
       "    0.0666227675974369,\n",
       "    0.08474689908325672,\n",
       "    0.0561613105237484,\n",
       "    0.056837914511561394,\n",
       "    0.06758140213787556,\n",
       "    0.08036716282367706,\n",
       "    0.062167927622795105,\n",
       "    0.0512021966278553,\n",
       "    0.05090458132326603,\n",
       "    0.053224340081214905,\n",
       "    0.04525951109826565,\n",
       "    0.055910708382725716,\n",
       "    0.053750403225421906,\n",
       "    0.0373664814978838,\n",
       "    0.03630437422543764,\n",
       "    0.03898312710225582,\n",
       "    0.040067607536911964,\n",
       "    0.046912044286727905,\n",
       "    0.03607586771249771,\n",
       "    0.04244311526417732,\n",
       "    0.023997285403311253,\n",
       "    0.028372149914503098,\n",
       "    0.026159613393247128,\n",
       "    0.042973555624485016,\n",
       "    0.02707718964666128,\n",
       "    0.03270685952156782,\n",
       "    0.03344408329576254,\n",
       "    0.025278820656239986,\n",
       "    0.02585465833544731,\n",
       "    0.026773061137646437,\n",
       "    0.021030688658356667,\n",
       "    0.028014271520078182,\n",
       "    0.02305351523682475,\n",
       "    0.02964208461344242,\n",
       "    0.026332699693739414,\n",
       "    0.02940695732831955,\n",
       "    0.03351951204240322,\n",
       "    0.0296437107026577],\n",
       "   'val_loss': [0.05770126357674599,\n",
       "    0.05472533404827118,\n",
       "    0.05219636857509613,\n",
       "    0.04983317852020264,\n",
       "    0.04911147058010101,\n",
       "    0.050200074911117554,\n",
       "    0.051478222012519836,\n",
       "    0.05427861958742142,\n",
       "    0.06124148145318031,\n",
       "    0.0632871761918068,\n",
       "    0.05804600939154625,\n",
       "    0.05443418025970459,\n",
       "    0.05093630030751228,\n",
       "    0.04634124040603638,\n",
       "    0.043015312403440475,\n",
       "    0.0418689101934433,\n",
       "    0.04119621589779854,\n",
       "    0.04024923965334892,\n",
       "    0.039197470992803574,\n",
       "    0.03861423209309578,\n",
       "    0.04135281220078468,\n",
       "    0.04236840829253197,\n",
       "    0.040372297167778015,\n",
       "    0.041555255651474,\n",
       "    0.04504774138331413,\n",
       "    0.043831419199705124,\n",
       "    0.05255390331149101,\n",
       "    0.07479143142700195,\n",
       "    0.0599331371486187,\n",
       "    0.0424746572971344,\n",
       "    0.039890535175800323,\n",
       "    0.039595093578100204,\n",
       "    0.039421506226062775,\n",
       "    0.04037174955010414,\n",
       "    0.04292965307831764,\n",
       "    0.04598085954785347,\n",
       "    0.048347704112529755,\n",
       "    0.05019117146730423,\n",
       "    0.051353584975004196,\n",
       "    0.0519736222922802],\n",
       "   'train_mae': [0.2923143729567528,\n",
       "    0.32878510653972626,\n",
       "    0.2953139841556549,\n",
       "    0.3252017945051193,\n",
       "    0.2705688774585724,\n",
       "    0.27762259542942047,\n",
       "    0.29423779249191284,\n",
       "    0.3110828995704651,\n",
       "    0.284056693315506,\n",
       "    0.2576928362250328,\n",
       "    0.2559037059545517,\n",
       "    0.2578763887286186,\n",
       "    0.2428547963500023,\n",
       "    0.2577321529388428,\n",
       "    0.2614152431488037,\n",
       "    0.20079955458641052,\n",
       "    0.1972206011414528,\n",
       "    0.19835620373487473,\n",
       "    0.20157355070114136,\n",
       "    0.2081657275557518,\n",
       "    0.1843796968460083,\n",
       "    0.2048756405711174,\n",
       "    0.15231547504663467,\n",
       "    0.15501072257757187,\n",
       "    0.15913739055395126,\n",
       "    0.19811926037073135,\n",
       "    0.16657983511686325,\n",
       "    0.18853173404932022,\n",
       "    0.19190648943185806,\n",
       "    0.1504637636244297,\n",
       "    0.16090521216392517,\n",
       "    0.1536230742931366,\n",
       "    0.1414262279868126,\n",
       "    0.15680257976055145,\n",
       "    0.1500435173511505,\n",
       "    0.16520658135414124,\n",
       "    0.15802084654569626,\n",
       "    0.16667181998491287,\n",
       "    0.16843216121196747,\n",
       "    0.17044758796691895],\n",
       "   'val_mae': [0.2868775725364685,\n",
       "    0.2806684076786041,\n",
       "    0.27602115273475647,\n",
       "    0.2720817029476166,\n",
       "    0.2736457288265228,\n",
       "    0.27736896276474,\n",
       "    0.2802140414714813,\n",
       "    0.28911304473876953,\n",
       "    0.3062623143196106,\n",
       "    0.31196656823158264,\n",
       "    0.3022357225418091,\n",
       "    0.2944808900356293,\n",
       "    0.28495097160339355,\n",
       "    0.2685377597808838,\n",
       "    0.25251254439353943,\n",
       "    0.2461490035057068,\n",
       "    0.24153216183185577,\n",
       "    0.2397625893354416,\n",
       "    0.2358567714691162,\n",
       "    0.22994962334632874,\n",
       "    0.2504742443561554,\n",
       "    0.2543186545372009,\n",
       "    0.2363959103822708,\n",
       "    0.21877549588680267,\n",
       "    0.21122314035892487,\n",
       "    0.2264237403869629,\n",
       "    0.2958929240703583,\n",
       "    0.35477927327156067,\n",
       "    0.31692442297935486,\n",
       "    0.24909375607967377,\n",
       "    0.21901075541973114,\n",
       "    0.21699871122837067,\n",
       "    0.2215651273727417,\n",
       "    0.23429571092128754,\n",
       "    0.2496785670518875,\n",
       "    0.2638782262802124,\n",
       "    0.2735165059566498,\n",
       "    0.28047996759414673,\n",
       "    0.2846301198005676,\n",
       "    0.28704410791397095],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005]},\n",
       "  'metrics': {'mse': 0.1039472445845604,\n",
       "   'rmse': 0.32240850575715335,\n",
       "   'mae': 0.28704413771629333,\n",
       "   'r2': 0.06550604104995728,\n",
       "   'mape': 72.9692611694336},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.06369752809405327,\n",
       "    0.07744146262605985,\n",
       "    0.06402349720398585,\n",
       "    0.06087345505754153,\n",
       "    0.06169188519318899,\n",
       "    0.0520206056535244,\n",
       "    0.05693471555908521,\n",
       "    0.049217390517393746,\n",
       "    0.05393077557285627,\n",
       "    0.04715893045067787,\n",
       "    0.04276937618851662,\n",
       "    0.044055952380100884,\n",
       "    0.039159788439671196,\n",
       "    0.049592221776644387,\n",
       "    0.040695808827877045,\n",
       "    0.04207648212711016,\n",
       "    0.03720266496141752,\n",
       "    0.040908707305788994,\n",
       "    0.032485129311680794,\n",
       "    0.038348520174622536,\n",
       "    0.031610346088806786,\n",
       "    0.03224086823562781,\n",
       "    0.04427427612245083,\n",
       "    0.03330689171950022,\n",
       "    0.040789367631077766,\n",
       "    0.03371818115313848,\n",
       "    0.03509928286075592,\n",
       "    0.03758615938325723,\n",
       "    0.03248435507218043,\n",
       "    0.038603742296497025,\n",
       "    0.028535325701038044,\n",
       "    0.02575154198954503,\n",
       "    0.028357392797867458,\n",
       "    0.026571953979631264,\n",
       "    0.033586585273345314,\n",
       "    0.03049057846268018,\n",
       "    0.026669050256411236,\n",
       "    0.030614705756306648,\n",
       "    0.0325519305964311,\n",
       "    0.032307639718055725,\n",
       "    0.025204307089249294,\n",
       "    0.03362979739904404,\n",
       "    0.032335774352153145,\n",
       "    0.024072018762429554,\n",
       "    0.031030116602778435,\n",
       "    0.02153863540540139,\n",
       "    0.026032570128639538,\n",
       "    0.02774816316862901,\n",
       "    0.024901308119297028,\n",
       "    0.02275252901017666],\n",
       "   'val_loss': [0.03658197447657585,\n",
       "    0.03158765658736229,\n",
       "    0.034342531114816666,\n",
       "    0.03203785791993141,\n",
       "    0.03183828666806221,\n",
       "    0.032375551760196686,\n",
       "    0.03192136809229851,\n",
       "    0.03172764927148819,\n",
       "    0.03001077100634575,\n",
       "    0.027561431750655174,\n",
       "    0.025750601664185524,\n",
       "    0.025331886485219002,\n",
       "    0.024942705407738686,\n",
       "    0.024731682613492012,\n",
       "    0.02464938350021839,\n",
       "    0.02447652444243431,\n",
       "    0.025317270308732986,\n",
       "    0.026787275448441505,\n",
       "    0.02890566736459732,\n",
       "    0.028956886380910873,\n",
       "    0.026645086705684662,\n",
       "    0.026170717552304268,\n",
       "    0.025701096281409264,\n",
       "    0.025867247954010963,\n",
       "    0.024634521454572678,\n",
       "    0.022573895752429962,\n",
       "    0.02245994098484516,\n",
       "    0.023611988872289658,\n",
       "    0.023552898317575455,\n",
       "    0.02218475006520748,\n",
       "    0.02161099761724472,\n",
       "    0.023157592862844467,\n",
       "    0.025325192138552666,\n",
       "    0.02208220399916172,\n",
       "    0.02142888866364956,\n",
       "    0.022196335718035698,\n",
       "    0.022562937811017036,\n",
       "    0.02206822670996189,\n",
       "    0.0230450090020895,\n",
       "    0.02481280453503132,\n",
       "    0.02329591102898121,\n",
       "    0.021408403292298317,\n",
       "    0.021750353276729584,\n",
       "    0.021424051374197006,\n",
       "    0.022249091416597366,\n",
       "    0.024741575121879578,\n",
       "    0.021469630300998688,\n",
       "    0.021841678768396378,\n",
       "    0.02159024216234684,\n",
       "    0.020742444321513176],\n",
       "   'train_mae': [0.29284321268399555,\n",
       "    0.3257165352503459,\n",
       "    0.2936607201894124,\n",
       "    0.28242552280426025,\n",
       "    0.2896217455466588,\n",
       "    0.27002479135990143,\n",
       "    0.27663464347521466,\n",
       "    0.261147677898407,\n",
       "    0.2631320208311081,\n",
       "    0.2484229157368342,\n",
       "    0.21950483322143555,\n",
       "    0.22756234804789224,\n",
       "    0.20533939202626547,\n",
       "    0.2370487799247106,\n",
       "    0.21122084061304727,\n",
       "    0.2127391199270884,\n",
       "    0.20174129803975424,\n",
       "    0.2231883406639099,\n",
       "    0.190581684311231,\n",
       "    0.19188280403614044,\n",
       "    0.1815310368935267,\n",
       "    0.17870967586835226,\n",
       "    0.20798506836096445,\n",
       "    0.19359743098417917,\n",
       "    0.21632833778858185,\n",
       "    0.20602128406365713,\n",
       "    0.20531010627746582,\n",
       "    0.20720531046390533,\n",
       "    0.18099225064118704,\n",
       "    0.18554587165514627,\n",
       "    0.17121302088101706,\n",
       "    0.17147532105445862,\n",
       "    0.17069107294082642,\n",
       "    0.16752955317497253,\n",
       "    0.19098603228727976,\n",
       "    0.18174557387828827,\n",
       "    0.1637940208117167,\n",
       "    0.17351782321929932,\n",
       "    0.17335829635461172,\n",
       "    0.17731441060702005,\n",
       "    0.15921339889367422,\n",
       "    0.185105433066686,\n",
       "    0.18286069730917612,\n",
       "    0.16860567529996237,\n",
       "    0.1810975025097529,\n",
       "    0.1521578406294187,\n",
       "    0.15647739171981812,\n",
       "    0.16324839492638907,\n",
       "    0.16559998691082,\n",
       "    0.15965413053830466],\n",
       "   'val_mae': [0.22556151449680328,\n",
       "    0.21659345924854279,\n",
       "    0.21892203390598297,\n",
       "    0.2183350920677185,\n",
       "    0.21460139751434326,\n",
       "    0.21381635963916779,\n",
       "    0.21210820972919464,\n",
       "    0.2103773057460785,\n",
       "    0.19946211576461792,\n",
       "    0.18344904482364655,\n",
       "    0.1675863415002823,\n",
       "    0.16048943996429443,\n",
       "    0.16049543023109436,\n",
       "    0.16205178201198578,\n",
       "    0.1640491932630539,\n",
       "    0.15514861047267914,\n",
       "    0.14847257733345032,\n",
       "    0.14587244391441345,\n",
       "    0.1468270868062973,\n",
       "    0.1449684351682663,\n",
       "    0.13804256916046143,\n",
       "    0.1351955085992813,\n",
       "    0.13592199981212616,\n",
       "    0.1374754011631012,\n",
       "    0.13852323591709137,\n",
       "    0.1502571851015091,\n",
       "    0.1412895917892456,\n",
       "    0.13108915090560913,\n",
       "    0.1265152245759964,\n",
       "    0.12678927183151245,\n",
       "    0.12856318056583405,\n",
       "    0.13104578852653503,\n",
       "    0.14329028129577637,\n",
       "    0.1351822167634964,\n",
       "    0.13179734349250793,\n",
       "    0.1311136782169342,\n",
       "    0.12792153656482697,\n",
       "    0.13000443577766418,\n",
       "    0.12454812973737717,\n",
       "    0.12644726037979126,\n",
       "    0.12560854852199554,\n",
       "    0.13124923408031464,\n",
       "    0.14816677570343018,\n",
       "    0.14270104467868805,\n",
       "    0.12542642652988434,\n",
       "    0.1329643875360489,\n",
       "    0.12511835992336273,\n",
       "    0.14825381338596344,\n",
       "    0.1508161425590515,\n",
       "    0.14345651865005493],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001]},\n",
       "  'metrics': {'mse': 0.04148488491773605,\n",
       "   'rmse': 0.20367838598569082,\n",
       "   'mae': 0.14345653355121613,\n",
       "   'r2': 0.5000544786453247,\n",
       "   'mape': 32.4563102722168},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.059449538588523865,\n",
       "    0.05592306237667799,\n",
       "    0.049204918555915356,\n",
       "    0.06195576209574938,\n",
       "    0.04628328653052449,\n",
       "    0.04565663356333971,\n",
       "    0.05956124234944582,\n",
       "    0.049106160178780556,\n",
       "    0.044541649520397186,\n",
       "    0.03720840159803629,\n",
       "    0.043214403092861176,\n",
       "    0.039069199468940496,\n",
       "    0.046270107850432396,\n",
       "    0.03438613237813115,\n",
       "    0.03390065999701619,\n",
       "    0.03277271194383502,\n",
       "    0.03391501773148775,\n",
       "    0.04108652286231518,\n",
       "    0.029396147467195988,\n",
       "    0.036815958097577095,\n",
       "    0.030052953865379095,\n",
       "    0.03865288011729717,\n",
       "    0.03414418688043952,\n",
       "    0.03371778782457113,\n",
       "    0.03072591871023178,\n",
       "    0.027943206019699574,\n",
       "    0.039553483948111534,\n",
       "    0.025028546922840178,\n",
       "    0.03303006058558822,\n",
       "    0.023791005136445165,\n",
       "    0.02555384999141097,\n",
       "    0.02374418661929667,\n",
       "    0.026594122406095266,\n",
       "    0.02713884925469756,\n",
       "    0.029941340442746878,\n",
       "    0.02660667710006237,\n",
       "    0.024367161560803652,\n",
       "    0.030938105657696724,\n",
       "    0.026379082817584276,\n",
       "    0.020598046714439988,\n",
       "    0.03128363145515323,\n",
       "    0.021829779259860516,\n",
       "    0.022730405442416668,\n",
       "    0.021648470778018236,\n",
       "    0.02186083677224815,\n",
       "    0.02974611846730113,\n",
       "    0.02725644549354911,\n",
       "    0.019280157051980495],\n",
       "   'val_loss': [0.028762081637978554,\n",
       "    0.021517137065529823,\n",
       "    0.01701037585735321,\n",
       "    0.013860098086297512,\n",
       "    0.013066306710243225,\n",
       "    0.013556581921875477,\n",
       "    0.012590749189257622,\n",
       "    0.013600409962236881,\n",
       "    0.013387339189648628,\n",
       "    0.012800730764865875,\n",
       "    0.012905116192996502,\n",
       "    0.012520377524197102,\n",
       "    0.012427154928445816,\n",
       "    0.012674045749008656,\n",
       "    0.01245596818625927,\n",
       "    0.014778878539800644,\n",
       "    0.013240219093859196,\n",
       "    0.011489255353808403,\n",
       "    0.011899655684828758,\n",
       "    0.012757360935211182,\n",
       "    0.013590658083558083,\n",
       "    0.013596979901194572,\n",
       "    0.012865173630416393,\n",
       "    0.01264698151499033,\n",
       "    0.01323927752673626,\n",
       "    0.01743648760020733,\n",
       "    0.01477180514484644,\n",
       "    0.011425362899899483,\n",
       "    0.01167573407292366,\n",
       "    0.013239290565252304,\n",
       "    0.013185335323214531,\n",
       "    0.012107187882065773,\n",
       "    0.011690299957990646,\n",
       "    0.01148148812353611,\n",
       "    0.012583880685269833,\n",
       "    0.012978426180779934,\n",
       "    0.01197776384651661,\n",
       "    0.012342650443315506,\n",
       "    0.012612476013600826,\n",
       "    0.0128336101770401,\n",
       "    0.012924103997647762,\n",
       "    0.012361099012196064,\n",
       "    0.011863704770803452,\n",
       "    0.011650207452476025,\n",
       "    0.01162785105407238,\n",
       "    0.011767682619392872,\n",
       "    0.01184246689081192,\n",
       "    0.011773125268518925],\n",
       "   'train_mae': [0.2836090438067913,\n",
       "    0.2682575024664402,\n",
       "    0.23655571788549423,\n",
       "    0.25452711433172226,\n",
       "    0.2304752878844738,\n",
       "    0.23690535500645638,\n",
       "    0.26782649382948875,\n",
       "    0.24938275292515755,\n",
       "    0.23736726492643356,\n",
       "    0.2151612527668476,\n",
       "    0.22514765337109566,\n",
       "    0.20745060220360756,\n",
       "    0.228132463991642,\n",
       "    0.1970439925789833,\n",
       "    0.19886307045817375,\n",
       "    0.18855875357985497,\n",
       "    0.18170371279120445,\n",
       "    0.20442988350987434,\n",
       "    0.1771251279860735,\n",
       "    0.1975906826555729,\n",
       "    0.17658031359314919,\n",
       "    0.2016509287059307,\n",
       "    0.19230296090245247,\n",
       "    0.18964328989386559,\n",
       "    0.18715588003396988,\n",
       "    0.16972566395998,\n",
       "    0.1874077506363392,\n",
       "    0.15626361593604088,\n",
       "    0.18298041820526123,\n",
       "    0.15743579156696796,\n",
       "    0.15736696869134903,\n",
       "    0.15096681006252766,\n",
       "    0.15657583065330982,\n",
       "    0.16592856124043465,\n",
       "    0.17750653252005577,\n",
       "    0.16155505180358887,\n",
       "    0.15857774019241333,\n",
       "    0.18557018786668777,\n",
       "    0.16692334413528442,\n",
       "    0.14154494367539883,\n",
       "    0.1753922961652279,\n",
       "    0.14327950030565262,\n",
       "    0.14879266917705536,\n",
       "    0.15074463561177254,\n",
       "    0.14842211082577705,\n",
       "    0.17158479616045952,\n",
       "    0.1612382009625435,\n",
       "    0.13620220683515072],\n",
       "   'val_mae': [0.20458854734897614,\n",
       "    0.17072492837905884,\n",
       "    0.1456683874130249,\n",
       "    0.13438740372657776,\n",
       "    0.13133788108825684,\n",
       "    0.11739270389080048,\n",
       "    0.11804713308811188,\n",
       "    0.13848839700222015,\n",
       "    0.13696745038032532,\n",
       "    0.12580034136772156,\n",
       "    0.12191906571388245,\n",
       "    0.12288019061088562,\n",
       "    0.1222316324710846,\n",
       "    0.12392482906579971,\n",
       "    0.1105552688241005,\n",
       "    0.10971628874540329,\n",
       "    0.09789790213108063,\n",
       "    0.10223153978586197,\n",
       "    0.11225534975528717,\n",
       "    0.1013694480061531,\n",
       "    0.10407175868749619,\n",
       "    0.10474871844053268,\n",
       "    0.09925907850265503,\n",
       "    0.0983097106218338,\n",
       "    0.09886735677719116,\n",
       "    0.1203121766448021,\n",
       "    0.10463157296180725,\n",
       "    0.09805313497781754,\n",
       "    0.09997053444385529,\n",
       "    0.1034439206123352,\n",
       "    0.10023029893636703,\n",
       "    0.09602318704128265,\n",
       "    0.09870877861976624,\n",
       "    0.09823241829872131,\n",
       "    0.09308484941720963,\n",
       "    0.09532754868268967,\n",
       "    0.09543114900588989,\n",
       "    0.09769652783870697,\n",
       "    0.09508077055215836,\n",
       "    0.0954386368393898,\n",
       "    0.09695100784301758,\n",
       "    0.09770234674215317,\n",
       "    0.10222040861845016,\n",
       "    0.10368473827838898,\n",
       "    0.1026291698217392,\n",
       "    0.09960047900676727,\n",
       "    0.0973365306854248,\n",
       "    0.09770087897777557],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005]},\n",
       "  'metrics': {'mse': 0.023546252399683,\n",
       "   'rmse': 0.15344788170477622,\n",
       "   'mae': 0.09770087897777557,\n",
       "   'r2': 0.49483931064605713,\n",
       "   'mape': 15.185403823852539},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.07135388702154159,\n",
       "    0.043080438673496244,\n",
       "    0.04837194085121155,\n",
       "    0.04164638966321945,\n",
       "    0.03737262785434723,\n",
       "    0.03313495814800262,\n",
       "    0.035105335712432864,\n",
       "    0.033657876402139665,\n",
       "    0.02941645383834839,\n",
       "    0.030484236776828766,\n",
       "    0.03769058063626289,\n",
       "    0.03509870879352093,\n",
       "    0.02960713244974613,\n",
       "    0.024707432836294174,\n",
       "    0.028605109453201293,\n",
       "    0.02477349191904068,\n",
       "    0.02435844987630844,\n",
       "    0.03568428047001362,\n",
       "    0.03038078360259533,\n",
       "    0.024673630855977534,\n",
       "    0.02294714320451021,\n",
       "    0.02240015994757414,\n",
       "    0.026925412565469743,\n",
       "    0.023258959874510765,\n",
       "    0.030570983327925204,\n",
       "    0.02191358953714371,\n",
       "    0.03253067508339882,\n",
       "    0.020236767642199994,\n",
       "    0.028971989825367927,\n",
       "    0.02652488201856613,\n",
       "    0.02142808437347412],\n",
       "   'val_loss': [0.02070511505007744,\n",
       "    0.009761308319866657,\n",
       "    0.007712225429713726,\n",
       "    0.011938348412513733,\n",
       "    0.008701246231794357,\n",
       "    0.007563871331512928,\n",
       "    0.006598574575036764,\n",
       "    0.005600316915661097,\n",
       "    0.004506865981966257,\n",
       "    0.004247534554451704,\n",
       "    0.004208649508655071,\n",
       "    0.004236605484038591,\n",
       "    0.006192823406308889,\n",
       "    0.007027068641036749,\n",
       "    0.006640160456299782,\n",
       "    0.006483091507107019,\n",
       "    0.00563153438270092,\n",
       "    0.005901264026761055,\n",
       "    0.006343197543174028,\n",
       "    0.007060423959046602,\n",
       "    0.007142567075788975,\n",
       "    0.007495209574699402,\n",
       "    0.0073035042732954025,\n",
       "    0.006587557960301638,\n",
       "    0.006301607470959425,\n",
       "    0.007849466055631638,\n",
       "    0.009083791635930538,\n",
       "    0.0101095549762249,\n",
       "    0.00920788198709488,\n",
       "    0.00849237572401762,\n",
       "    0.007706610951572657],\n",
       "   'train_mae': [0.29457409381866456,\n",
       "    0.22843575477600098,\n",
       "    0.22819347083568572,\n",
       "    0.21703635454177855,\n",
       "    0.20836269855499268,\n",
       "    0.192076613008976,\n",
       "    0.196844619512558,\n",
       "    0.19321051239967346,\n",
       "    0.1689379870891571,\n",
       "    0.17683686316013336,\n",
       "    0.19058477282524108,\n",
       "    0.19323773980140685,\n",
       "    0.1779786765575409,\n",
       "    0.16128090769052505,\n",
       "    0.1643500506877899,\n",
       "    0.15700165331363677,\n",
       "    0.155220228433609,\n",
       "    0.19096459746360778,\n",
       "    0.18213672637939454,\n",
       "    0.15375147461891175,\n",
       "    0.14036829322576522,\n",
       "    0.14553515017032623,\n",
       "    0.1618417978286743,\n",
       "    0.1503885954618454,\n",
       "    0.17557332068681716,\n",
       "    0.14758467972278594,\n",
       "    0.17347404956817628,\n",
       "    0.13960521221160888,\n",
       "    0.16794603765010835,\n",
       "    0.15990722477436065,\n",
       "    0.1459841638803482],\n",
       "   'val_mae': [0.17023949325084686,\n",
       "    0.12481971830129623,\n",
       "    0.09935018420219421,\n",
       "    0.12020227313041687,\n",
       "    0.10328323394060135,\n",
       "    0.09253934025764465,\n",
       "    0.0842590183019638,\n",
       "    0.07770327478647232,\n",
       "    0.06320023536682129,\n",
       "    0.0593978576362133,\n",
       "    0.055783096700906754,\n",
       "    0.057755470275878906,\n",
       "    0.08504373580217361,\n",
       "    0.09202205389738083,\n",
       "    0.08897943049669266,\n",
       "    0.08976702392101288,\n",
       "    0.07981012761592865,\n",
       "    0.07891759276390076,\n",
       "    0.07953036576509476,\n",
       "    0.08919014781713486,\n",
       "    0.09367232769727707,\n",
       "    0.09687765687704086,\n",
       "    0.09466961771249771,\n",
       "    0.08838149160146713,\n",
       "    0.08696042001247406,\n",
       "    0.10224323719739914,\n",
       "    0.11116289347410202,\n",
       "    0.11797480285167694,\n",
       "    0.11263938993215561,\n",
       "    0.10904314368963242,\n",
       "    0.1037384644150734],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005]},\n",
       "  'metrics': {'mse': 0.015413228422403336,\n",
       "   'rmse': 0.12415002385180333,\n",
       "   'mae': 0.10373847186565399,\n",
       "   'r2': 0.42251622676849365,\n",
       "   'mape': 11.34990119934082},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.05029337015002966,\n",
       "    0.041088311622540154,\n",
       "    0.03906923703228434,\n",
       "    0.03591785083214442,\n",
       "    0.033933792573710285,\n",
       "    0.030366020742803812,\n",
       "    0.029016330372542143,\n",
       "    0.02451170371690144,\n",
       "    0.027060828792552154,\n",
       "    0.025626929787298042,\n",
       "    0.041285169310867786,\n",
       "    0.022510228135312598,\n",
       "    0.027396071857462328,\n",
       "    0.022222593193873763,\n",
       "    0.023385499138385057,\n",
       "    0.02191212571536501,\n",
       "    0.019602412668367226,\n",
       "    0.03360413387417793,\n",
       "    0.026399787049740553,\n",
       "    0.028477434068918228,\n",
       "    0.026834289853771526,\n",
       "    0.020992691861465573],\n",
       "   'val_loss': [0.013552584685385227,\n",
       "    0.011409711092710495,\n",
       "    0.013458539731800556,\n",
       "    0.01340384129434824,\n",
       "    0.012934517115354538,\n",
       "    0.012364485301077366,\n",
       "    0.012228517793118954,\n",
       "    0.01289291586726904,\n",
       "    0.01321202702820301,\n",
       "    0.012869531288743019,\n",
       "    0.01301333587616682,\n",
       "    0.014065993018448353,\n",
       "    0.014817863702774048,\n",
       "    0.014570368453860283,\n",
       "    0.014639909379184246,\n",
       "    0.01451636478304863,\n",
       "    0.014059516601264477,\n",
       "    0.013957555405795574,\n",
       "    0.013506240211427212,\n",
       "    0.013134420849382877,\n",
       "    0.013115284964442253,\n",
       "    0.012857040390372276],\n",
       "   'train_mae': [0.22250283261140189,\n",
       "    0.19564673552910486,\n",
       "    0.20100344717502594,\n",
       "    0.19043264041344324,\n",
       "    0.18175635735193887,\n",
       "    0.16447948043545088,\n",
       "    0.16641776636242867,\n",
       "    0.14934757351875305,\n",
       "    0.1588182101647059,\n",
       "    0.14328554521004358,\n",
       "    0.18795057634512582,\n",
       "    0.14331458633144697,\n",
       "    0.15573247273763022,\n",
       "    0.1334308876345555,\n",
       "    0.14077004541953406,\n",
       "    0.13694950938224792,\n",
       "    0.13180489713946977,\n",
       "    0.16789804647366205,\n",
       "    0.15056627492109934,\n",
       "    0.16280976310372353,\n",
       "    0.15372870365778604,\n",
       "    0.1302614938467741],\n",
       "   'val_mae': [0.11038123071193695,\n",
       "    0.0955624058842659,\n",
       "    0.1040067970752716,\n",
       "    0.10255621373653412,\n",
       "    0.09377245604991913,\n",
       "    0.0898808091878891,\n",
       "    0.09166929870843887,\n",
       "    0.08584042638540268,\n",
       "    0.08368729799985886,\n",
       "    0.08532336354255676,\n",
       "    0.08590258657932281,\n",
       "    0.0866442322731018,\n",
       "    0.0910111665725708,\n",
       "    0.08949930220842361,\n",
       "    0.08847586810588837,\n",
       "    0.08705876022577286,\n",
       "    0.0838785395026207,\n",
       "    0.08337079733610153,\n",
       "    0.08245775103569031,\n",
       "    0.0840788334608078,\n",
       "    0.0858263298869133,\n",
       "    0.08462657779455185],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005]},\n",
       "  'metrics': {'mse': 0.025714082643389702,\n",
       "   'rmse': 0.16035611196143945,\n",
       "   'mae': 0.08462658524513245,\n",
       "   'r2': 0.35501039028167725,\n",
       "   'mape': 16.588106155395508},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.0416385257350547,\n",
       "    0.031271579010146,\n",
       "    0.028223866862910136,\n",
       "    0.028618344238826206,\n",
       "    0.027900444343686104,\n",
       "    0.02901338626231466,\n",
       "    0.025404494388827255,\n",
       "    0.05803338864019939,\n",
       "    0.02464078020836626,\n",
       "    0.022326839888202294,\n",
       "    0.02292730945295521,\n",
       "    0.02129382493772677,\n",
       "    0.020105548302776048,\n",
       "    0.021155616800699915,\n",
       "    0.018930740787514617,\n",
       "    0.02195428271910974,\n",
       "    0.019251408454562937,\n",
       "    0.02986728958785534,\n",
       "    0.022546017542481422,\n",
       "    0.02470284208123173,\n",
       "    0.022785498893686702,\n",
       "    0.020183016479547535,\n",
       "    0.05800637882202864,\n",
       "    0.025546359164374217,\n",
       "    0.023235842552302138,\n",
       "    0.021739221843225614,\n",
       "    0.020634602954877273,\n",
       "    0.019548344558903148,\n",
       "    0.018388241595987762,\n",
       "    0.03254211467823812,\n",
       "    0.028980708015816554,\n",
       "    0.019323434348085096,\n",
       "    0.01904015110007354,\n",
       "    0.025494664242225035,\n",
       "    0.02004704757460526,\n",
       "    0.01792595475646002,\n",
       "    0.016404835457381393,\n",
       "    0.0263753260618874,\n",
       "    0.01902322717277067,\n",
       "    0.03282347082027367,\n",
       "    0.019874051745448793,\n",
       "    0.016847179803465093,\n",
       "    0.01629907132259437,\n",
       "    0.017262703173660805,\n",
       "    0.017494678497314453,\n",
       "    0.01679938765508788,\n",
       "    0.017640360897140845,\n",
       "    0.024362641253641674,\n",
       "    0.017356418738407747,\n",
       "    0.01999004344855036],\n",
       "   'val_loss': [0.030682362616062164,\n",
       "    0.023721380159258842,\n",
       "    0.023867914453148842,\n",
       "    0.021626103669404984,\n",
       "    0.023782305419445038,\n",
       "    0.02344362623989582,\n",
       "    0.02246025949716568,\n",
       "    0.023568831384181976,\n",
       "    0.02185744419693947,\n",
       "    0.02120540477335453,\n",
       "    0.019946258515119553,\n",
       "    0.022653263062238693,\n",
       "    0.02089638262987137,\n",
       "    0.020114658400416374,\n",
       "    0.02116105705499649,\n",
       "    0.01947578601539135,\n",
       "    0.020252544432878494,\n",
       "    0.022568965330719948,\n",
       "    0.021878760308027267,\n",
       "    0.02271244302392006,\n",
       "    0.021152708679437637,\n",
       "    0.02060992829501629,\n",
       "    0.021025732159614563,\n",
       "    0.02154911682009697,\n",
       "    0.02232109010219574,\n",
       "    0.02213139273226261,\n",
       "    0.021597839891910553,\n",
       "    0.0208953358232975,\n",
       "    0.02037474885582924,\n",
       "    0.02008003741502762,\n",
       "    0.020255079492926598,\n",
       "    0.020396195352077484,\n",
       "    0.02010601945221424,\n",
       "    0.01897887885570526,\n",
       "    0.018850555643439293,\n",
       "    0.01898597553372383,\n",
       "    0.01903289556503296,\n",
       "    0.01928124949336052,\n",
       "    0.019762001931667328,\n",
       "    0.020011665299534798,\n",
       "    0.022035637870430946,\n",
       "    0.020871970802545547,\n",
       "    0.020346328616142273,\n",
       "    0.019968600943684578,\n",
       "    0.020218342542648315,\n",
       "    0.02032417804002762,\n",
       "    0.020020434632897377,\n",
       "    0.019030313938856125,\n",
       "    0.019118648022413254,\n",
       "    0.019180722534656525],\n",
       "   'train_mae': [0.21955126311097825,\n",
       "    0.17217360969100678,\n",
       "    0.16538700248513902,\n",
       "    0.17084319250924246,\n",
       "    0.15807094957147325,\n",
       "    0.16589793988636561,\n",
       "    0.1549763913665499,\n",
       "    0.2219350657292775,\n",
       "    0.1461691888315337,\n",
       "    0.13904978334903717,\n",
       "    0.13316353357263974,\n",
       "    0.13761029073170253,\n",
       "    0.1268690824508667,\n",
       "    0.1323569608586175,\n",
       "    0.12345790224415916,\n",
       "    0.13983843475580215,\n",
       "    0.1285587442772729,\n",
       "    0.16563529201916286,\n",
       "    0.15603433123656682,\n",
       "    0.15691843841757094,\n",
       "    0.1347235558288438,\n",
       "    0.13015681505203247,\n",
       "    0.21954813706023352,\n",
       "    0.15855562686920166,\n",
       "    0.1330935891185488,\n",
       "    0.13331400070871627,\n",
       "    0.13264982295887812,\n",
       "    0.1331767322761672,\n",
       "    0.1254322220172201,\n",
       "    0.1698785818048886,\n",
       "    0.16354120416300638,\n",
       "    0.12410654659782137,\n",
       "    0.12466759660414287,\n",
       "    0.14102491842848913,\n",
       "    0.13552577048540115,\n",
       "    0.11894257366657257,\n",
       "    0.10919799442802157,\n",
       "    0.1528807921069009,\n",
       "    0.12052579011235919,\n",
       "    0.17046871462038585,\n",
       "    0.13792387502534048,\n",
       "    0.12150972762278148,\n",
       "    0.11292711592146329,\n",
       "    0.1137382739356586,\n",
       "    0.1240904831460544,\n",
       "    0.11586657698665347,\n",
       "    0.11961821679558073,\n",
       "    0.14694135529654367,\n",
       "    0.11988537119967597,\n",
       "    0.12721607089042664],\n",
       "   'val_mae': [0.18179142475128174,\n",
       "    0.11842630803585052,\n",
       "    0.12391243129968643,\n",
       "    0.11805213242769241,\n",
       "    0.13443179428577423,\n",
       "    0.13068346679210663,\n",
       "    0.11811909824609756,\n",
       "    0.11899346858263016,\n",
       "    0.11892245709896088,\n",
       "    0.11489075422286987,\n",
       "    0.12251487374305725,\n",
       "    0.1278139054775238,\n",
       "    0.11407992243766785,\n",
       "    0.11214554309844971,\n",
       "    0.1269078552722931,\n",
       "    0.11812549829483032,\n",
       "    0.10991718620061874,\n",
       "    0.11998172104358673,\n",
       "    0.13485483825206757,\n",
       "    0.12170293927192688,\n",
       "    0.10937179625034332,\n",
       "    0.10832740366458893,\n",
       "    0.11463536322116852,\n",
       "    0.11649083346128464,\n",
       "    0.11558477580547333,\n",
       "    0.11628270894289017,\n",
       "    0.1229376494884491,\n",
       "    0.11724412441253662,\n",
       "    0.10870325565338135,\n",
       "    0.10499100387096405,\n",
       "    0.10537178069353104,\n",
       "    0.10827753692865372,\n",
       "    0.10920000076293945,\n",
       "    0.11042460054159164,\n",
       "    0.10993263870477676,\n",
       "    0.10907256603240967,\n",
       "    0.10814607888460159,\n",
       "    0.11304415017366409,\n",
       "    0.12374552339315414,\n",
       "    0.13326694071292877,\n",
       "    0.16539524495601654,\n",
       "    0.14543543756008148,\n",
       "    0.12716320157051086,\n",
       "    0.12578997015953064,\n",
       "    0.12148921936750412,\n",
       "    0.12382402271032333,\n",
       "    0.1249794289469719,\n",
       "    0.12695083022117615,\n",
       "    0.1202002763748169,\n",
       "    0.11803218722343445],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.00025,\n",
       "    0.00025,\n",
       "    0.00025,\n",
       "    0.00025]},\n",
       "  'metrics': {'mse': 0.038361452519893646,\n",
       "   'rmse': 0.19586079883400262,\n",
       "   'mae': 0.11803219467401505,\n",
       "   'r2': 0.4598422050476074,\n",
       "   'mape': 30.9086856842041},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.04270877795560019,\n",
       "    0.033517751576645036,\n",
       "    0.030387388808386668,\n",
       "    0.029526270925998688,\n",
       "    0.029576638447386876,\n",
       "    0.02764375614268439,\n",
       "    0.026318724267184734,\n",
       "    0.023912581216011728,\n",
       "    0.023630776841725622,\n",
       "    0.022707419469952583,\n",
       "    0.02221181908888476,\n",
       "    0.021841257411454405,\n",
       "    0.021786864580852643,\n",
       "    0.01996926690584847,\n",
       "    0.018526449001261165,\n",
       "    0.019413511401840618,\n",
       "    0.01842974405735731,\n",
       "    0.017872360135827745,\n",
       "    0.018997219258121083,\n",
       "    0.018349966699523584,\n",
       "    0.017671240360609124,\n",
       "    0.017497105124805654,\n",
       "    0.017171221652201245,\n",
       "    0.017074710556438992],\n",
       "   'val_loss': [0.025272391736507416,\n",
       "    0.023326326161623,\n",
       "    0.0221258457750082,\n",
       "    0.022083671763539314,\n",
       "    0.02354823425412178,\n",
       "    0.022803006693720818,\n",
       "    0.023132380098104477,\n",
       "    0.023561028763651848,\n",
       "    0.023963749408721924,\n",
       "    0.024232327938079834,\n",
       "    0.02560274302959442,\n",
       "    0.024046309292316437,\n",
       "    0.025879453867673874,\n",
       "    0.025286760181188583,\n",
       "    0.024617468938231468,\n",
       "    0.02534291334450245,\n",
       "    0.025916049256920815,\n",
       "    0.026517366990447044,\n",
       "    0.02502676285803318,\n",
       "    0.02764023467898369,\n",
       "    0.026258524507284164,\n",
       "    0.02597498893737793,\n",
       "    0.024975724518299103,\n",
       "    0.026260288432240486],\n",
       "   'train_mae': [0.22138425707817078,\n",
       "    0.18299985144819533,\n",
       "    0.17258555335657938,\n",
       "    0.17113059972013747,\n",
       "    0.15896344291312353,\n",
       "    0.16027937403747014,\n",
       "    0.15472248728786195,\n",
       "    0.14705027320555278,\n",
       "    0.14619063585996628,\n",
       "    0.14023954101971217,\n",
       "    0.13567869365215302,\n",
       "    0.1358415803739003,\n",
       "    0.13493014765637262,\n",
       "    0.12951617794377462,\n",
       "    0.12582824592079436,\n",
       "    0.12790508036102569,\n",
       "    0.1279794318335397,\n",
       "    0.11957718538386482,\n",
       "    0.13020654448441096,\n",
       "    0.12543174411569322,\n",
       "    0.1211141049861908,\n",
       "    0.11963629935468946,\n",
       "    0.12092808846916471,\n",
       "    0.12057467017854963],\n",
       "   'val_mae': [0.164655864238739,\n",
       "    0.1477353721857071,\n",
       "    0.1368298977613449,\n",
       "    0.13500085473060608,\n",
       "    0.13878507912158966,\n",
       "    0.1374613493680954,\n",
       "    0.13160866498947144,\n",
       "    0.12721611559391022,\n",
       "    0.12616847455501556,\n",
       "    0.12447106838226318,\n",
       "    0.11428958177566528,\n",
       "    0.11923002451658249,\n",
       "    0.11386420577764511,\n",
       "    0.11352625489234924,\n",
       "    0.11315121501684189,\n",
       "    0.11239007115364075,\n",
       "    0.11526911705732346,\n",
       "    0.11490115523338318,\n",
       "    0.11551795154809952,\n",
       "    0.11812757700681686,\n",
       "    0.11282467842102051,\n",
       "    0.11157319694757462,\n",
       "    0.11655442416667938,\n",
       "    0.11151337623596191],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005]},\n",
       "  'metrics': {'mse': 0.0525997094810009,\n",
       "   'rmse': 0.22934626546120365,\n",
       "   'mae': 0.11151337623596191,\n",
       "   'r2': 0.2132994532585144,\n",
       "   'mape': 32.239017486572266},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.03864515922032297,\n",
       "    0.03190914331935346,\n",
       "    0.02924246317707002,\n",
       "    0.027740336256101727,\n",
       "    0.026892150519415736,\n",
       "    0.024685578187927604,\n",
       "    0.023524619755335152,\n",
       "    0.022249935660511255,\n",
       "    0.023545901873148978,\n",
       "    0.024121661204844713,\n",
       "    0.021998095326125622,\n",
       "    0.021571992547251284,\n",
       "    0.021054717246443033,\n",
       "    0.019859326537698507,\n",
       "    0.019768660073168576,\n",
       "    0.01993094850331545,\n",
       "    0.019502533948980272,\n",
       "    0.01852256420534104,\n",
       "    0.018293629749678075,\n",
       "    0.016980398097075522,\n",
       "    0.018142284243367612,\n",
       "    0.01721034455113113,\n",
       "    0.01690431172028184,\n",
       "    0.017547446535900235,\n",
       "    0.015908570610918105,\n",
       "    0.016467008739709854,\n",
       "    0.017090806737542152,\n",
       "    0.017055147036444396,\n",
       "    0.014774955809116364,\n",
       "    0.014837190276011825,\n",
       "    0.014593475498259068,\n",
       "    0.01461887953337282,\n",
       "    0.014058477245271206,\n",
       "    0.014045506832189858,\n",
       "    0.014230378903448582],\n",
       "   'val_loss': [0.018001077696681023,\n",
       "    0.01846068911254406,\n",
       "    0.021621348336338997,\n",
       "    0.019866373389959335,\n",
       "    0.02259218879044056,\n",
       "    0.022278131917119026,\n",
       "    0.021745529025793076,\n",
       "    0.029840726405382156,\n",
       "    0.016679368913173676,\n",
       "    0.018277045339345932,\n",
       "    0.018530772998929024,\n",
       "    0.01670694537460804,\n",
       "    0.018276238813996315,\n",
       "    0.021246856078505516,\n",
       "    0.015107749029994011,\n",
       "    0.01586887799203396,\n",
       "    0.016336655244231224,\n",
       "    0.015841621905565262,\n",
       "    0.01718093827366829,\n",
       "    0.01591646485030651,\n",
       "    0.021013259887695312,\n",
       "    0.016220448538661003,\n",
       "    0.018919434398412704,\n",
       "    0.016662422567605972,\n",
       "    0.016658352687954903,\n",
       "    0.01768021285533905,\n",
       "    0.021257635205984116,\n",
       "    0.016869885846972466,\n",
       "    0.016574222594499588,\n",
       "    0.018144119530916214,\n",
       "    0.01753668300807476,\n",
       "    0.016856376081705093,\n",
       "    0.01825024001300335,\n",
       "    0.017990559339523315,\n",
       "    0.018094096332788467],\n",
       "   'train_mae': [0.20170041173696518,\n",
       "    0.1685820184648037,\n",
       "    0.1595391072332859,\n",
       "    0.16455244459211826,\n",
       "    0.15354913286864758,\n",
       "    0.14997893571853638,\n",
       "    0.14338912535458803,\n",
       "    0.1419717948883772,\n",
       "    0.14031732454895973,\n",
       "    0.13762136083096266,\n",
       "    0.1331595443189144,\n",
       "    0.13870742078870535,\n",
       "    0.12725705094635487,\n",
       "    0.1313912896439433,\n",
       "    0.12663518637418747,\n",
       "    0.12643131520599127,\n",
       "    0.12414853926748037,\n",
       "    0.12349934224039316,\n",
       "    0.12029544822871685,\n",
       "    0.1172936549410224,\n",
       "    0.12344790529459715,\n",
       "    0.1152519378811121,\n",
       "    0.11579952668398619,\n",
       "    0.12084723357111216,\n",
       "    0.11136860772967339,\n",
       "    0.11493076663464308,\n",
       "    0.1128716403618455,\n",
       "    0.11527664959430695,\n",
       "    0.10584517568349838,\n",
       "    0.10958588868379593,\n",
       "    0.10762736294418573,\n",
       "    0.10675875004380941,\n",
       "    0.10474570281803608,\n",
       "    0.10435789823532104,\n",
       "    0.10728456266224384],\n",
       "   'val_mae': [0.1475667655467987,\n",
       "    0.14705698192119598,\n",
       "    0.1765611171722412,\n",
       "    0.1633833646774292,\n",
       "    0.17867209017276764,\n",
       "    0.17522293329238892,\n",
       "    0.1545058786869049,\n",
       "    0.19994333386421204,\n",
       "    0.11645790189504623,\n",
       "    0.14088599383831024,\n",
       "    0.1357566863298416,\n",
       "    0.12806828320026398,\n",
       "    0.1407461315393448,\n",
       "    0.1489640176296234,\n",
       "    0.11251349002122879,\n",
       "    0.1248915046453476,\n",
       "    0.13118189573287964,\n",
       "    0.12896661460399628,\n",
       "    0.13040821254253387,\n",
       "    0.13290396332740784,\n",
       "    0.16145548224449158,\n",
       "    0.13561633229255676,\n",
       "    0.15652920305728912,\n",
       "    0.14140956103801727,\n",
       "    0.14168469607830048,\n",
       "    0.1532379686832428,\n",
       "    0.17420895397663116,\n",
       "    0.13812272250652313,\n",
       "    0.1374206840991974,\n",
       "    0.15605485439300537,\n",
       "    0.14951257407665253,\n",
       "    0.14248929917812347,\n",
       "    0.1529494673013687,\n",
       "    0.150239497423172,\n",
       "    0.1500348001718521],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005]},\n",
       "  'metrics': {'mse': 0.03618819639086723,\n",
       "   'rmse': 0.19023195417927882,\n",
       "   'mae': 0.1500348150730133,\n",
       "   'r2': 0.23902446031570435,\n",
       "   'mape': 20.815839767456055},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.037017624411318034,\n",
       "    0.030788312355677288,\n",
       "    0.027881159550613828,\n",
       "    0.027625987099276647,\n",
       "    0.025141369758380785,\n",
       "    0.024265615166061454,\n",
       "    0.022919190116226673,\n",
       "    0.020820628437731,\n",
       "    0.022551532834768295,\n",
       "    0.021541491150856018,\n",
       "    0.021206856498287782,\n",
       "    0.02014706304503812,\n",
       "    0.0201025502756238,\n",
       "    0.019505651874674693,\n",
       "    0.019738112576305866,\n",
       "    0.018798292614519596,\n",
       "    0.018926111567351554,\n",
       "    0.01784791849139664,\n",
       "    0.017441388736996386,\n",
       "    0.01782768385277854,\n",
       "    0.01769734313711524,\n",
       "    0.0196207861105601,\n",
       "    0.0178999530358447,\n",
       "    0.018561311066150665,\n",
       "    0.017678794244097337,\n",
       "    0.015457391324970458,\n",
       "    0.015363596534977356,\n",
       "    0.017230881481534906,\n",
       "    0.016245625706182584,\n",
       "    0.014600679071413146,\n",
       "    0.014466446100009812,\n",
       "    0.014975121969150173,\n",
       "    0.01443696798135837,\n",
       "    0.01380464517407947,\n",
       "    0.014148024428221915,\n",
       "    0.013773647343946828,\n",
       "    0.013304155733850267],\n",
       "   'val_loss': [0.04715300723910332,\n",
       "    0.043925460427999496,\n",
       "    0.04353079944849014,\n",
       "    0.04447903856635094,\n",
       "    0.04775123670697212,\n",
       "    0.04504573345184326,\n",
       "    0.042429082095623016,\n",
       "    0.03814583271741867,\n",
       "    0.040908332914114,\n",
       "    0.03915145620703697,\n",
       "    0.042661186307668686,\n",
       "    0.043073851615190506,\n",
       "    0.04426995664834976,\n",
       "    0.03902232274413109,\n",
       "    0.04503755643963814,\n",
       "    0.03955372795462608,\n",
       "    0.036966901272535324,\n",
       "    0.04053390026092529,\n",
       "    0.039524611085653305,\n",
       "    0.04178536683320999,\n",
       "    0.04020622745156288,\n",
       "    0.041645076125860214,\n",
       "    0.040303271263837814,\n",
       "    0.04596887156367302,\n",
       "    0.03929577395319939,\n",
       "    0.04606781527400017,\n",
       "    0.04056544601917267,\n",
       "    0.04404307156801224,\n",
       "    0.0412149503827095,\n",
       "    0.039760854095220566,\n",
       "    0.042945317924022675,\n",
       "    0.04321741685271263,\n",
       "    0.04247331991791725,\n",
       "    0.042093005031347275,\n",
       "    0.04421311616897583,\n",
       "    0.03863860294222832,\n",
       "    0.04282946512103081],\n",
       "   'train_mae': [0.19013947579595777,\n",
       "    0.1725134270058738,\n",
       "    0.15988157358434466,\n",
       "    0.16317755480607352,\n",
       "    0.14859098278813893,\n",
       "    0.14699059310886595,\n",
       "    0.14356483022371927,\n",
       "    0.1315646469593048,\n",
       "    0.13863368746307161,\n",
       "    0.12914128684335285,\n",
       "    0.13436777724160087,\n",
       "    0.1273262600104014,\n",
       "    0.12931341346767214,\n",
       "    0.1284935043917762,\n",
       "    0.12326805790265401,\n",
       "    0.12390188376108806,\n",
       "    0.12140930195649464,\n",
       "    0.12282889667484495,\n",
       "    0.11962070481644736,\n",
       "    0.11819910920328563,\n",
       "    0.11921014802323447,\n",
       "    0.12310575279924604,\n",
       "    0.12097570217318004,\n",
       "    0.12295303410953945,\n",
       "    0.1148424314128028,\n",
       "    0.11353285445107354,\n",
       "    0.10983134888940388,\n",
       "    0.11593878848685159,\n",
       "    0.11023962580495411,\n",
       "    0.11014167467753093,\n",
       "    0.10721659246418211,\n",
       "    0.10924031999376085,\n",
       "    0.10552758226792018,\n",
       "    0.10389715101983812,\n",
       "    0.11043038301997715,\n",
       "    0.1020264940129386,\n",
       "    0.10406270954344007],\n",
       "   'val_mae': [0.23102375864982605,\n",
       "    0.24911752343177795,\n",
       "    0.2494538575410843,\n",
       "    0.24191957712173462,\n",
       "    0.2629004716873169,\n",
       "    0.22488680481910706,\n",
       "    0.21332044899463654,\n",
       "    0.20379361510276794,\n",
       "    0.19965702295303345,\n",
       "    0.21910196542739868,\n",
       "    0.19593015313148499,\n",
       "    0.1986512988805771,\n",
       "    0.18894314765930176,\n",
       "    0.1912919133901596,\n",
       "    0.18485285341739655,\n",
       "    0.1895073503255844,\n",
       "    0.1922745406627655,\n",
       "    0.18524816632270813,\n",
       "    0.1918269693851471,\n",
       "    0.18380561470985413,\n",
       "    0.19339899718761444,\n",
       "    0.1827508509159088,\n",
       "    0.1927245557308197,\n",
       "    0.18233975768089294,\n",
       "    0.18749284744262695,\n",
       "    0.18257546424865723,\n",
       "    0.19786497950553894,\n",
       "    0.18057172000408173,\n",
       "    0.18562795221805573,\n",
       "    0.18889081478118896,\n",
       "    0.18602725863456726,\n",
       "    0.1865883320569992,\n",
       "    0.18463489413261414,\n",
       "    0.18610218167304993,\n",
       "    0.18414835631847382,\n",
       "    0.18866008520126343,\n",
       "    0.18544481694698334],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005]},\n",
       "  'metrics': {'mse': 0.08565893024206161,\n",
       "   'rmse': 0.2926754691498104,\n",
       "   'mae': 0.18544480204582214,\n",
       "   'r2': 0.2303316593170166,\n",
       "   'mape': 59.476558685302734},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.04155615530908108,\n",
       "    0.03354899920523167,\n",
       "    0.030496589094400405,\n",
       "    0.028986548073589802,\n",
       "    0.028177323937416076,\n",
       "    0.02706227693706751,\n",
       "    0.025302855856716634,\n",
       "    0.02354701440781355,\n",
       "    0.023568115569651125,\n",
       "    0.022019524592906237,\n",
       "    0.022414102032780648,\n",
       "    0.022405327949672937,\n",
       "    0.022997780237346888,\n",
       "    0.021088263764977456,\n",
       "    0.02146185329183936,\n",
       "    0.02073037959635258,\n",
       "    0.020242841634899377,\n",
       "    0.02070433311164379,\n",
       "    0.01933878008276224,\n",
       "    0.01892990432679653,\n",
       "    0.018183738552033903,\n",
       "    0.01806538589298725,\n",
       "    0.018353823199868204,\n",
       "    0.017459029331803322,\n",
       "    0.01693067271262407,\n",
       "    0.017179363779723643],\n",
       "   'val_loss': [0.07962222397327423,\n",
       "    0.06571713089942932,\n",
       "    0.05500999093055725,\n",
       "    0.049510303884744644,\n",
       "    0.04875360056757927,\n",
       "    0.04525306075811386,\n",
       "    0.04664410278201103,\n",
       "    0.0468178354203701,\n",
       "    0.05180124565958977,\n",
       "    0.049728307873010635,\n",
       "    0.06814102083444595,\n",
       "    0.04857785627245903,\n",
       "    0.04656204581260681,\n",
       "    0.049948107451200485,\n",
       "    0.051183395087718964,\n",
       "    0.04776058346033096,\n",
       "    0.04997219145298004,\n",
       "    0.0493290014564991,\n",
       "    0.0585993155837059,\n",
       "    0.0607304647564888,\n",
       "    0.0545656681060791,\n",
       "    0.06580250710248947,\n",
       "    0.05077257752418518,\n",
       "    0.0516333132982254,\n",
       "    0.050720710307359695,\n",
       "    0.05164497345685959],\n",
       "   'train_mae': [0.2118481755256653,\n",
       "    0.18216368854045867,\n",
       "    0.17620100677013398,\n",
       "    0.16698424220085145,\n",
       "    0.16173501014709474,\n",
       "    0.1581866979598999,\n",
       "    0.15324430167675018,\n",
       "    0.14484361410140992,\n",
       "    0.14894916042685508,\n",
       "    0.1397700384259224,\n",
       "    0.13975942954421045,\n",
       "    0.1441831573843956,\n",
       "    0.14189542979001998,\n",
       "    0.13723716512322426,\n",
       "    0.1316341146826744,\n",
       "    0.1350018747150898,\n",
       "    0.13202813118696213,\n",
       "    0.1311137095093727,\n",
       "    0.12907826527953148,\n",
       "    0.12553366646170616,\n",
       "    0.12429352402687073,\n",
       "    0.12196696251630783,\n",
       "    0.12467396706342697,\n",
       "    0.1213665895164013,\n",
       "    0.11902932301163674,\n",
       "    0.12093752548098564],\n",
       "   'val_mae': [0.3160422444343567,\n",
       "    0.2992556691169739,\n",
       "    0.28334179520606995,\n",
       "    0.2649707496166229,\n",
       "    0.26219531893730164,\n",
       "    0.25067755579948425,\n",
       "    0.24512283504009247,\n",
       "    0.2493991255760193,\n",
       "    0.25294753909111023,\n",
       "    0.2537142336368561,\n",
       "    0.28979673981666565,\n",
       "    0.23535262048244476,\n",
       "    0.23599472641944885,\n",
       "    0.24118991196155548,\n",
       "    0.24510686099529266,\n",
       "    0.233343243598938,\n",
       "    0.23351161181926727,\n",
       "    0.23397032916545868,\n",
       "    0.2448880672454834,\n",
       "    0.2493220418691635,\n",
       "    0.24739715456962585,\n",
       "    0.25953131914138794,\n",
       "    0.2364886999130249,\n",
       "    0.23938576877117157,\n",
       "    0.23774383962154388,\n",
       "    0.23531435430049896],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005]},\n",
       "  'metrics': {'mse': 0.10329541563987732,\n",
       "   'rmse': 0.3213960417302573,\n",
       "   'mae': 0.23531439900398254,\n",
       "   'r2': 0.27518224716186523,\n",
       "   'mape': 84.27233123779297},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.04146492684429342,\n",
       "    0.03275428983298215,\n",
       "    0.030655697157437153,\n",
       "    0.030257673595439304,\n",
       "    0.028707549737935715,\n",
       "    0.02713167294859886,\n",
       "    0.026336478577418762,\n",
       "    0.025838266550139946,\n",
       "    0.026131930168379437,\n",
       "    0.025034984743053264,\n",
       "    0.024420668625018814,\n",
       "    0.024940105324441738,\n",
       "    0.024038596840744667,\n",
       "    0.024057845669713886,\n",
       "    0.02317589885470542,\n",
       "    0.022967240349812942,\n",
       "    0.021614982831207188,\n",
       "    0.02173429939218543,\n",
       "    0.020406675186346878,\n",
       "    0.022229776802388104,\n",
       "    0.021913935938342052,\n",
       "    0.021494124000045387,\n",
       "    0.020082959058609875,\n",
       "    0.0200679422440854],\n",
       "   'val_loss': [0.03782632574439049,\n",
       "    0.03686075285077095,\n",
       "    0.036554694175720215,\n",
       "    0.03590830788016319,\n",
       "    0.03610960766673088,\n",
       "    0.0370447151362896,\n",
       "    0.038217317312955856,\n",
       "    0.038569126278162,\n",
       "    0.038760699331760406,\n",
       "    0.04204777255654335,\n",
       "    0.04025450348854065,\n",
       "    0.04393578693270683,\n",
       "    0.04599808529019356,\n",
       "    0.04207935184240341,\n",
       "    0.04336059093475342,\n",
       "    0.04296884313225746,\n",
       "    0.042791448533535004,\n",
       "    0.04373779147863388,\n",
       "    0.047083862125873566,\n",
       "    0.04410555958747864,\n",
       "    0.045852143317461014,\n",
       "    0.04455355182290077,\n",
       "    0.045622121542692184,\n",
       "    0.04773297533392906],\n",
       "   'train_mae': [0.21164571155201306,\n",
       "    0.1808897460048849,\n",
       "    0.17656592211940073,\n",
       "    0.17094281315803528,\n",
       "    0.1640912280841307,\n",
       "    0.15956373512744904,\n",
       "    0.15701325170018457,\n",
       "    0.1565227061510086,\n",
       "    0.15357061543247916,\n",
       "    0.15278923172842374,\n",
       "    0.15006228197704663,\n",
       "    0.14922902597622437,\n",
       "    0.14580298486081036,\n",
       "    0.15018589726903223,\n",
       "    0.14359856058250775,\n",
       "    0.14004930718378586,\n",
       "    0.1385308097709309,\n",
       "    0.13702636618505826,\n",
       "    0.1323515691540458,\n",
       "    0.1376891170035709,\n",
       "    0.13712101429700851,\n",
       "    0.13870307870886542,\n",
       "    0.13050511343912644,\n",
       "    0.13058268617499957],\n",
       "   'val_mae': [0.21789959073066711,\n",
       "    0.2279863953590393,\n",
       "    0.21280471980571747,\n",
       "    0.21110373735427856,\n",
       "    0.21061472594738007,\n",
       "    0.21163026988506317,\n",
       "    0.21282294392585754,\n",
       "    0.2262965887784958,\n",
       "    0.2118227183818817,\n",
       "    0.21384307742118835,\n",
       "    0.21889247000217438,\n",
       "    0.2151188850402832,\n",
       "    0.22836834192276,\n",
       "    0.21312180161476135,\n",
       "    0.2140464186668396,\n",
       "    0.21437028050422668,\n",
       "    0.21378426253795624,\n",
       "    0.21476487815380096,\n",
       "    0.22018852829933167,\n",
       "    0.215122789144516,\n",
       "    0.21675772964954376,\n",
       "    0.21593116223812103,\n",
       "    0.21940208971500397,\n",
       "    0.22338692843914032],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005]},\n",
       "  'metrics': {'mse': 0.09546594321727753,\n",
       "   'rmse': 0.30897563531333266,\n",
       "   'mae': 0.22338689863681793,\n",
       "   'r2': -0.047246336936950684,\n",
       "   'mape': 45.464603424072266},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.044480322704960905,\n",
       "    0.03467735322192311,\n",
       "    0.03304900873141984,\n",
       "    0.03110291777799527,\n",
       "    0.030274860560894012,\n",
       "    0.02862304278338949,\n",
       "    0.028525784766922396,\n",
       "    0.028809411606440943,\n",
       "    0.027181876978526514,\n",
       "    0.026226361592610676,\n",
       "    0.026474081988756854,\n",
       "    0.026134340713421505,\n",
       "    0.025682813487946987,\n",
       "    0.025957991213848192,\n",
       "    0.026515485253185034,\n",
       "    0.024840160738676786,\n",
       "    0.02441975474357605,\n",
       "    0.02405627832437555,\n",
       "    0.023325876332819462,\n",
       "    0.023051165820409853,\n",
       "    0.0231264173053205,\n",
       "    0.021446972930183012,\n",
       "    0.021914170278857153,\n",
       "    0.022065997822210193,\n",
       "    0.02177543138774733,\n",
       "    0.021660275602092344,\n",
       "    0.022018426408370335,\n",
       "    0.02096132926332454,\n",
       "    0.020973123687629897,\n",
       "    0.021854490352173645],\n",
       "   'val_loss': [0.04785672947764397,\n",
       "    0.04966948553919792,\n",
       "    0.046410635113716125,\n",
       "    0.04816132038831711,\n",
       "    0.04473220184445381,\n",
       "    0.044266264885663986,\n",
       "    0.03890608251094818,\n",
       "    0.039783280342817307,\n",
       "    0.04094170033931732,\n",
       "    0.036254554986953735,\n",
       "    0.04122412949800491,\n",
       "    0.03797825798392296,\n",
       "    0.04123218357563019,\n",
       "    0.039992883801460266,\n",
       "    0.039122577756643295,\n",
       "    0.03707204759120941,\n",
       "    0.03877246752381325,\n",
       "    0.03828226402401924,\n",
       "    0.041256316006183624,\n",
       "    0.03971252590417862,\n",
       "    0.040076617151498795,\n",
       "    0.03892119601368904,\n",
       "    0.041262686252593994,\n",
       "    0.04077811911702156,\n",
       "    0.0421161986887455,\n",
       "    0.04423459619283676,\n",
       "    0.04272780194878578,\n",
       "    0.0431441105902195,\n",
       "    0.04549024999141693,\n",
       "    0.04474114626646042],\n",
       "   'train_mae': [0.23794864863157272,\n",
       "    0.19772044693430266,\n",
       "    0.18508977194627127,\n",
       "    0.1843002326786518,\n",
       "    0.1777776467303435,\n",
       "    0.17098497847716013,\n",
       "    0.1700268772741159,\n",
       "    0.1673111729323864,\n",
       "    0.1662954663236936,\n",
       "    0.1579428923626741,\n",
       "    0.16434619079033533,\n",
       "    0.15788896443943182,\n",
       "    0.16042431133488813,\n",
       "    0.15433862805366516,\n",
       "    0.16046227887272835,\n",
       "    0.15325384897490343,\n",
       "    0.15244991021851698,\n",
       "    0.1527668945491314,\n",
       "    0.14665859378874302,\n",
       "    0.1486564427614212,\n",
       "    0.14699184149503708,\n",
       "    0.1410402531425158,\n",
       "    0.14159292976061502,\n",
       "    0.1458412930369377,\n",
       "    0.13839358215530714,\n",
       "    0.14343168213963509,\n",
       "    0.13955536670982838,\n",
       "    0.14015296225746474,\n",
       "    0.1354903895407915,\n",
       "    0.14279429242014885],\n",
       "   'val_mae': [0.2533683478832245,\n",
       "    0.24543842673301697,\n",
       "    0.24044516682624817,\n",
       "    0.23862126469612122,\n",
       "    0.22480224072933197,\n",
       "    0.22190627455711365,\n",
       "    0.21010954678058624,\n",
       "    0.22119930386543274,\n",
       "    0.20380374789237976,\n",
       "    0.217740997672081,\n",
       "    0.2024487853050232,\n",
       "    0.20905336737632751,\n",
       "    0.20062170922756195,\n",
       "    0.2266622632741928,\n",
       "    0.20619265735149384,\n",
       "    0.20383894443511963,\n",
       "    0.2093055099248886,\n",
       "    0.2042904794216156,\n",
       "    0.20706088840961456,\n",
       "    0.2084040343761444,\n",
       "    0.20915670692920685,\n",
       "    0.20605851709842682,\n",
       "    0.20634648203849792,\n",
       "    0.205079585313797,\n",
       "    0.2117224484682083,\n",
       "    0.20942501723766327,\n",
       "    0.2150012105703354,\n",
       "    0.21218295395374298,\n",
       "    0.21302661299705505,\n",
       "    0.21327155828475952],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005]},\n",
       "  'metrics': {'mse': 0.08948229998350143,\n",
       "   'rmse': 0.2991359222552541,\n",
       "   'mae': 0.21327155828475952,\n",
       "   'r2': 0.23650729656219482,\n",
       "   'mape': 70.45777130126953},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.038815613549489245,\n",
       "    0.03371834382414818,\n",
       "    0.03211524753043285,\n",
       "    0.03017937406324423,\n",
       "    0.028745831563495673,\n",
       "    0.028091654897882387,\n",
       "    0.02719588907292256,\n",
       "    0.027362430611482032,\n",
       "    0.02589168456884531,\n",
       "    0.025482564304883663,\n",
       "    0.025442225572008353,\n",
       "    0.02617691349811279,\n",
       "    0.02547196499430216,\n",
       "    0.024449721408578064,\n",
       "    0.024604020210412834,\n",
       "    0.023587218557412807,\n",
       "    0.023292569515223686,\n",
       "    0.022528298055896394,\n",
       "    0.022473586579928033,\n",
       "    0.022154165861698296,\n",
       "    0.022430888305489834,\n",
       "    0.021957575558469847,\n",
       "    0.0222871398123411,\n",
       "    0.02285166299686982,\n",
       "    0.02201610029889987],\n",
       "   'val_loss': [0.029383067041635513,\n",
       "    0.03007294051349163,\n",
       "    0.029467429965734482,\n",
       "    0.0284255500882864,\n",
       "    0.024372486397624016,\n",
       "    0.026961827650666237,\n",
       "    0.02651030942797661,\n",
       "    0.030473753809928894,\n",
       "    0.031679727137088776,\n",
       "    0.029128190129995346,\n",
       "    0.024434585124254227,\n",
       "    0.032717783004045486,\n",
       "    0.027538998052477837,\n",
       "    0.026199083775281906,\n",
       "    0.02878989651799202,\n",
       "    0.03141758218407631,\n",
       "    0.03127552941441536,\n",
       "    0.03250263258814812,\n",
       "    0.03087250515818596,\n",
       "    0.03383202105760574,\n",
       "    0.0297600906342268,\n",
       "    0.03294415399432182,\n",
       "    0.03308350592851639,\n",
       "    0.0290833730250597,\n",
       "    0.0330314077436924],\n",
       "   'train_mae': [0.20647211372852325,\n",
       "    0.1917298688338353,\n",
       "    0.18292835698677942,\n",
       "    0.17601239451995263,\n",
       "    0.17060916011150068,\n",
       "    0.16669828272782838,\n",
       "    0.16347863238591415,\n",
       "    0.16121239501696366,\n",
       "    0.16021610681827253,\n",
       "    0.1571191537838716,\n",
       "    0.1538772783600367,\n",
       "    0.15709798209942305,\n",
       "    0.1546990596331083,\n",
       "    0.15206494354284728,\n",
       "    0.15008467837021902,\n",
       "    0.14844929369596335,\n",
       "    0.14507204064956078,\n",
       "    0.14289765518445235,\n",
       "    0.13957958897719017,\n",
       "    0.14406218494360262,\n",
       "    0.13836648487127745,\n",
       "    0.142095464353378,\n",
       "    0.14028075967843717,\n",
       "    0.1450373255289518,\n",
       "    0.13796189771248743],\n",
       "   'val_mae': [0.19782137870788574,\n",
       "    0.19768093526363373,\n",
       "    0.19526930153369904,\n",
       "    0.18952952325344086,\n",
       "    0.16388671100139618,\n",
       "    0.1747927963733673,\n",
       "    0.16556516289710999,\n",
       "    0.18810081481933594,\n",
       "    0.1822492480278015,\n",
       "    0.18195290863513947,\n",
       "    0.1686786562204361,\n",
       "    0.1878785938024521,\n",
       "    0.17014987766742706,\n",
       "    0.1645175665616989,\n",
       "    0.1721746176481247,\n",
       "    0.17708022892475128,\n",
       "    0.1758221536874771,\n",
       "    0.17483820021152496,\n",
       "    0.17274217307567596,\n",
       "    0.17861244082450867,\n",
       "    0.16909433901309967,\n",
       "    0.17673853039741516,\n",
       "    0.17836745083332062,\n",
       "    0.16832299530506134,\n",
       "    0.17698541283607483],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005]},\n",
       "  'metrics': {'mse': 0.0660628229379654,\n",
       "   'rmse': 0.2570268914685103,\n",
       "   'mae': 0.17698542773723602,\n",
       "   'r2': 0.16876119375228882,\n",
       "   'mape': 28.949127197265625},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.039503744803369045,\n",
       "    0.03374870786709445,\n",
       "    0.031960752326995134,\n",
       "    0.030429257213004997,\n",
       "    0.03002745977469853,\n",
       "    0.0276863340820585,\n",
       "    0.02808315133942025,\n",
       "    0.026463831375752176,\n",
       "    0.027407640192125524,\n",
       "    0.026470213862402097,\n",
       "    0.026271614379116466,\n",
       "    0.024867061459060227,\n",
       "    0.025685330985912254,\n",
       "    0.02552148187533021,\n",
       "    0.024408334999212196,\n",
       "    0.024763571763677255,\n",
       "    0.024878608994185925,\n",
       "    0.026288368473095552,\n",
       "    0.024308409275753156,\n",
       "    0.024144628351288184,\n",
       "    0.02369466470554471,\n",
       "    0.0225821718174432,\n",
       "    0.022785371568586146,\n",
       "    0.02236285073948758,\n",
       "    0.021144221503553644,\n",
       "    0.021466040518134832,\n",
       "    0.02149165315287454,\n",
       "    0.0214488986613495,\n",
       "    0.022147162218711207,\n",
       "    0.021408582904509137,\n",
       "    0.02066137781366706,\n",
       "    0.01982352856014456,\n",
       "    0.019292361635182585,\n",
       "    0.019674486946314573,\n",
       "    0.020206459199211428,\n",
       "    0.019624588678457906,\n",
       "    0.018778613036764518,\n",
       "    0.02044719025226576,\n",
       "    0.0183742645728801,\n",
       "    0.019544960066143955,\n",
       "    0.018167320572371994,\n",
       "    0.01786440970110042,\n",
       "    0.017889031807758977,\n",
       "    0.016063378724668707,\n",
       "    0.016231626737862825,\n",
       "    0.015230695317898477,\n",
       "    0.015038469739790474,\n",
       "    0.015063550655863114,\n",
       "    0.01398594690752881,\n",
       "    0.01487090315536729],\n",
       "   'val_loss': [0.02406056597828865,\n",
       "    0.024473978206515312,\n",
       "    0.024837184697389603,\n",
       "    0.022671842947602272,\n",
       "    0.022187672555446625,\n",
       "    0.021719614043831825,\n",
       "    0.02152378298342228,\n",
       "    0.02036973647773266,\n",
       "    0.019948666915297508,\n",
       "    0.02098851650953293,\n",
       "    0.02068893238902092,\n",
       "    0.02082170732319355,\n",
       "    0.021945342421531677,\n",
       "    0.020754234865307808,\n",
       "    0.019132792949676514,\n",
       "    0.0201568640768528,\n",
       "    0.019425729289650917,\n",
       "    0.020183511078357697,\n",
       "    0.020000210031867027,\n",
       "    0.019335100427269936,\n",
       "    0.020239761099219322,\n",
       "    0.02038647048175335,\n",
       "    0.01995076797902584,\n",
       "    0.018889639526605606,\n",
       "    0.017720850184559822,\n",
       "    0.018743766471743584,\n",
       "    0.018528442829847336,\n",
       "    0.018407123163342476,\n",
       "    0.017942799255251884,\n",
       "    0.01801244542002678,\n",
       "    0.018516365438699722,\n",
       "    0.018125571310520172,\n",
       "    0.01801116392016411,\n",
       "    0.017665758728981018,\n",
       "    0.01798819564282894,\n",
       "    0.016319405287504196,\n",
       "    0.02065357193350792,\n",
       "    0.019930554553866386,\n",
       "    0.018740640953183174,\n",
       "    0.017503274604678154,\n",
       "    0.017773890867829323,\n",
       "    0.017909284681081772,\n",
       "    0.016228996217250824,\n",
       "    0.016803573817014694,\n",
       "    0.018583575263619423,\n",
       "    0.01577007584273815,\n",
       "    0.01717486046254635,\n",
       "    0.01789226569235325,\n",
       "    0.016346650198101997,\n",
       "    0.015742262825369835],\n",
       "   'train_mae': [0.2112354380743844,\n",
       "    0.19178092692579543,\n",
       "    0.18343211923326766,\n",
       "    0.17704839152949198,\n",
       "    0.1720177818621908,\n",
       "    0.16893962451389857,\n",
       "    0.16450338172061102,\n",
       "    0.1592887102493218,\n",
       "    0.16285797847168787,\n",
       "    0.1600813732615539,\n",
       "    0.15770582588655607,\n",
       "    0.15584092906543187,\n",
       "    0.15421192507658685,\n",
       "    0.15717179753950664,\n",
       "    0.15277968240635736,\n",
       "    0.1537722987788064,\n",
       "    0.1490823525403227,\n",
       "    0.1548362821340561,\n",
       "    0.15069855749607086,\n",
       "    0.14817051216959953,\n",
       "    0.1497224856700216,\n",
       "    0.14414483521665847,\n",
       "    0.1462211890944413,\n",
       "    0.14561612904071808,\n",
       "    0.13805702275463513,\n",
       "    0.13923597655126027,\n",
       "    0.1452283790068967,\n",
       "    0.13708519988826343,\n",
       "    0.13745551077382906,\n",
       "    0.1399319033537592,\n",
       "    0.13821589148470334,\n",
       "    0.13089961665017263,\n",
       "    0.13121129465954645,\n",
       "    0.13334220754248755,\n",
       "    0.1343547904065677,\n",
       "    0.13148843018071993,\n",
       "    0.1314805613032409,\n",
       "    0.1333801890058177,\n",
       "    0.1279454380273819,\n",
       "    0.1297468218420233,\n",
       "    0.13279316574335098,\n",
       "    0.12634448441011564,\n",
       "    0.12450161948800087,\n",
       "    0.12160007283091545,\n",
       "    0.11858870035835675,\n",
       "    0.11396106226103646,\n",
       "    0.11563931618418012,\n",
       "    0.11695766129664012,\n",
       "    0.11121659725904465,\n",
       "    0.11423472953694207],\n",
       "   'val_mae': [0.1491984874010086,\n",
       "    0.15096357464790344,\n",
       "    0.15019671618938446,\n",
       "    0.13609544932842255,\n",
       "    0.14172141253948212,\n",
       "    0.12323333323001862,\n",
       "    0.11720818281173706,\n",
       "    0.1189553514122963,\n",
       "    0.1225561648607254,\n",
       "    0.12275853008031845,\n",
       "    0.12033159285783768,\n",
       "    0.12326736003160477,\n",
       "    0.1316477358341217,\n",
       "    0.1356130838394165,\n",
       "    0.12396591901779175,\n",
       "    0.11927969008684158,\n",
       "    0.11776432394981384,\n",
       "    0.13276061415672302,\n",
       "    0.12055453658103943,\n",
       "    0.11272185295820236,\n",
       "    0.12919235229492188,\n",
       "    0.11098948121070862,\n",
       "    0.11837680637836456,\n",
       "    0.12618803977966309,\n",
       "    0.11306630074977875,\n",
       "    0.11803852766752243,\n",
       "    0.11167926341295242,\n",
       "    0.1140550896525383,\n",
       "    0.11799081414937973,\n",
       "    0.10766606777906418,\n",
       "    0.11424639075994492,\n",
       "    0.10834784060716629,\n",
       "    0.1059076115489006,\n",
       "    0.10578601062297821,\n",
       "    0.1182338148355484,\n",
       "    0.0994759127497673,\n",
       "    0.10804591327905655,\n",
       "    0.12243018299341202,\n",
       "    0.1188206821680069,\n",
       "    0.11577204614877701,\n",
       "    0.09993362426757812,\n",
       "    0.10466627776622772,\n",
       "    0.10800110548734665,\n",
       "    0.10243523865938187,\n",
       "    0.10487868636846542,\n",
       "    0.1116584911942482,\n",
       "    0.106486976146698,\n",
       "    0.10444452613592148,\n",
       "    0.10546988993883133,\n",
       "    0.10018327832221985],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001]},\n",
       "  'metrics': {'mse': 0.03148452565073967,\n",
       "   'rmse': 0.17743879409740043,\n",
       "   'mae': 0.10018330067396164,\n",
       "   'r2': 0.6261155605316162,\n",
       "   'mape': 27.18384552001953},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.03788718692958355,\n",
       "    0.03299435190856457,\n",
       "    0.031149000177780786,\n",
       "    0.029882816473642985,\n",
       "    0.028418725728988646,\n",
       "    0.028143957381447156,\n",
       "    0.027148237203558287,\n",
       "    0.026632648954788844,\n",
       "    0.025632678158581258,\n",
       "    0.025229238097866376,\n",
       "    0.02523862545688947,\n",
       "    0.025646159052848817,\n",
       "    0.02501855914791425,\n",
       "    0.025848521043856938,\n",
       "    0.02436160494883855,\n",
       "    0.024222138958672684,\n",
       "    0.02456200818220774,\n",
       "    0.02306837247063716,\n",
       "    0.02354901737223069,\n",
       "    0.02391948060443004,\n",
       "    0.02369348475088676,\n",
       "    0.022477812630434833,\n",
       "    0.021973878766099613,\n",
       "    0.021731149901946387,\n",
       "    0.022141701231400173,\n",
       "    0.021664408221840858,\n",
       "    0.02224042291442553,\n",
       "    0.021244091416398683,\n",
       "    0.020760743257900078,\n",
       "    0.020609868317842485,\n",
       "    0.019686045621832213,\n",
       "    0.019059362448751928,\n",
       "    0.018227982272704443,\n",
       "    0.01860309299081564,\n",
       "    0.018977081527312596,\n",
       "    0.01792305465787649,\n",
       "    0.0169318376109004,\n",
       "    0.017587703776856263,\n",
       "    0.01683832568426927],\n",
       "   'val_loss': [0.01861860230565071,\n",
       "    0.01868448592722416,\n",
       "    0.018513556569814682,\n",
       "    0.018519314005970955,\n",
       "    0.019442014396190643,\n",
       "    0.01812194660305977,\n",
       "    0.01863871142268181,\n",
       "    0.018580511212348938,\n",
       "    0.017930466681718826,\n",
       "    0.019629361107945442,\n",
       "    0.018338706344366074,\n",
       "    0.018332036212086678,\n",
       "    0.01914915256202221,\n",
       "    0.01821301504969597,\n",
       "    0.019738277420401573,\n",
       "    0.01956423744559288,\n",
       "    0.01942422240972519,\n",
       "    0.021730748936533928,\n",
       "    0.01784539595246315,\n",
       "    0.022387513890862465,\n",
       "    0.01945585012435913,\n",
       "    0.021993154659867287,\n",
       "    0.021227382123470306,\n",
       "    0.020444802939891815,\n",
       "    0.021949876099824905,\n",
       "    0.018702296540141106,\n",
       "    0.022628845646977425,\n",
       "    0.020257001742720604,\n",
       "    0.021070891991257668,\n",
       "    0.020981192588806152,\n",
       "    0.019909285008907318,\n",
       "    0.02147763967514038,\n",
       "    0.021388916298747063,\n",
       "    0.02062183804810047,\n",
       "    0.02037121169269085,\n",
       "    0.021468287333846092,\n",
       "    0.020575663074851036,\n",
       "    0.021309223026037216,\n",
       "    0.019928693771362305],\n",
       "   'train_mae': [0.1997892419497172,\n",
       "    0.18641638159751892,\n",
       "    0.17923707167307537,\n",
       "    0.17793877919514975,\n",
       "    0.17075136601924895,\n",
       "    0.16597330967585247,\n",
       "    0.16329961617787678,\n",
       "    0.1598917216062546,\n",
       "    0.15904712875684102,\n",
       "    0.1543558011452357,\n",
       "    0.15629223187764485,\n",
       "    0.15545303424199422,\n",
       "    0.1533179461956024,\n",
       "    0.15345780849456786,\n",
       "    0.15141624063253403,\n",
       "    0.15077007015546162,\n",
       "    0.1508707270026207,\n",
       "    0.14926083932320278,\n",
       "    0.14259397536516188,\n",
       "    0.15037719160318375,\n",
       "    0.1473596195379893,\n",
       "    0.14384837249914806,\n",
       "    0.14249761352936427,\n",
       "    0.14199222375949225,\n",
       "    0.1406714012225469,\n",
       "    0.13843214760224024,\n",
       "    0.14175057311852773,\n",
       "    0.14021472185850142,\n",
       "    0.1350011726220449,\n",
       "    0.13694615066051483,\n",
       "    0.13031978160142899,\n",
       "    0.13028287291526794,\n",
       "    0.1279768024881681,\n",
       "    0.12662241011857986,\n",
       "    0.12754714637994766,\n",
       "    0.12655096848805744,\n",
       "    0.12161074827114741,\n",
       "    0.12439180463552475,\n",
       "    0.12173836131890615],\n",
       "   'val_mae': [0.12973876297473907,\n",
       "    0.12092845886945724,\n",
       "    0.13405339419841766,\n",
       "    0.11884724348783493,\n",
       "    0.11800631135702133,\n",
       "    0.1279948204755783,\n",
       "    0.1337842047214508,\n",
       "    0.11612144112586975,\n",
       "    0.12242674082517624,\n",
       "    0.12547136843204498,\n",
       "    0.11598239094018936,\n",
       "    0.11960405856370926,\n",
       "    0.11168492585420609,\n",
       "    0.12171564996242523,\n",
       "    0.11392022669315338,\n",
       "    0.11267180740833282,\n",
       "    0.11505342274904251,\n",
       "    0.11444378644227982,\n",
       "    0.12216459214687347,\n",
       "    0.12213850766420364,\n",
       "    0.11405302584171295,\n",
       "    0.11762996762990952,\n",
       "    0.11727966368198395,\n",
       "    0.1134866252541542,\n",
       "    0.11164619028568268,\n",
       "    0.12313845008611679,\n",
       "    0.11583893746137619,\n",
       "    0.11408427357673645,\n",
       "    0.11286032944917679,\n",
       "    0.10998713225126266,\n",
       "    0.11333230137825012,\n",
       "    0.11205635964870453,\n",
       "    0.11016286164522171,\n",
       "    0.1102861762046814,\n",
       "    0.11169734597206116,\n",
       "    0.11071174591779709,\n",
       "    0.11261175572872162,\n",
       "    0.10975321382284164,\n",
       "    0.11425688117742538],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005]},\n",
       "  'metrics': {'mse': 0.03985738754272461,\n",
       "   'rmse': 0.19964315050290257,\n",
       "   'mae': 0.11425688117742538,\n",
       "   'r2': 0.2875586152076721,\n",
       "   'mape': 42.1710205078125},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.04024163063149899,\n",
       "    0.032973867491818964,\n",
       "    0.032324525993317366,\n",
       "    0.030760459834709764,\n",
       "    0.02868863579351455,\n",
       "    0.02712088485714048,\n",
       "    0.027041175053454936,\n",
       "    0.026980345835909247,\n",
       "    0.025160498218610883,\n",
       "    0.025231583509594202,\n",
       "    0.024938883318100125,\n",
       "    0.024894411675632,\n",
       "    0.02377297164639458,\n",
       "    0.024511840718332678,\n",
       "    0.02417356870137155,\n",
       "    0.02364523889264092,\n",
       "    0.02307193074375391,\n",
       "    0.021843655442353338,\n",
       "    0.022386154683772475,\n",
       "    0.021785212156828493,\n",
       "    0.022176679514814168,\n",
       "    0.022253400005865842,\n",
       "    0.0220376817160286,\n",
       "    0.020808027358725667,\n",
       "    0.020057785557582974,\n",
       "    0.02053162467200309,\n",
       "    0.019878890656400472,\n",
       "    0.020026524434797466,\n",
       "    0.019861822540406138,\n",
       "    0.0189423612318933,\n",
       "    0.019133584515657276,\n",
       "    0.01919749309308827,\n",
       "    0.01878793543437496,\n",
       "    0.01873336941935122,\n",
       "    0.01808528701076284,\n",
       "    0.01791389856953174,\n",
       "    0.018205751926871017,\n",
       "    0.018908373778685927,\n",
       "    0.01837400274234824,\n",
       "    0.018844717531464994,\n",
       "    0.018116524443030357,\n",
       "    0.017894736432936043,\n",
       "    0.016200619633309543,\n",
       "    0.018556057359091938,\n",
       "    0.01727775571634993,\n",
       "    0.016649319062707946,\n",
       "    0.015989926061592996,\n",
       "    0.016373621037928388,\n",
       "    0.016450404305942357,\n",
       "    0.01589913497446105],\n",
       "   'val_loss': [0.019012629985809326,\n",
       "    0.017314888536930084,\n",
       "    0.0169313196092844,\n",
       "    0.017368365079164505,\n",
       "    0.0171930193901062,\n",
       "    0.017129959538578987,\n",
       "    0.01777423545718193,\n",
       "    0.01686958596110344,\n",
       "    0.017324138432741165,\n",
       "    0.017043931409716606,\n",
       "    0.017388660460710526,\n",
       "    0.017598891630768776,\n",
       "    0.01810656674206257,\n",
       "    0.017697574570775032,\n",
       "    0.017318814992904663,\n",
       "    0.0172582995146513,\n",
       "    0.017337383702397346,\n",
       "    0.01696442998945713,\n",
       "    0.01737067848443985,\n",
       "    0.017060494050383568,\n",
       "    0.016612835228443146,\n",
       "    0.01794901117682457,\n",
       "    0.01716662384569645,\n",
       "    0.0166705921292305,\n",
       "    0.017060065641999245,\n",
       "    0.01708231307566166,\n",
       "    0.016425805166363716,\n",
       "    0.01726659946143627,\n",
       "    0.01685437001287937,\n",
       "    0.016880447044968605,\n",
       "    0.01664057746529579,\n",
       "    0.016698364168405533,\n",
       "    0.016382068395614624,\n",
       "    0.01679196208715439,\n",
       "    0.016612796112895012,\n",
       "    0.016345739364624023,\n",
       "    0.01679205894470215,\n",
       "    0.016436168923974037,\n",
       "    0.01619366742670536,\n",
       "    0.016319289803504944,\n",
       "    0.01687045767903328,\n",
       "    0.016350312158465385,\n",
       "    0.01635991968214512,\n",
       "    0.016824303194880486,\n",
       "    0.01677483133971691,\n",
       "    0.0169055238366127,\n",
       "    0.01643790490925312,\n",
       "    0.01634334959089756,\n",
       "    0.01629173569381237,\n",
       "    0.016697047278285027],\n",
       "   'train_mae': [0.20853293128311634,\n",
       "    0.18761167861521244,\n",
       "    0.18453054036945105,\n",
       "    0.17815840430557728,\n",
       "    0.17015677317976952,\n",
       "    0.1615707315504551,\n",
       "    0.16349210310727358,\n",
       "    0.15955023607239127,\n",
       "    0.158178448677063,\n",
       "    0.15485851047560573,\n",
       "    0.15419465815648437,\n",
       "    0.15150005836039782,\n",
       "    0.1507259220816195,\n",
       "    0.15088710561394691,\n",
       "    0.15356954373419285,\n",
       "    0.1453448487445712,\n",
       "    0.14789208211004734,\n",
       "    0.13960651168599725,\n",
       "    0.1429528258740902,\n",
       "    0.1409306824207306,\n",
       "    0.14112531766295433,\n",
       "    0.1432769037783146,\n",
       "    0.1399760008789599,\n",
       "    0.14060924435034394,\n",
       "    0.1337904343381524,\n",
       "    0.13277713302522898,\n",
       "    0.13263544254004955,\n",
       "    0.1364810336381197,\n",
       "    0.13075182447209954,\n",
       "    0.13117410149425268,\n",
       "    0.12763337278738618,\n",
       "    0.12933678505942225,\n",
       "    0.12897037528455257,\n",
       "    0.1274336939677596,\n",
       "    0.12655612733215094,\n",
       "    0.12616557767614722,\n",
       "    0.12628456950187683,\n",
       "    0.1317758997902274,\n",
       "    0.12452544458210468,\n",
       "    0.1314782747067511,\n",
       "    0.12652885727584362,\n",
       "    0.12575009418651462,\n",
       "    0.12041064631193876,\n",
       "    0.12789406767114997,\n",
       "    0.11940501909703016,\n",
       "    0.12564680352807045,\n",
       "    0.11732478719204664,\n",
       "    0.12034629937261343,\n",
       "    0.1194370468147099,\n",
       "    0.11981044709682465],\n",
       "   'val_mae': [0.12714633345603943,\n",
       "    0.11471013724803925,\n",
       "    0.1232534721493721,\n",
       "    0.10683506727218628,\n",
       "    0.10196022689342499,\n",
       "    0.1055346131324768,\n",
       "    0.09309990704059601,\n",
       "    0.10430657863616943,\n",
       "    0.10585448145866394,\n",
       "    0.10314542800188065,\n",
       "    0.1089620441198349,\n",
       "    0.1080000028014183,\n",
       "    0.0932106226682663,\n",
       "    0.12347107380628586,\n",
       "    0.10346249490976334,\n",
       "    0.09907255321741104,\n",
       "    0.10838103294372559,\n",
       "    0.10603609681129456,\n",
       "    0.10923796892166138,\n",
       "    0.10036148130893707,\n",
       "    0.1131972149014473,\n",
       "    0.0974011942744255,\n",
       "    0.12294115126132965,\n",
       "    0.10231412202119827,\n",
       "    0.10435616970062256,\n",
       "    0.10402867943048477,\n",
       "    0.1064407080411911,\n",
       "    0.09409406036138535,\n",
       "    0.11809416115283966,\n",
       "    0.09884648770093918,\n",
       "    0.10268139839172363,\n",
       "    0.10818038135766983,\n",
       "    0.10394534468650818,\n",
       "    0.11254843324422836,\n",
       "    0.09836804121732712,\n",
       "    0.09439540654420853,\n",
       "    0.10345130413770676,\n",
       "    0.11042813211679459,\n",
       "    0.10788386315107346,\n",
       "    0.1094052717089653,\n",
       "    0.10506922006607056,\n",
       "    0.104693204164505,\n",
       "    0.0945790633559227,\n",
       "    0.09255307912826538,\n",
       "    0.12194053828716278,\n",
       "    0.09340237081050873,\n",
       "    0.11098676174879074,\n",
       "    0.10187997668981552,\n",
       "    0.10634270310401917,\n",
       "    0.11186205595731735],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005]},\n",
       "  'metrics': {'mse': 0.03339409455657005,\n",
       "   'rmse': 0.18274051153635873,\n",
       "   'mae': 0.11186208575963974,\n",
       "   'r2': 0.43103325366973877,\n",
       "   'mape': 25.231674194335938},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.03850892953136388,\n",
       "    0.032292914083775354,\n",
       "    0.031062687911531505,\n",
       "    0.028840327964109534,\n",
       "    0.02795821735087563,\n",
       "    0.026503928663099512,\n",
       "    0.027213702745297375,\n",
       "    0.024732338593286628,\n",
       "    0.02534640098319334,\n",
       "    0.02450713509803309,\n",
       "    0.02523490446893608,\n",
       "    0.024288657285711345,\n",
       "    0.02462577447295189,\n",
       "    0.02343800658469691,\n",
       "    0.0242807682713165,\n",
       "    0.024296984733904108,\n",
       "    0.022808854358599466,\n",
       "    0.022486143745481968,\n",
       "    0.023464940214420065,\n",
       "    0.021473003003527138,\n",
       "    0.023603804409503937,\n",
       "    0.0230654738843441,\n",
       "    0.02209727542803568,\n",
       "    0.021581334335839048,\n",
       "    0.021862298852818852,\n",
       "    0.022064566612243652,\n",
       "    0.021122074412072405,\n",
       "    0.02007780098082388,\n",
       "    0.018787822576568407,\n",
       "    0.01857253142139491,\n",
       "    0.019414147690815085,\n",
       "    0.018252584042356294,\n",
       "    0.019154281738926384,\n",
       "    0.01822054194396033,\n",
       "    0.01849470314953257,\n",
       "    0.01859687444041757,\n",
       "    0.01925646190476768,\n",
       "    0.017624092781368422,\n",
       "    0.01675272809670252,\n",
       "    0.016937035026357454,\n",
       "    0.016475433894597432,\n",
       "    0.015595220631974585,\n",
       "    0.015957260833067054,\n",
       "    0.015994329717667663,\n",
       "    0.015595683718428892,\n",
       "    0.015189099454266183,\n",
       "    0.015184694901108742,\n",
       "    0.014959497815545867],\n",
       "   'val_loss': [0.015309859998524189,\n",
       "    0.015300149098038673,\n",
       "    0.014707333408296108,\n",
       "    0.014423842541873455,\n",
       "    0.01410029735416174,\n",
       "    0.01400389801710844,\n",
       "    0.013358627445995808,\n",
       "    0.01436805073171854,\n",
       "    0.013823643326759338,\n",
       "    0.014529290609061718,\n",
       "    0.013721217401325703,\n",
       "    0.013670017011463642,\n",
       "    0.013381758704781532,\n",
       "    0.01409704890102148,\n",
       "    0.013286558911204338,\n",
       "    0.01340631116181612,\n",
       "    0.013908777385950089,\n",
       "    0.013529011979699135,\n",
       "    0.013494264334440231,\n",
       "    0.014462732709944248,\n",
       "    0.01384477037936449,\n",
       "    0.01441134698688984,\n",
       "    0.014232255518436432,\n",
       "    0.01558700855821371,\n",
       "    0.014684304594993591,\n",
       "    0.013958202674984932,\n",
       "    0.013303340412676334,\n",
       "    0.013228921219706535,\n",
       "    0.013680300675332546,\n",
       "    0.015591767616569996,\n",
       "    0.014094047248363495,\n",
       "    0.014586308039724827,\n",
       "    0.014171013608574867,\n",
       "    0.014282913878560066,\n",
       "    0.01336422935128212,\n",
       "    0.013462900184094906,\n",
       "    0.016722768545150757,\n",
       "    0.01359737478196621,\n",
       "    0.01483394019305706,\n",
       "    0.014175244607031345,\n",
       "    0.014711004681885242,\n",
       "    0.014553248882293701,\n",
       "    0.01387814898043871,\n",
       "    0.01447967253625393,\n",
       "    0.014523166231811047,\n",
       "    0.016869474202394485,\n",
       "    0.015660271048545837,\n",
       "    0.016764290630817413],\n",
       "   'train_mae': [0.20518720412955566,\n",
       "    0.18113080193014705,\n",
       "    0.1796667777440127,\n",
       "    0.17026915094431708,\n",
       "    0.165625422316439,\n",
       "    0.15706720246988184,\n",
       "    0.1599687521948534,\n",
       "    0.153669453719083,\n",
       "    0.15336622955167994,\n",
       "    0.14972119559259975,\n",
       "    0.1526815102380865,\n",
       "    0.1477008589050349,\n",
       "    0.15178471759838216,\n",
       "    0.14689688384532928,\n",
       "    0.15200706133071115,\n",
       "    0.14598558229558609,\n",
       "    0.14510995412574096,\n",
       "    0.14210949443718968,\n",
       "    0.1470345412107075,\n",
       "    0.14052347137647517,\n",
       "    0.14564626313307705,\n",
       "    0.14384335893041947,\n",
       "    0.14377583870116403,\n",
       "    0.14009797923705158,\n",
       "    0.1406390154186417,\n",
       "    0.13875568044536254,\n",
       "    0.13596737253315308,\n",
       "    0.13391171877875047,\n",
       "    0.12942367090898402,\n",
       "    0.1266226623864735,\n",
       "    0.12974421942935271,\n",
       "    0.12657025894697974,\n",
       "    0.12895344416884816,\n",
       "    0.12462168975788004,\n",
       "    0.13064075699623892,\n",
       "    0.12424135690226275,\n",
       "    0.12834271963904886,\n",
       "    0.1254404454546816,\n",
       "    0.11925676158245872,\n",
       "    0.12198065396617441,\n",
       "    0.1179468605448218,\n",
       "    0.11716818940990112,\n",
       "    0.11768839376814225,\n",
       "    0.11839036774985931,\n",
       "    0.11648869821253945,\n",
       "    0.11462086263824911,\n",
       "    0.11470062285661697,\n",
       "    0.11363497770884458],\n",
       "   'val_mae': [0.10757329314947128,\n",
       "    0.11710423231124878,\n",
       "    0.09905741363763809,\n",
       "    0.09812451153993607,\n",
       "    0.09979067742824554,\n",
       "    0.09466511011123657,\n",
       "    0.09738018363714218,\n",
       "    0.1215992420911789,\n",
       "    0.09351713210344315,\n",
       "    0.12424362450838089,\n",
       "    0.10930193215608597,\n",
       "    0.10704641789197922,\n",
       "    0.10205398499965668,\n",
       "    0.117556631565094,\n",
       "    0.10381068289279938,\n",
       "    0.10580246150493622,\n",
       "    0.11548855900764465,\n",
       "    0.09721352905035019,\n",
       "    0.10713344067335129,\n",
       "    0.11958591639995575,\n",
       "    0.1070481538772583,\n",
       "    0.12074119597673416,\n",
       "    0.11267309635877609,\n",
       "    0.12808266282081604,\n",
       "    0.08407079428434372,\n",
       "    0.10018830001354218,\n",
       "    0.09844885766506195,\n",
       "    0.10108345001935959,\n",
       "    0.10680756717920303,\n",
       "    0.12559722363948822,\n",
       "    0.11122263222932816,\n",
       "    0.11824876070022583,\n",
       "    0.1023973822593689,\n",
       "    0.11442939192056656,\n",
       "    0.10239563137292862,\n",
       "    0.10713852196931839,\n",
       "    0.13616228103637695,\n",
       "    0.10414554178714752,\n",
       "    0.11877245455980301,\n",
       "    0.11002547293901443,\n",
       "    0.11470136791467667,\n",
       "    0.11396045982837677,\n",
       "    0.10828300565481186,\n",
       "    0.11374011635780334,\n",
       "    0.1124584972858429,\n",
       "    0.12528257071971893,\n",
       "    0.12167059630155563,\n",
       "    0.12650184333324432],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.00025,\n",
       "    0.00025,\n",
       "    0.00025,\n",
       "    0.00025,\n",
       "    0.00025,\n",
       "    0.00025,\n",
       "    0.00025,\n",
       "    0.00025,\n",
       "    0.00025]},\n",
       "  'metrics': {'mse': 0.03352858126163483,\n",
       "   'rmse': 0.18310811358766937,\n",
       "   'mae': 0.12650185823440552,\n",
       "   'r2': 0.22459298372268677,\n",
       "   'mape': 19.122329711914062},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.036900052800774574,\n",
       "    0.031245859546793833,\n",
       "    0.02845188147491879,\n",
       "    0.027156068839960627,\n",
       "    0.026132941866914432,\n",
       "    0.025326252397563722,\n",
       "    0.02517426231255134,\n",
       "    0.024153765394455858,\n",
       "    0.024261228119333584,\n",
       "    0.023515139395991962,\n",
       "    0.023919289362513356,\n",
       "    0.022943761872334614,\n",
       "    0.023407529418667156,\n",
       "    0.023186793964770105,\n",
       "    0.023894469978080854,\n",
       "    0.022999256228407223,\n",
       "    0.022863153368234634,\n",
       "    0.021068543661385775,\n",
       "    0.020821949415322807,\n",
       "    0.02124353963881731,\n",
       "    0.02116113616567519,\n",
       "    0.02108230395242572,\n",
       "    0.020559354219585657,\n",
       "    0.020711705450796418,\n",
       "    0.02015666937869456],\n",
       "   'val_loss': [0.028167415410280228,\n",
       "    0.026777109131217003,\n",
       "    0.025773223489522934,\n",
       "    0.02663450501859188,\n",
       "    0.025563117116689682,\n",
       "    0.026382530108094215,\n",
       "    0.025678694248199463,\n",
       "    0.025767970830202103,\n",
       "    0.025905171409249306,\n",
       "    0.02561355009675026,\n",
       "    0.0259705837816,\n",
       "    0.02592337317764759,\n",
       "    0.025863198563456535,\n",
       "    0.02611757256090641,\n",
       "    0.027285443618893623,\n",
       "    0.026656674221158028,\n",
       "    0.027263954281806946,\n",
       "    0.02715328335762024,\n",
       "    0.027577539905905724,\n",
       "    0.02906336449086666,\n",
       "    0.026251155883073807,\n",
       "    0.027917589992284775,\n",
       "    0.026460731402039528,\n",
       "    0.02831425704061985,\n",
       "    0.027219660580158234],\n",
       "   'train_mae': [0.19389801886346605,\n",
       "    0.17999260210328633,\n",
       "    0.16437583789229393,\n",
       "    0.16210661455988884,\n",
       "    0.15555519196722242,\n",
       "    0.15436146077182558,\n",
       "    0.1532873631351524,\n",
       "    0.14965620636940002,\n",
       "    0.14847905726896393,\n",
       "    0.1472375136282709,\n",
       "    0.1514473586446709,\n",
       "    0.14438564537300003,\n",
       "    0.14408384594652388,\n",
       "    0.14880778185195392,\n",
       "    0.1463409819536739,\n",
       "    0.14257963622609773,\n",
       "    0.1446535943282975,\n",
       "    0.13706717805729973,\n",
       "    0.1370795009036859,\n",
       "    0.13476047582096523,\n",
       "    0.1372517185906569,\n",
       "    0.13490301867326102,\n",
       "    0.1344105245338546,\n",
       "    0.13704683010776839,\n",
       "    0.13343576217691103],\n",
       "   'val_mae': [0.15954594314098358,\n",
       "    0.138843834400177,\n",
       "    0.13065943121910095,\n",
       "    0.13128149509429932,\n",
       "    0.1232934519648552,\n",
       "    0.12472041696310043,\n",
       "    0.11738041043281555,\n",
       "    0.13396872580051422,\n",
       "    0.13330046832561493,\n",
       "    0.11452797800302505,\n",
       "    0.1160263940691948,\n",
       "    0.137022003531456,\n",
       "    0.12745805084705353,\n",
       "    0.1250022053718567,\n",
       "    0.1452202945947647,\n",
       "    0.13405515253543854,\n",
       "    0.13658832013607025,\n",
       "    0.13253392279148102,\n",
       "    0.13450947403907776,\n",
       "    0.1494704633951187,\n",
       "    0.12262482196092606,\n",
       "    0.13810335099697113,\n",
       "    0.1235242486000061,\n",
       "    0.1441122144460678,\n",
       "    0.12796995043754578],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005]},\n",
       "  'metrics': {'mse': 0.05473346635699272,\n",
       "   'rmse': 0.2339518462354865,\n",
       "   'mae': 0.12796993553638458,\n",
       "   'r2': 0.3060649037361145,\n",
       "   'mape': 12.768417358398438},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.03690303950325439,\n",
       "    0.03078429851877062,\n",
       "    0.030230689891859105,\n",
       "    0.02748454950357738,\n",
       "    0.02692958114570693,\n",
       "    0.02575992067393504,\n",
       "    0.026045246049761772,\n",
       "    0.024774242437591677,\n",
       "    0.024602496506352173,\n",
       "    0.02674881971784328,\n",
       "    0.024407904497102687,\n",
       "    0.025021901795346486,\n",
       "    0.024282215653281463,\n",
       "    0.024419113993644714,\n",
       "    0.024553701918768257,\n",
       "    0.023972592757720696,\n",
       "    0.02322780086021674,\n",
       "    0.022045181377937918,\n",
       "    0.02254290069992605,\n",
       "    0.02259401947652039,\n",
       "    0.02141272698185946,\n",
       "    0.021695616557017752,\n",
       "    0.021087440318967168,\n",
       "    0.021495493864150422,\n",
       "    0.020961571827922996,\n",
       "    0.02142453953427704,\n",
       "    0.02045948847540115,\n",
       "    0.019924843879906756,\n",
       "    0.018549268011395868,\n",
       "    0.01798418965680819,\n",
       "    0.019329725029437167,\n",
       "    0.018477534679205793,\n",
       "    0.01774381331511234,\n",
       "    0.018139604135955636,\n",
       "    0.018408972769975662,\n",
       "    0.017678894788811083],\n",
       "   'val_loss': [0.02090335264801979,\n",
       "    0.020581508055329323,\n",
       "    0.020394911989569664,\n",
       "    0.020288845524191856,\n",
       "    0.01966756209731102,\n",
       "    0.018524324521422386,\n",
       "    0.018911398947238922,\n",
       "    0.018309030681848526,\n",
       "    0.017315920442342758,\n",
       "    0.017233634367585182,\n",
       "    0.018135633319616318,\n",
       "    0.017447439953684807,\n",
       "    0.017839262261986732,\n",
       "    0.016843561083078384,\n",
       "    0.017303617671132088,\n",
       "    0.016587253659963608,\n",
       "    0.01795395277440548,\n",
       "    0.01753394491970539,\n",
       "    0.017796408385038376,\n",
       "    0.017807714641094208,\n",
       "    0.0173280518501997,\n",
       "    0.01733436807990074,\n",
       "    0.018257347866892815,\n",
       "    0.01769173890352249,\n",
       "    0.016763733699917793,\n",
       "    0.017576053738594055,\n",
       "    0.017510348930954933,\n",
       "    0.016871219500899315,\n",
       "    0.01789892464876175,\n",
       "    0.01700100488960743,\n",
       "    0.018281085416674614,\n",
       "    0.017833257094025612,\n",
       "    0.017544539645314217,\n",
       "    0.017131341621279716,\n",
       "    0.017436949536204338,\n",
       "    0.017054300755262375],\n",
       "   'train_mae': [0.20206832728887858,\n",
       "    0.17757996524635114,\n",
       "    0.1733473633465014,\n",
       "    0.16408938130265788,\n",
       "    0.16068771795222633,\n",
       "    0.1558416862236826,\n",
       "    0.1569738980186613,\n",
       "    0.1512276851817181,\n",
       "    0.15193830156012586,\n",
       "    0.15624542220642693,\n",
       "    0.15009746347603045,\n",
       "    0.1494212460361029,\n",
       "    0.15240475378538432,\n",
       "    0.14731903491835846,\n",
       "    0.1492340996077186,\n",
       "    0.14672414997690603,\n",
       "    0.1449044884035462,\n",
       "    0.1402879420079683,\n",
       "    0.14272424029676536,\n",
       "    0.1389049993533837,\n",
       "    0.13892898159591774,\n",
       "    0.13872022181749344,\n",
       "    0.1354905794325628,\n",
       "    0.13688617160445765,\n",
       "    0.1354665211156795,\n",
       "    0.13780338630864494,\n",
       "    0.13308930083325035,\n",
       "    0.12924215315203919,\n",
       "    0.12650327070763237,\n",
       "    0.12457832341131411,\n",
       "    0.12938326401145836,\n",
       "    0.12395325852067847,\n",
       "    0.12427452677174618,\n",
       "    0.12467074002090253,\n",
       "    0.12292079313805229,\n",
       "    0.12626061392457863],\n",
       "   'val_mae': [0.15293586254119873,\n",
       "    0.14397519826889038,\n",
       "    0.1467447429895401,\n",
       "    0.13981220126152039,\n",
       "    0.1306248903274536,\n",
       "    0.14241333305835724,\n",
       "    0.1435803771018982,\n",
       "    0.141671285033226,\n",
       "    0.127040296792984,\n",
       "    0.122455894947052,\n",
       "    0.13331793248653412,\n",
       "    0.13043315708637238,\n",
       "    0.12495248764753342,\n",
       "    0.12384790927171707,\n",
       "    0.12615986168384552,\n",
       "    0.12827172875404358,\n",
       "    0.13481229543685913,\n",
       "    0.12953241169452667,\n",
       "    0.13017664849758148,\n",
       "    0.12682317197322845,\n",
       "    0.12348581105470657,\n",
       "    0.12549352645874023,\n",
       "    0.12116467207670212,\n",
       "    0.11663684248924255,\n",
       "    0.11439293622970581,\n",
       "    0.11178801953792572,\n",
       "    0.1209886223077774,\n",
       "    0.1265675127506256,\n",
       "    0.11728636175394058,\n",
       "    0.1256033480167389,\n",
       "    0.11477302014827728,\n",
       "    0.12460972368717194,\n",
       "    0.11674841493368149,\n",
       "    0.1130068376660347,\n",
       "    0.1333772838115692,\n",
       "    0.117491215467453],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005]},\n",
       "  'metrics': {'mse': 0.03410860151052475,\n",
       "   'rmse': 0.1846851415531979,\n",
       "   'mae': 0.1174912303686142,\n",
       "   'r2': 0.4808819890022278,\n",
       "   'mape': 22.039339065551758},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.03417258225381374,\n",
       "    0.0305876356549561,\n",
       "    0.02780792103148997,\n",
       "    0.027114065270870923,\n",
       "    0.026026808796450494,\n",
       "    0.025798117043450475,\n",
       "    0.02414234250318259,\n",
       "    0.025201758183538914,\n",
       "    0.024619772378355265,\n",
       "    0.023941616388037802,\n",
       "    0.02345044482499361,\n",
       "    0.02365434910170734,\n",
       "    0.02308728373609483,\n",
       "    0.023259647795930503,\n",
       "    0.022900807298719884,\n",
       "    0.02283620275557041,\n",
       "    0.021951156901195645,\n",
       "    0.022533703269436954,\n",
       "    0.022302375035360457,\n",
       "    0.020962695172056556,\n",
       "    0.02126497384160757,\n",
       "    0.020560036599636077,\n",
       "    0.02097446718253195,\n",
       "    0.02037578811869025,\n",
       "    0.02077121869660914,\n",
       "    0.019990091631188988,\n",
       "    0.019478546641767024,\n",
       "    0.02075270200148225,\n",
       "    0.019216729886829854,\n",
       "    0.01893639387562871,\n",
       "    0.018067705060821025,\n",
       "    0.016916284523904323,\n",
       "    0.016559935407713056,\n",
       "    0.015916943666525185,\n",
       "    0.016600326262414457,\n",
       "    0.015558683662675321,\n",
       "    0.016093410062603654,\n",
       "    0.01505223308922723,\n",
       "    0.015578779997304082],\n",
       "   'val_loss': [0.017899315804243088,\n",
       "    0.01783870719373226,\n",
       "    0.01876378431916237,\n",
       "    0.019835304468870163,\n",
       "    0.018181361258029938,\n",
       "    0.016246404498815536,\n",
       "    0.01625751703977585,\n",
       "    0.01780741475522518,\n",
       "    0.016435706987977028,\n",
       "    0.016064392402768135,\n",
       "    0.020921830087900162,\n",
       "    0.0169615987688303,\n",
       "    0.01983902044594288,\n",
       "    0.01632685400545597,\n",
       "    0.01620575226843357,\n",
       "    0.016397304832935333,\n",
       "    0.016383271664381027,\n",
       "    0.016015993431210518,\n",
       "    0.014846313744783401,\n",
       "    0.01776430942118168,\n",
       "    0.019986998289823532,\n",
       "    0.01622183248400688,\n",
       "    0.01876748539507389,\n",
       "    0.016346188262104988,\n",
       "    0.015679171308875084,\n",
       "    0.016353944316506386,\n",
       "    0.01942071132361889,\n",
       "    0.019058924168348312,\n",
       "    0.01686173491179943,\n",
       "    0.016638752073049545,\n",
       "    0.016869939863681793,\n",
       "    0.02085847035050392,\n",
       "    0.020428821444511414,\n",
       "    0.018833832815289497,\n",
       "    0.021683815866708755,\n",
       "    0.019512934610247612,\n",
       "    0.028465954586863518,\n",
       "    0.019917147234082222,\n",
       "    0.02497982606291771],\n",
       "   'train_mae': [0.19469161704182625,\n",
       "    0.17595874816179274,\n",
       "    0.16592033952474594,\n",
       "    0.16291562020778655,\n",
       "    0.15890435688197613,\n",
       "    0.15520423837006092,\n",
       "    0.14988591372966767,\n",
       "    0.15621798299252987,\n",
       "    0.14831120185554028,\n",
       "    0.14837260507047176,\n",
       "    0.14715350419282913,\n",
       "    0.1454698324203491,\n",
       "    0.14619973488152027,\n",
       "    0.1462399810552597,\n",
       "    0.14332125075161456,\n",
       "    0.14260600544512272,\n",
       "    0.1417919833213091,\n",
       "    0.13829055204987525,\n",
       "    0.14156604707241058,\n",
       "    0.13559570908546448,\n",
       "    0.13680677562952043,\n",
       "    0.1365281105041504,\n",
       "    0.13652655184268953,\n",
       "    0.13419512547552587,\n",
       "    0.1327854637056589,\n",
       "    0.1314047873020172,\n",
       "    0.13026544637978077,\n",
       "    0.13140739537775517,\n",
       "    0.13125392682850362,\n",
       "    0.1272333160042763,\n",
       "    0.12474799826741219,\n",
       "    0.11836050748825074,\n",
       "    0.12002897746860981,\n",
       "    0.11453701332211494,\n",
       "    0.11774827875196933,\n",
       "    0.11590737737715244,\n",
       "    0.11730979010462761,\n",
       "    0.11156389638781547,\n",
       "    0.11507299430668354],\n",
       "   'val_mae': [0.15360839664936066,\n",
       "    0.14990170300006866,\n",
       "    0.15809468924999237,\n",
       "    0.1542595773935318,\n",
       "    0.13820475339889526,\n",
       "    0.12498699128627777,\n",
       "    0.12543733417987823,\n",
       "    0.1287597417831421,\n",
       "    0.13081027567386627,\n",
       "    0.1283937245607376,\n",
       "    0.13873839378356934,\n",
       "    0.13360491394996643,\n",
       "    0.15796463191509247,\n",
       "    0.11703868955373764,\n",
       "    0.1147255003452301,\n",
       "    0.1203811764717102,\n",
       "    0.11353060603141785,\n",
       "    0.12329540401697159,\n",
       "    0.11685874313116074,\n",
       "    0.11850447952747345,\n",
       "    0.14948143064975739,\n",
       "    0.10088470578193665,\n",
       "    0.12797343730926514,\n",
       "    0.11291790008544922,\n",
       "    0.1192162036895752,\n",
       "    0.11267491430044174,\n",
       "    0.13023220002651215,\n",
       "    0.13658085465431213,\n",
       "    0.12090801447629929,\n",
       "    0.11468019336462021,\n",
       "    0.10963129252195358,\n",
       "    0.13626742362976074,\n",
       "    0.12904776632785797,\n",
       "    0.1239856630563736,\n",
       "    0.1325606405735016,\n",
       "    0.12179206311702728,\n",
       "    0.15286140143871307,\n",
       "    0.1273638755083084,\n",
       "    0.1407422423362732],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005]},\n",
       "  'metrics': {'mse': 0.04995965585112572,\n",
       "   'rmse': 0.22351656728557218,\n",
       "   'mae': 0.1407422423362732,\n",
       "   'r2': -0.03522384166717529,\n",
       "   'mape': 20.251352310180664},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.039007848039979025,\n",
       "    0.030282473014224143,\n",
       "    0.028622217964203583,\n",
       "    0.028620712459087372,\n",
       "    0.02632526308298111,\n",
       "    0.026164306815536248,\n",
       "    0.026083250175274554,\n",
       "    0.024499730650512946,\n",
       "    0.024062605441680977,\n",
       "    0.024729147065608276,\n",
       "    0.02596163284033537,\n",
       "    0.02342516396726881,\n",
       "    0.023924543965785278,\n",
       "    0.02371840010441485,\n",
       "    0.022831603042071776,\n",
       "    0.021939834845917567,\n",
       "    0.02258893701114825,\n",
       "    0.022063290389875572,\n",
       "    0.02193828583473251,\n",
       "    0.02110752339164416,\n",
       "    0.022315304282875287,\n",
       "    0.020689222429479872,\n",
       "    0.021198523186502,\n",
       "    0.020924715768723262,\n",
       "    0.02192479669160786,\n",
       "    0.020915625084723746,\n",
       "    0.01918744903412603,\n",
       "    0.01930153871043807,\n",
       "    0.018406303000769446,\n",
       "    0.018425454873414265,\n",
       "    0.018252630230216754,\n",
       "    0.019064214036223433,\n",
       "    0.01783795180242686,\n",
       "    0.01740694274416282,\n",
       "    0.017443957782927014],\n",
       "   'val_loss': [0.03641928359866142,\n",
       "    0.04026252403855324,\n",
       "    0.03482819348573685,\n",
       "    0.03501206636428833,\n",
       "    0.036436568945646286,\n",
       "    0.03705841675400734,\n",
       "    0.03464283421635628,\n",
       "    0.03433150053024292,\n",
       "    0.033482346683740616,\n",
       "    0.033601582050323486,\n",
       "    0.033992305397987366,\n",
       "    0.03671570122241974,\n",
       "    0.03434443101286888,\n",
       "    0.034066278487443924,\n",
       "    0.03345643728971481,\n",
       "    0.0349198579788208,\n",
       "    0.03408671170473099,\n",
       "    0.03569081053137779,\n",
       "    0.036011915653944016,\n",
       "    0.03510544076561928,\n",
       "    0.03358951583504677,\n",
       "    0.03455839678645134,\n",
       "    0.03795430436730385,\n",
       "    0.03458784893155098,\n",
       "    0.036468587815761566,\n",
       "    0.03911030292510986,\n",
       "    0.033486057072877884,\n",
       "    0.03499047830700874,\n",
       "    0.03769512102007866,\n",
       "    0.03908186033368111,\n",
       "    0.038839761167764664,\n",
       "    0.040662143379449844,\n",
       "    0.04256196320056915,\n",
       "    0.034019727259874344,\n",
       "    0.03976073116064072],\n",
       "   'train_mae': [0.21796602507432303,\n",
       "    0.1786539483638037,\n",
       "    0.17268782073543185,\n",
       "    0.1701751450697581,\n",
       "    0.16243240875857218,\n",
       "    0.15923437369721277,\n",
       "    0.1578725931190309,\n",
       "    0.1548639220141229,\n",
       "    0.15221866823378064,\n",
       "    0.1531503771742185,\n",
       "    0.15491584014324916,\n",
       "    0.149196856433437,\n",
       "    0.14572752125206448,\n",
       "    0.14893637952350436,\n",
       "    0.14352196809791384,\n",
       "    0.14125480822154454,\n",
       "    0.14396804180883227,\n",
       "    0.14174144502196992,\n",
       "    0.1412582074602445,\n",
       "    0.13827133036795117,\n",
       "    0.13980823790743238,\n",
       "    0.13526215723582677,\n",
       "    0.13668285806973776,\n",
       "    0.13802639430477506,\n",
       "    0.1386603716583479,\n",
       "    0.14001463531028657,\n",
       "    0.12991591081732795,\n",
       "    0.12937384240684055,\n",
       "    0.1265030623901458,\n",
       "    0.12382979903902326,\n",
       "    0.12502335686059224,\n",
       "    0.12767577455157325,\n",
       "    0.12426073707285382,\n",
       "    0.1233002884047372,\n",
       "    0.12222026643298921],\n",
       "   'val_mae': [0.21317552030086517,\n",
       "    0.23876336216926575,\n",
       "    0.19910570979118347,\n",
       "    0.18982282280921936,\n",
       "    0.19855642318725586,\n",
       "    0.1963004320859909,\n",
       "    0.19298791885375977,\n",
       "    0.17302638292312622,\n",
       "    0.16813603043556213,\n",
       "    0.1676608771085739,\n",
       "    0.16909083724021912,\n",
       "    0.20025426149368286,\n",
       "    0.1861131638288498,\n",
       "    0.17884881794452667,\n",
       "    0.1815822422504425,\n",
       "    0.18779323995113373,\n",
       "    0.17495602369308472,\n",
       "    0.19163501262664795,\n",
       "    0.17422795295715332,\n",
       "    0.18765504658222198,\n",
       "    0.17067201435565948,\n",
       "    0.19022195041179657,\n",
       "    0.204642653465271,\n",
       "    0.18892011046409607,\n",
       "    0.18357476592063904,\n",
       "    0.2135569155216217,\n",
       "    0.17478826642036438,\n",
       "    0.17268352210521698,\n",
       "    0.17264649271965027,\n",
       "    0.20326602458953857,\n",
       "    0.19354389607906342,\n",
       "    0.20552514493465424,\n",
       "    0.219121515750885,\n",
       "    0.1619309037923813,\n",
       "    0.18743208050727844],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005]},\n",
       "  'metrics': {'mse': 0.07952302694320679,\n",
       "   'rmse': 0.2819982747167202,\n",
       "   'mae': 0.18743208050727844,\n",
       "   'r2': 0.053123414516448975,\n",
       "   'mape': 45.07408905029297},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.03345635330135172,\n",
       "    0.030354529195888477,\n",
       "    0.027714960658076136,\n",
       "    0.027618524110452694,\n",
       "    0.027470639906823635,\n",
       "    0.028181255837394434,\n",
       "    0.02595345349982381,\n",
       "    0.025365387089550495,\n",
       "    0.024798473978245802,\n",
       "    0.0252335240048441,\n",
       "    0.02429020819677548,\n",
       "    0.024305803743614393,\n",
       "    0.023387830662117762,\n",
       "    0.023653005122799765,\n",
       "    0.022991589143533598,\n",
       "    0.023935463491149923,\n",
       "    0.022538111883808266,\n",
       "    0.022098294205286285,\n",
       "    0.0215382869778709,\n",
       "    0.02199986500834877,\n",
       "    0.021799348421733488,\n",
       "    0.02137603001161055,\n",
       "    0.021077275995842436,\n",
       "    0.02138286041603847,\n",
       "    0.02120153686370362],\n",
       "   'val_loss': [0.043615661561489105,\n",
       "    0.043632131069898605,\n",
       "    0.044433485716581345,\n",
       "    0.04355750232934952,\n",
       "    0.04083314165472984,\n",
       "    0.043873656541109085,\n",
       "    0.05564671382308006,\n",
       "    0.04932401701807976,\n",
       "    0.045271072536706924,\n",
       "    0.047678541392087936,\n",
       "    0.05140567570924759,\n",
       "    0.04340077564120293,\n",
       "    0.0435783714056015,\n",
       "    0.04380878061056137,\n",
       "    0.04845884442329407,\n",
       "    0.04662361368536949,\n",
       "    0.045173484832048416,\n",
       "    0.04770473763346672,\n",
       "    0.045196615159511566,\n",
       "    0.044189728796482086,\n",
       "    0.04684332758188248,\n",
       "    0.043716344982385635,\n",
       "    0.04644810035824776,\n",
       "    0.04286988824605942,\n",
       "    0.044946376234292984],\n",
       "   'train_mae': [0.18679988248781723,\n",
       "    0.17541795088486237,\n",
       "    0.16649970378388057,\n",
       "    0.16518968038938261,\n",
       "    0.16080579838969492,\n",
       "    0.16265358504923907,\n",
       "    0.15861224349249492,\n",
       "    0.15668589554049753,\n",
       "    0.15293984961780635,\n",
       "    0.15450851009650665,\n",
       "    0.15146109089255333,\n",
       "    0.14814671907912602,\n",
       "    0.1447224183516069,\n",
       "    0.1485066373239864,\n",
       "    0.14547025446187367,\n",
       "    0.1446549083021554,\n",
       "    0.14327609098770402,\n",
       "    0.14066216078671542,\n",
       "    0.1373837519098412,\n",
       "    0.13760206272656267,\n",
       "    0.14182731407609853,\n",
       "    0.13545763526450505,\n",
       "    0.13758790560744025,\n",
       "    0.13517511940815233,\n",
       "    0.13789902017875152],\n",
       "   'val_mae': [0.2350648194551468,\n",
       "    0.23556585609912872,\n",
       "    0.24113500118255615,\n",
       "    0.2355368733406067,\n",
       "    0.2315264344215393,\n",
       "    0.23488961160182953,\n",
       "    0.25695928931236267,\n",
       "    0.24822022020816803,\n",
       "    0.2434663623571396,\n",
       "    0.24401238560676575,\n",
       "    0.2498931884765625,\n",
       "    0.23360034823417664,\n",
       "    0.23135600984096527,\n",
       "    0.2342216521501541,\n",
       "    0.24031224846839905,\n",
       "    0.23980526626110077,\n",
       "    0.2343250811100006,\n",
       "    0.23925408720970154,\n",
       "    0.2348090410232544,\n",
       "    0.23101234436035156,\n",
       "    0.23759602010250092,\n",
       "    0.22987104952335358,\n",
       "    0.23784857988357544,\n",
       "    0.22809059917926788,\n",
       "    0.22939790785312653],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005]},\n",
       "  'metrics': {'mse': 0.08989275246858597,\n",
       "   'rmse': 0.29982120083240604,\n",
       "   'mae': 0.22939790785312653,\n",
       "   'r2': 0.20379048585891724,\n",
       "   'mape': 74.1172103881836},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.03791367627032425,\n",
       "    0.03230633278903754,\n",
       "    0.03039419877788295,\n",
       "    0.028754985445867413,\n",
       "    0.0286393826422484,\n",
       "    0.02862696009485618,\n",
       "    0.026921505918321403,\n",
       "    0.026183739061588825,\n",
       "    0.026177148375174274,\n",
       "    0.02806371053599793,\n",
       "    0.025856352046779964,\n",
       "    0.025549192143523174,\n",
       "    0.025939586815302788,\n",
       "    0.024229892529547215,\n",
       "    0.024473967354582703,\n",
       "    0.024199145886561146,\n",
       "    0.02414755721617004,\n",
       "    0.02590657803027526,\n",
       "    0.02393509888940531,\n",
       "    0.024273422303731026,\n",
       "    0.023727240405328896,\n",
       "    0.02191179735667032,\n",
       "    0.022645326290765534,\n",
       "    0.02187045857958172,\n",
       "    0.023471109731041866,\n",
       "    0.022090439117797043,\n",
       "    0.021516478094069855,\n",
       "    0.021635065908017365,\n",
       "    0.02121679950505495],\n",
       "   'val_loss': [0.07006961107254028,\n",
       "    0.0698176920413971,\n",
       "    0.06707488000392914,\n",
       "    0.05833349749445915,\n",
       "    0.06138690188527107,\n",
       "    0.06493372470140457,\n",
       "    0.059720396995544434,\n",
       "    0.05866072326898575,\n",
       "    0.057019591331481934,\n",
       "    0.05924215912818909,\n",
       "    0.06197964400053024,\n",
       "    0.05809445306658745,\n",
       "    0.06468673050403595,\n",
       "    0.05967060104012489,\n",
       "    0.059255070984363556,\n",
       "    0.06289038807153702,\n",
       "    0.06561689078807831,\n",
       "    0.07305068522691727,\n",
       "    0.06650897115468979,\n",
       "    0.06758152693510056,\n",
       "    0.06565636396408081,\n",
       "    0.06931345909833908,\n",
       "    0.06514403223991394,\n",
       "    0.06359792500734329,\n",
       "    0.06628867238759995,\n",
       "    0.07073959708213806,\n",
       "    0.07289663702249527,\n",
       "    0.06956008076667786,\n",
       "    0.0699826255440712],\n",
       "   'train_mae': [0.21245768860630368,\n",
       "    0.18804213275080142,\n",
       "    0.17992161279139313,\n",
       "    0.17341698641362396,\n",
       "    0.16494418421517248,\n",
       "    0.1691048206842464,\n",
       "    0.16265747080678525,\n",
       "    0.15927046407823978,\n",
       "    0.15776928481848343,\n",
       "    0.15973663103321326,\n",
       "    0.159178389803223,\n",
       "    0.15799925962220068,\n",
       "    0.1567713974610619,\n",
       "    0.151333377737066,\n",
       "    0.1511557205863621,\n",
       "    0.14787706117267194,\n",
       "    0.15124271259359692,\n",
       "    0.15575459891039392,\n",
       "    0.14927292582781418,\n",
       "    0.1510872089344522,\n",
       "    0.1475578374836756,\n",
       "    0.1413382521790007,\n",
       "    0.1433587349627329,\n",
       "    0.14130870414816815,\n",
       "    0.14617063947345899,\n",
       "    0.14068465971428415,\n",
       "    0.13927167621643646,\n",
       "    0.14087958471930545,\n",
       "    0.13832392057646875],\n",
       "   'val_mae': [0.2930898368358612,\n",
       "    0.3051632046699524,\n",
       "    0.2920079827308655,\n",
       "    0.28443124890327454,\n",
       "    0.2790672779083252,\n",
       "    0.28728851675987244,\n",
       "    0.28469836711883545,\n",
       "    0.2767031490802765,\n",
       "    0.2679789364337921,\n",
       "    0.2793353199958801,\n",
       "    0.280764639377594,\n",
       "    0.26951363682746887,\n",
       "    0.2911943793296814,\n",
       "    0.27776220440864563,\n",
       "    0.2716285288333893,\n",
       "    0.28061211109161377,\n",
       "    0.28577446937561035,\n",
       "    0.293591171503067,\n",
       "    0.2790987193584442,\n",
       "    0.28739747405052185,\n",
       "    0.27655741572380066,\n",
       "    0.2847159206867218,\n",
       "    0.27534621953964233,\n",
       "    0.2750456631183624,\n",
       "    0.27753910422325134,\n",
       "    0.27700015902519226,\n",
       "    0.2823910415172577,\n",
       "    0.27798375487327576,\n",
       "    0.2835594415664673],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005]},\n",
       "  'metrics': {'mse': 0.1400621086359024,\n",
       "   'rmse': 0.37424872563029843,\n",
       "   'mae': 0.2835594415664673,\n",
       "   'r2': 0.0917089581489563,\n",
       "   'mape': 77.8864517211914},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.035459964657607285,\n",
       "    0.03143429383635521,\n",
       "    0.029763080179691315,\n",
       "    0.028749639006412548,\n",
       "    0.027399305254220963,\n",
       "    0.02777455601355304,\n",
       "    0.02720444058270558,\n",
       "    0.027576304324295208,\n",
       "    0.026783162242044575,\n",
       "    0.026879241690039635,\n",
       "    0.0263522587230672,\n",
       "    0.02668030932545662,\n",
       "    0.026045554965410545,\n",
       "    0.025403985915624577,\n",
       "    0.025175086910957874,\n",
       "    0.02485339238267878,\n",
       "    0.024880295092968838,\n",
       "    0.02437962405383587,\n",
       "    0.02433848247417937,\n",
       "    0.024477811003832714,\n",
       "    0.024167315184098224,\n",
       "    0.023832111943351185,\n",
       "    0.023581943110279415],\n",
       "   'val_loss': [0.05309637263417244,\n",
       "    0.050087716430425644,\n",
       "    0.05001915246248245,\n",
       "    0.05144563317298889,\n",
       "    0.05180105194449425,\n",
       "    0.05064462125301361,\n",
       "    0.056580837815999985,\n",
       "    0.05153428390622139,\n",
       "    0.05218740180134773,\n",
       "    0.051399603486061096,\n",
       "    0.05348553508520126,\n",
       "    0.0556633397936821,\n",
       "    0.050928689539432526,\n",
       "    0.051724206656217575,\n",
       "    0.05190421640872955,\n",
       "    0.053778424859046936,\n",
       "    0.05459213629364967,\n",
       "    0.05421353504061699,\n",
       "    0.05183423310518265,\n",
       "    0.055714838206768036,\n",
       "    0.05561327561736107,\n",
       "    0.05444831773638725,\n",
       "    0.053922802209854126],\n",
       "   'train_mae': [0.1977872382039609,\n",
       "    0.17978552372559256,\n",
       "    0.17128865744756616,\n",
       "    0.16828796073146488,\n",
       "    0.1612234410384427,\n",
       "    0.16402669574903406,\n",
       "    0.15877858225418173,\n",
       "    0.1638672947883606,\n",
       "    0.15772517511378165,\n",
       "    0.16023273442102515,\n",
       "    0.15599829930326212,\n",
       "    0.1575095867333205,\n",
       "    0.15473400153543637,\n",
       "    0.1524421880426614,\n",
       "    0.15065328323322794,\n",
       "    0.15186755132416022,\n",
       "    0.15113169861876447,\n",
       "    0.1476950059118478,\n",
       "    0.14594722085672876,\n",
       "    0.14867728073959766,\n",
       "    0.15075474070466083,\n",
       "    0.14520806009354797,\n",
       "    0.1455746587851773],\n",
       "   'val_mae': [0.23447491228580475,\n",
       "    0.22435320913791656,\n",
       "    0.2268494963645935,\n",
       "    0.22907204926013947,\n",
       "    0.22283241152763367,\n",
       "    0.2171727865934372,\n",
       "    0.2509051263332367,\n",
       "    0.2237710803747177,\n",
       "    0.22499775886535645,\n",
       "    0.22520719468593597,\n",
       "    0.23255716264247894,\n",
       "    0.2460511028766632,\n",
       "    0.21567605435848236,\n",
       "    0.2228648066520691,\n",
       "    0.2122250348329544,\n",
       "    0.22013363242149353,\n",
       "    0.22151006758213043,\n",
       "    0.2271769642829895,\n",
       "    0.21296480298042297,\n",
       "    0.22645261883735657,\n",
       "    0.22269345819950104,\n",
       "    0.221234992146492,\n",
       "    0.2191706746816635],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005]},\n",
       "  'metrics': {'mse': 0.10822409391403198,\n",
       "   'rmse': 0.32897430585690424,\n",
       "   'mae': 0.2191706746816635,\n",
       "   'r2': 0.1041150689125061,\n",
       "   'mape': 19.701723098754883},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.039363169111311436,\n",
       "    0.032825886349504195,\n",
       "    0.031216413558771212,\n",
       "    0.0305664277014633,\n",
       "    0.03049215612312158,\n",
       "    0.02925003948621452,\n",
       "    0.02843654827059557,\n",
       "    0.027770940912887454,\n",
       "    0.02821290275702874,\n",
       "    0.02784357957231502,\n",
       "    0.027526229658784967,\n",
       "    0.026845655171200633,\n",
       "    0.027596814208664,\n",
       "    0.026342376174094777,\n",
       "    0.026372793208186824,\n",
       "    0.026004818306925397,\n",
       "    0.02584955709365507,\n",
       "    0.026125367886076372,\n",
       "    0.025164528129001457,\n",
       "    0.0245372461892354,\n",
       "    0.024357450117046636,\n",
       "    0.02396428305655718,\n",
       "    0.023685726647575695,\n",
       "    0.022835378418676555,\n",
       "    0.023673988607091207,\n",
       "    0.022716145069959264,\n",
       "    0.023601217040171225,\n",
       "    0.022291398762414854,\n",
       "    0.02322794155528148,\n",
       "    0.022118856199085712,\n",
       "    0.02146610408090055,\n",
       "    0.020909748699826498,\n",
       "    0.021145096863619983,\n",
       "    0.02047923960102101,\n",
       "    0.01992143294773996],\n",
       "   'val_loss': [0.053921233862638474,\n",
       "    0.0523546002805233,\n",
       "    0.048357319086790085,\n",
       "    0.05053962022066116,\n",
       "    0.040261976420879364,\n",
       "    0.04085015878081322,\n",
       "    0.037801217287778854,\n",
       "    0.03745284304022789,\n",
       "    0.040578365325927734,\n",
       "    0.03833340108394623,\n",
       "    0.043158456683158875,\n",
       "    0.03946152329444885,\n",
       "    0.04107778146862984,\n",
       "    0.03980819508433342,\n",
       "    0.03745217248797417,\n",
       "    0.04043237119913101,\n",
       "    0.041492193937301636,\n",
       "    0.03892533481121063,\n",
       "    0.04205872118473053,\n",
       "    0.04039476066827774,\n",
       "    0.04518493264913559,\n",
       "    0.042288411408662796,\n",
       "    0.041603781282901764,\n",
       "    0.04709558188915253,\n",
       "    0.04110270366072655,\n",
       "    0.042365990579128265,\n",
       "    0.046979211270809174,\n",
       "    0.04019645228981972,\n",
       "    0.041065800935029984,\n",
       "    0.050225768238306046,\n",
       "    0.04606092721223831,\n",
       "    0.04848114028573036,\n",
       "    0.04394996166229248,\n",
       "    0.04802523925900459,\n",
       "    0.051257625222206116],\n",
       "   'train_mae': [0.20996633420387903,\n",
       "    0.18560820693771043,\n",
       "    0.17711885211368403,\n",
       "    0.17319964555402598,\n",
       "    0.1757493708282709,\n",
       "    0.16851188552876314,\n",
       "    0.16670416947454214,\n",
       "    0.1632473006223639,\n",
       "    0.16496555848668018,\n",
       "    0.1635092655196786,\n",
       "    0.16202272412677607,\n",
       "    0.15545085103561482,\n",
       "    0.1609496157616377,\n",
       "    0.15819274634122849,\n",
       "    0.15706553868949413,\n",
       "    0.15512834520389637,\n",
       "    0.15144398187597594,\n",
       "    0.1594525290032228,\n",
       "    0.15046620927751064,\n",
       "    0.14802067012836537,\n",
       "    0.1499296153585116,\n",
       "    0.1468082694336772,\n",
       "    0.14574295406540236,\n",
       "    0.14295625562469164,\n",
       "    0.14468964375555515,\n",
       "    0.14347631949931383,\n",
       "    0.14314674834410349,\n",
       "    0.14130343279490867,\n",
       "    0.1417463095858693,\n",
       "    0.14228051900863647,\n",
       "    0.13607913938661417,\n",
       "    0.13544856570661068,\n",
       "    0.1344115516791741,\n",
       "    0.13450960100938877,\n",
       "    0.13211951156457266],\n",
       "   'val_mae': [0.2687419354915619,\n",
       "    0.24891796708106995,\n",
       "    0.25100111961364746,\n",
       "    0.24594828486442566,\n",
       "    0.22238077223300934,\n",
       "    0.2219705581665039,\n",
       "    0.21057827770709991,\n",
       "    0.20183081924915314,\n",
       "    0.22640536725521088,\n",
       "    0.20998549461364746,\n",
       "    0.22507397830486298,\n",
       "    0.21738116443157196,\n",
       "    0.21818162500858307,\n",
       "    0.20836672186851501,\n",
       "    0.2108166664838791,\n",
       "    0.20939604938030243,\n",
       "    0.21725013852119446,\n",
       "    0.2083498239517212,\n",
       "    0.22108612954616547,\n",
       "    0.2128218412399292,\n",
       "    0.22369638085365295,\n",
       "    0.21835476160049438,\n",
       "    0.21282422542572021,\n",
       "    0.22215914726257324,\n",
       "    0.21189093589782715,\n",
       "    0.21403996646404266,\n",
       "    0.22372467815876007,\n",
       "    0.20823490619659424,\n",
       "    0.2112463414669037,\n",
       "    0.22920508682727814,\n",
       "    0.22152896225452423,\n",
       "    0.2207460254430771,\n",
       "    0.2146225869655609,\n",
       "    0.21959847211837769,\n",
       "    0.2253907322883606],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.00025,\n",
       "    0.00025,\n",
       "    0.00025,\n",
       "    0.00025,\n",
       "    0.00025]},\n",
       "  'metrics': {'mse': 0.10251526534557343,\n",
       "   'rmse': 0.3201800514485145,\n",
       "   'mae': 0.2253907322883606,\n",
       "   'r2': 0.27848291397094727,\n",
       "   'mape': 138.4080352783203},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.03688508361577988,\n",
       "    0.033242621794342994,\n",
       "    0.03132289163768291,\n",
       "    0.03022624034434557,\n",
       "    0.029392594546079634,\n",
       "    0.028748088404536246,\n",
       "    0.028721955865621568,\n",
       "    0.027830734252929687,\n",
       "    0.02790990024805069,\n",
       "    0.027282213121652604,\n",
       "    0.02843987725675106,\n",
       "    0.027611985988914966,\n",
       "    0.02729731485247612,\n",
       "    0.02744858533143997,\n",
       "    0.026343723870813848,\n",
       "    0.025971322879195215,\n",
       "    0.026479184702038763,\n",
       "    0.026471633948385715,\n",
       "    0.02577460628002882,\n",
       "    0.025671148747205733,\n",
       "    0.026581073701381682,\n",
       "    0.024927011206746102,\n",
       "    0.02479558076709509,\n",
       "    0.024475802928209306,\n",
       "    0.02416322622448206,\n",
       "    0.024046627543866633,\n",
       "    0.024127707332372666,\n",
       "    0.023074961155653,\n",
       "    0.022707443237304687,\n",
       "    0.022865619435906412,\n",
       "    0.022564697675406933,\n",
       "    0.021608186177909373,\n",
       "    0.023284958936274053,\n",
       "    0.020716744773089886,\n",
       "    0.020680389627814294,\n",
       "    0.019599018953740596,\n",
       "    0.01935957297682762,\n",
       "    0.01896594962105155,\n",
       "    0.018884973376989366,\n",
       "    0.018248601406812667,\n",
       "    0.017143009305000304,\n",
       "    0.01711744464933872],\n",
       "   'val_loss': [0.03754710406064987,\n",
       "    0.036625538021326065,\n",
       "    0.03456244617700577,\n",
       "    0.03172201290726662,\n",
       "    0.031414639204740524,\n",
       "    0.030257057398557663,\n",
       "    0.029378363862633705,\n",
       "    0.028768956661224365,\n",
       "    0.029021305963397026,\n",
       "    0.03035329282283783,\n",
       "    0.029397496953606606,\n",
       "    0.028126655146479607,\n",
       "    0.030395876616239548,\n",
       "    0.029214223846793175,\n",
       "    0.029222900047898293,\n",
       "    0.0280842836946249,\n",
       "    0.028127353638410568,\n",
       "    0.030292941257357597,\n",
       "    0.027559679001569748,\n",
       "    0.028439410030841827,\n",
       "    0.028960909694433212,\n",
       "    0.02627243846654892,\n",
       "    0.02679031528532505,\n",
       "    0.026807093992829323,\n",
       "    0.02693812921643257,\n",
       "    0.02813040465116501,\n",
       "    0.027921924367547035,\n",
       "    0.028344247490167618,\n",
       "    0.028207408264279366,\n",
       "    0.028448665514588356,\n",
       "    0.029725288972258568,\n",
       "    0.02786034345626831,\n",
       "    0.031259145587682724,\n",
       "    0.030449699610471725,\n",
       "    0.032215870916843414,\n",
       "    0.03201534226536751,\n",
       "    0.03185736760497093,\n",
       "    0.0365276001393795,\n",
       "    0.03387421369552612,\n",
       "    0.03870227187871933,\n",
       "    0.037409719079732895,\n",
       "    0.03972093388438225],\n",
       "   'train_mae': [0.1986418682336807,\n",
       "    0.18539897739887237,\n",
       "    0.17883268535137176,\n",
       "    0.17390672683715822,\n",
       "    0.168857661485672,\n",
       "    0.16686775743961335,\n",
       "    0.16899899303913116,\n",
       "    0.16203825533390045,\n",
       "    0.16345082938671113,\n",
       "    0.16109181731939315,\n",
       "    0.16335080444812775,\n",
       "    0.160151786506176,\n",
       "    0.15879782021045685,\n",
       "    0.1610918763279915,\n",
       "    0.15598247855901717,\n",
       "    0.15467202365398408,\n",
       "    0.15710540890693664,\n",
       "    0.159447360932827,\n",
       "    0.1545900759100914,\n",
       "    0.15120764166116715,\n",
       "    0.15934311479330063,\n",
       "    0.15247347950935364,\n",
       "    0.1490141561627388,\n",
       "    0.14989524930715561,\n",
       "    0.14632147073745727,\n",
       "    0.1466990253329277,\n",
       "    0.144550222158432,\n",
       "    0.14446195095777511,\n",
       "    0.1419028154015541,\n",
       "    0.1462435582280159,\n",
       "    0.14066585808992385,\n",
       "    0.13933592349290846,\n",
       "    0.14310984462499618,\n",
       "    0.13535159856081008,\n",
       "    0.13411097705364228,\n",
       "    0.1320006999373436,\n",
       "    0.12994419664144516,\n",
       "    0.12745718121528626,\n",
       "    0.12941616773605347,\n",
       "    0.12648722141981125,\n",
       "    0.12144838809967042,\n",
       "    0.12202223062515259],\n",
       "   'val_mae': [0.17696991562843323,\n",
       "    0.16794238984584808,\n",
       "    0.17815949022769928,\n",
       "    0.15634268522262573,\n",
       "    0.15158526599407196,\n",
       "    0.1639062613248825,\n",
       "    0.1628102958202362,\n",
       "    0.1485883891582489,\n",
       "    0.15569184720516205,\n",
       "    0.1506783366203308,\n",
       "    0.15959107875823975,\n",
       "    0.15835697948932648,\n",
       "    0.15753406286239624,\n",
       "    0.15801873803138733,\n",
       "    0.16432824730873108,\n",
       "    0.17191244661808014,\n",
       "    0.16307592391967773,\n",
       "    0.1499129980802536,\n",
       "    0.1561819314956665,\n",
       "    0.17079328000545502,\n",
       "    0.14525757730007172,\n",
       "    0.14717498421669006,\n",
       "    0.1570591926574707,\n",
       "    0.15397414565086365,\n",
       "    0.15851134061813354,\n",
       "    0.16600100696086884,\n",
       "    0.16222232580184937,\n",
       "    0.15902197360992432,\n",
       "    0.16219496726989746,\n",
       "    0.1551024168729782,\n",
       "    0.1707320511341095,\n",
       "    0.16433140635490417,\n",
       "    0.1586541086435318,\n",
       "    0.16257254779338837,\n",
       "    0.16705352067947388,\n",
       "    0.1642436683177948,\n",
       "    0.16926738619804382,\n",
       "    0.1750025898218155,\n",
       "    0.15725864470005035,\n",
       "    0.17338827252388,\n",
       "    0.1830684095621109,\n",
       "    0.1731199324131012],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005]},\n",
       "  'metrics': {'mse': 0.07944187521934509,\n",
       "   'rmse': 0.28185435107399903,\n",
       "   'mae': 0.1731199324131012,\n",
       "   'r2': 0.26749753952026367,\n",
       "   'mape': 58.626792907714844},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.03862749742200741,\n",
       "    0.032487530870219834,\n",
       "    0.032413586257742,\n",
       "    0.03040480026258872,\n",
       "    0.029751032805786684,\n",
       "    0.02947699959174945,\n",
       "    0.029030624633798234,\n",
       "    0.028562369254919198,\n",
       "    0.028702227756954156,\n",
       "    0.02880109603015276,\n",
       "    0.0284591568275713,\n",
       "    0.02756237990867633,\n",
       "    0.02644221305560607,\n",
       "    0.027478258949346267,\n",
       "    0.027421675282172285,\n",
       "    0.027750944503797934,\n",
       "    0.027174454612227585,\n",
       "    0.026734535976384696,\n",
       "    0.026236968831374094,\n",
       "    0.02595617023941416,\n",
       "    0.025168556397637494,\n",
       "    0.025348977508166663,\n",
       "    0.02544841019866558,\n",
       "    0.02471789330817186,\n",
       "    0.024709766300824974,\n",
       "    0.024378615646408155,\n",
       "    0.02347136690066411,\n",
       "    0.02326958433080178,\n",
       "    0.02274198901767914,\n",
       "    0.023060928457058393,\n",
       "    0.02228996827482031,\n",
       "    0.021708814463076685],\n",
       "   'val_loss': [0.032930318266153336,\n",
       "    0.03293569013476372,\n",
       "    0.03222866728901863,\n",
       "    0.027179550379514694,\n",
       "    0.02756151556968689,\n",
       "    0.024285275489091873,\n",
       "    0.024058425799012184,\n",
       "    0.02736351639032364,\n",
       "    0.024776063859462738,\n",
       "    0.025131819769740105,\n",
       "    0.024595890194177628,\n",
       "    0.022863252088427544,\n",
       "    0.024084337055683136,\n",
       "    0.025441234931349754,\n",
       "    0.02859949693083763,\n",
       "    0.02363850735127926,\n",
       "    0.023121794685721397,\n",
       "    0.02418173849582672,\n",
       "    0.028957994654774666,\n",
       "    0.02790362574160099,\n",
       "    0.02486354112625122,\n",
       "    0.03118368424475193,\n",
       "    0.02607678808271885,\n",
       "    0.029431333765387535,\n",
       "    0.02892373874783516,\n",
       "    0.033034782856702805,\n",
       "    0.03163847327232361,\n",
       "    0.029313672333955765,\n",
       "    0.03304239362478256,\n",
       "    0.03090687468647957,\n",
       "    0.036405812948942184,\n",
       "    0.03165500983595848],\n",
       "   'train_mae': [0.20296949434738892,\n",
       "    0.18519237064398253,\n",
       "    0.18035466797076738,\n",
       "    0.1741403152163212,\n",
       "    0.17099392872590286,\n",
       "    0.16761967425162977,\n",
       "    0.1703212599341686,\n",
       "    0.16643053379196387,\n",
       "    0.16431069374084473,\n",
       "    0.1647683923634199,\n",
       "    0.16357683734251902,\n",
       "    0.1627330997815499,\n",
       "    0.15919119406204957,\n",
       "    0.15989175582161316,\n",
       "    0.159382025209757,\n",
       "    0.15986185549543455,\n",
       "    0.16103779811125535,\n",
       "    0.1588710812995067,\n",
       "    0.15529540954874113,\n",
       "    0.15449381963564798,\n",
       "    0.15274960117844436,\n",
       "    0.15197872943603075,\n",
       "    0.15290105801362258,\n",
       "    0.14963805904755226,\n",
       "    0.15071303483385307,\n",
       "    0.14666570579776397,\n",
       "    0.14674531295895576,\n",
       "    0.14385023741767958,\n",
       "    0.1430073558137967,\n",
       "    0.14208665289557898,\n",
       "    0.142094143308126,\n",
       "    0.13885987320771584],\n",
       "   'val_mae': [0.17938096821308136,\n",
       "    0.17611178755760193,\n",
       "    0.16087207198143005,\n",
       "    0.15784701704978943,\n",
       "    0.14846976101398468,\n",
       "    0.15149138867855072,\n",
       "    0.15092119574546814,\n",
       "    0.1439250409603119,\n",
       "    0.1422017216682434,\n",
       "    0.14655877649784088,\n",
       "    0.14515337347984314,\n",
       "    0.14703747630119324,\n",
       "    0.1450119912624359,\n",
       "    0.14673148095607758,\n",
       "    0.1478612720966339,\n",
       "    0.14389586448669434,\n",
       "    0.14355550706386566,\n",
       "    0.14366257190704346,\n",
       "    0.15115416049957275,\n",
       "    0.14391624927520752,\n",
       "    0.1403372585773468,\n",
       "    0.16030928492546082,\n",
       "    0.14380428194999695,\n",
       "    0.14896559715270996,\n",
       "    0.15036192536354065,\n",
       "    0.1604834794998169,\n",
       "    0.15171532332897186,\n",
       "    0.1531655192375183,\n",
       "    0.15852726995944977,\n",
       "    0.15444785356521606,\n",
       "    0.16557848453521729,\n",
       "    0.15218313038349152],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005]},\n",
       "  'metrics': {'mse': 0.06331002712249756,\n",
       "   'rmse': 0.2516148388360622,\n",
       "   'mae': 0.1521831452846527,\n",
       "   'r2': 0.23688292503356934,\n",
       "   'mape': 41.8788948059082},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.03795562063654264,\n",
       "    0.03245304183413585,\n",
       "    0.031550671698318586,\n",
       "    0.030170782020798436,\n",
       "    0.029805340493718784,\n",
       "    0.02832150379954665,\n",
       "    0.02837119417058097,\n",
       "    0.02793414490642371,\n",
       "    0.027552417444962042,\n",
       "    0.027798228655700332,\n",
       "    0.027976250793370936,\n",
       "    0.027146137836906645,\n",
       "    0.02671710022345737,\n",
       "    0.027304653043824213,\n",
       "    0.026570586211703443,\n",
       "    0.025709215344654188,\n",
       "    0.025737208969615125,\n",
       "    0.025506630746854678,\n",
       "    0.024815378986574984,\n",
       "    0.02482307167654788,\n",
       "    0.02506020747952991,\n",
       "    0.024387790407571528,\n",
       "    0.02385918702930212,\n",
       "    0.02451360018716918,\n",
       "    0.024068594806724124,\n",
       "    0.02385775365487293,\n",
       "    0.023600566925274,\n",
       "    0.02259376455374338,\n",
       "    0.02230848199515431,\n",
       "    0.021537161915114632,\n",
       "    0.021432566525483573,\n",
       "    0.02154190220904571,\n",
       "    0.021556675296138833,\n",
       "    0.02083354136320176,\n",
       "    0.02042652849383928,\n",
       "    0.020465789190321056,\n",
       "    0.020252116403921886,\n",
       "    0.020535737448544415,\n",
       "    0.020192414867105307,\n",
       "    0.020055703370383492,\n",
       "    0.01995870115718356,\n",
       "    0.019514426416545,\n",
       "    0.01959437621688401,\n",
       "    0.01903953527410825,\n",
       "    0.01831757404875976,\n",
       "    0.018067888363643928,\n",
       "    0.017954859399685153,\n",
       "    0.017916469731264643,\n",
       "    0.017875322934102128,\n",
       "    0.017995860220657453],\n",
       "   'val_loss': [0.008044889196753502,\n",
       "    0.008819467388093472,\n",
       "    0.007887742482125759,\n",
       "    0.007530142553150654,\n",
       "    0.007961592637002468,\n",
       "    0.007553101982921362,\n",
       "    0.008770552463829517,\n",
       "    0.008507251739501953,\n",
       "    0.008426537737250328,\n",
       "    0.007540615275502205,\n",
       "    0.008550389669835567,\n",
       "    0.007934908382594585,\n",
       "    0.008817791938781738,\n",
       "    0.00878144335001707,\n",
       "    0.008481455966830254,\n",
       "    0.007381268311291933,\n",
       "    0.008641555905342102,\n",
       "    0.008537826128304005,\n",
       "    0.008164020255208015,\n",
       "    0.009205815382301807,\n",
       "    0.009762936271727085,\n",
       "    0.009218594990670681,\n",
       "    0.007387062069028616,\n",
       "    0.007897828705608845,\n",
       "    0.008439871482551098,\n",
       "    0.008234694600105286,\n",
       "    0.008444574661552906,\n",
       "    0.0075432052835822105,\n",
       "    0.009423982352018356,\n",
       "    0.007918252609670162,\n",
       "    0.008712963201105595,\n",
       "    0.007313064765185118,\n",
       "    0.009717702865600586,\n",
       "    0.008122518658638,\n",
       "    0.007985640317201614,\n",
       "    0.008892884477972984,\n",
       "    0.007621077820658684,\n",
       "    0.009089451283216476,\n",
       "    0.007388785947114229,\n",
       "    0.0077912816777825356,\n",
       "    0.008008121512830257,\n",
       "    0.007727101445198059,\n",
       "    0.007344659883528948,\n",
       "    0.007308389525860548,\n",
       "    0.007888969965279102,\n",
       "    0.007242304272949696,\n",
       "    0.00739390030503273,\n",
       "    0.007097357884049416,\n",
       "    0.007157212123274803,\n",
       "    0.0069125620648264885],\n",
       "   'train_mae': [0.20334552449208718,\n",
       "    0.18578045070171356,\n",
       "    0.17885877909483733,\n",
       "    0.17516334961961816,\n",
       "    0.17109404338730705,\n",
       "    0.16742491473754248,\n",
       "    0.16466714000260388,\n",
       "    0.16623513234986198,\n",
       "    0.16105548457966912,\n",
       "    0.1614485322325318,\n",
       "    0.16219388086486747,\n",
       "    0.15885163070978942,\n",
       "    0.1614698905635763,\n",
       "    0.16015860521131092,\n",
       "    0.1581071471726453,\n",
       "    0.15469633180786063,\n",
       "    0.15220781967595773,\n",
       "    0.1515823464702677,\n",
       "    0.1528776944787414,\n",
       "    0.14998283899492687,\n",
       "    0.14914132544287928,\n",
       "    0.14884264629196237,\n",
       "    0.14847041556128748,\n",
       "    0.15012449743571105,\n",
       "    0.14643752961247056,\n",
       "    0.14599577834208807,\n",
       "    0.1453752393523852,\n",
       "    0.14379622042179108,\n",
       "    0.14012502630551657,\n",
       "    0.14019465198119482,\n",
       "    0.13833358718289268,\n",
       "    0.13730254603756797,\n",
       "    0.1390401298801104,\n",
       "    0.1353076649484811,\n",
       "    0.1354055114918285,\n",
       "    0.13415265359260417,\n",
       "    0.1329691125838845,\n",
       "    0.13332970330008753,\n",
       "    0.13396305839220682,\n",
       "    0.13352578574860538,\n",
       "    0.13269544668771602,\n",
       "    0.13144678125778833,\n",
       "    0.12922870202196968,\n",
       "    0.13182765466195565,\n",
       "    0.12765975655229003,\n",
       "    0.12712158842219246,\n",
       "    0.12429077316213537,\n",
       "    0.1252209508308658,\n",
       "    0.1238157409760687,\n",
       "    0.12574148895563902],\n",
       "   'val_mae': [0.09700217843055725,\n",
       "    0.11220118403434753,\n",
       "    0.09681713581085205,\n",
       "    0.08593250066041946,\n",
       "    0.09708725661039352,\n",
       "    0.08017586171627045,\n",
       "    0.10744882375001907,\n",
       "    0.09887751936912537,\n",
       "    0.09175834059715271,\n",
       "    0.08455181866884232,\n",
       "    0.08533991873264313,\n",
       "    0.09473288804292679,\n",
       "    0.10131128877401352,\n",
       "    0.0902414470911026,\n",
       "    0.09508726000785828,\n",
       "    0.07560067623853683,\n",
       "    0.0979449450969696,\n",
       "    0.09130514413118362,\n",
       "    0.09133164584636688,\n",
       "    0.10374065488576889,\n",
       "    0.10066147148609161,\n",
       "    0.0962744653224945,\n",
       "    0.08791863918304443,\n",
       "    0.07755590975284576,\n",
       "    0.08849945664405823,\n",
       "    0.09125634282827377,\n",
       "    0.08436042070388794,\n",
       "    0.08230248093605042,\n",
       "    0.094286248087883,\n",
       "    0.07932943105697632,\n",
       "    0.09193293005228043,\n",
       "    0.08086526393890381,\n",
       "    0.10323748737573624,\n",
       "    0.08704009652137756,\n",
       "    0.08015793561935425,\n",
       "    0.09013883024454117,\n",
       "    0.08438678830862045,\n",
       "    0.09490755945444107,\n",
       "    0.08039391785860062,\n",
       "    0.08682039380073547,\n",
       "    0.09333491325378418,\n",
       "    0.08424042165279388,\n",
       "    0.08313096314668655,\n",
       "    0.08457739651203156,\n",
       "    0.08569769561290741,\n",
       "    0.07428472489118576,\n",
       "    0.08108263462781906,\n",
       "    0.08046466112136841,\n",
       "    0.0817793533205986,\n",
       "    0.07570352405309677],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.00025,\n",
       "    0.00025,\n",
       "    0.00025,\n",
       "    0.00025,\n",
       "    0.00025,\n",
       "    0.00025,\n",
       "    0.00025,\n",
       "    0.00025,\n",
       "    0.00025,\n",
       "    0.00025,\n",
       "    0.00025,\n",
       "    0.00025,\n",
       "    0.00025,\n",
       "    0.00025,\n",
       "    0.00025,\n",
       "    0.00025,\n",
       "    0.000125,\n",
       "    0.000125,\n",
       "    0.000125,\n",
       "    0.000125,\n",
       "    0.000125,\n",
       "    0.000125,\n",
       "    0.000125]},\n",
       "  'metrics': {'mse': 0.0138251269236207,\n",
       "   'rmse': 0.11758029989594643,\n",
       "   'mae': 0.07570353895425797,\n",
       "   'r2': 0.663294792175293,\n",
       "   'mape': 10.910395622253418},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.034986546130052636,\n",
       "    0.03186688206291625,\n",
       "    0.029952087439596653,\n",
       "    0.029600969549002393,\n",
       "    0.029096757460917746,\n",
       "    0.02755409186439855,\n",
       "    0.027766721855316843,\n",
       "    0.02729891100898385,\n",
       "    0.02686611463182739,\n",
       "    0.02663215875093426,\n",
       "    0.026158209457727417,\n",
       "    0.02643365486125861,\n",
       "    0.026454020025474683,\n",
       "    0.026022800743313774,\n",
       "    0.02615611581131816,\n",
       "    0.02651328227615782,\n",
       "    0.025440830271691084,\n",
       "    0.025031532493552992,\n",
       "    0.024945064713912352,\n",
       "    0.02416515619760113,\n",
       "    0.025037690930600678,\n",
       "    0.025275525271094272,\n",
       "    0.024392534546287998,\n",
       "    0.024043753310771927,\n",
       "    0.02273520647681185,\n",
       "    0.02356523448335273,\n",
       "    0.02300677644754095,\n",
       "    0.023186860739120414,\n",
       "    0.02315059766572501,\n",
       "    0.023014764667355588,\n",
       "    0.02130113685104464,\n",
       "    0.02004730312286743,\n",
       "    0.019413009518757463,\n",
       "    0.018863654529143657,\n",
       "    0.018483037195567573,\n",
       "    0.017986637772992253,\n",
       "    0.01828732674143144,\n",
       "    0.017535582284576127,\n",
       "    0.016815325977014645,\n",
       "    0.01620588102377951,\n",
       "    0.01654500138413693,\n",
       "    0.01614715514837631,\n",
       "    0.015717560425400734,\n",
       "    0.014974507320273136,\n",
       "    0.014561950195846813,\n",
       "    0.013791648055692869,\n",
       "    0.013980034489317663,\n",
       "    0.013784886720324201,\n",
       "    0.013619225206119674,\n",
       "    0.013540992047637701],\n",
       "   'val_loss': [0.021252784878015518,\n",
       "    0.022659234702587128,\n",
       "    0.022058609873056412,\n",
       "    0.02236958220601082,\n",
       "    0.020544543862342834,\n",
       "    0.021142687648534775,\n",
       "    0.020572034642100334,\n",
       "    0.02038097009062767,\n",
       "    0.020484205335378647,\n",
       "    0.02050228789448738,\n",
       "    0.020083533599972725,\n",
       "    0.02061123587191105,\n",
       "    0.020431214943528175,\n",
       "    0.02020999602973461,\n",
       "    0.02074013277888298,\n",
       "    0.0206241887062788,\n",
       "    0.020697785541415215,\n",
       "    0.020159689709544182,\n",
       "    0.02048138901591301,\n",
       "    0.019995881244540215,\n",
       "    0.020784007385373116,\n",
       "    0.020370787009596825,\n",
       "    0.022362960502505302,\n",
       "    0.02007635496556759,\n",
       "    0.02016913704574108,\n",
       "    0.02046990767121315,\n",
       "    0.020906411111354828,\n",
       "    0.020606864243745804,\n",
       "    0.021256526932120323,\n",
       "    0.02004188671708107,\n",
       "    0.020822135731577873,\n",
       "    0.019978372380137444,\n",
       "    0.01998046040534973,\n",
       "    0.020536581054329872,\n",
       "    0.02044660784304142,\n",
       "    0.020392315462231636,\n",
       "    0.020510034635663033,\n",
       "    0.020047903060913086,\n",
       "    0.02003178372979164,\n",
       "    0.02001688815653324,\n",
       "    0.020063335075974464,\n",
       "    0.020222021266818047,\n",
       "    0.02028951421380043,\n",
       "    0.020495565608143806,\n",
       "    0.020215919241309166,\n",
       "    0.020124496892094612,\n",
       "    0.020220162346959114,\n",
       "    0.020119821652770042,\n",
       "    0.020338136702775955,\n",
       "    0.020185083150863647],\n",
       "   'train_mae': [0.19524268060922623,\n",
       "    0.1835965599332537,\n",
       "    0.17630855153713906,\n",
       "    0.1710011355046715,\n",
       "    0.17055052305970872,\n",
       "    0.16181933666978562,\n",
       "    0.1630976546023573,\n",
       "    0.15976617485284805,\n",
       "    0.1597945647580283,\n",
       "    0.1573942018938916,\n",
       "    0.15540364624134131,\n",
       "    0.15923557111195155,\n",
       "    0.15608329299305165,\n",
       "    0.15440381850515092,\n",
       "    0.15472790439214026,\n",
       "    0.15875925283346856,\n",
       "    0.15218610156859672,\n",
       "    0.15088984290403978,\n",
       "    0.15100646018981934,\n",
       "    0.14856842585972377,\n",
       "    0.15055546696696961,\n",
       "    0.1477956125246627,\n",
       "    0.1492842693946191,\n",
       "    0.14795080545757497,\n",
       "    0.14112907275557518,\n",
       "    0.1468686812690326,\n",
       "    0.142944887014372,\n",
       "    0.14429798243301256,\n",
       "    0.14228964197848523,\n",
       "    0.1454285377902644,\n",
       "    0.1378402691334486,\n",
       "    0.13228982181421348,\n",
       "    0.12984808241682394,\n",
       "    0.12935893583510602,\n",
       "    0.12652539328805038,\n",
       "    0.12509317243737833,\n",
       "    0.12597589833395823,\n",
       "    0.12337863285626684,\n",
       "    0.12185230771345752,\n",
       "    0.11948950987841402,\n",
       "    0.12041888146528176,\n",
       "    0.11852993895964963,\n",
       "    0.11704667152038642,\n",
       "    0.11649649616863046,\n",
       "    0.1105947702058724,\n",
       "    0.11090117959039551,\n",
       "    0.11118243874183723,\n",
       "    0.10856813272195202,\n",
       "    0.10866815197680678,\n",
       "    0.10874349384435586],\n",
       "   'val_mae': [0.1182350292801857,\n",
       "    0.1155073493719101,\n",
       "    0.12359635531902313,\n",
       "    0.13375090062618256,\n",
       "    0.09061132371425629,\n",
       "    0.10022694617509842,\n",
       "    0.09284612536430359,\n",
       "    0.09479699283838272,\n",
       "    0.08394190669059753,\n",
       "    0.07413812726736069,\n",
       "    0.08212370425462723,\n",
       "    0.07437315583229065,\n",
       "    0.0976320132613182,\n",
       "    0.07385330647230148,\n",
       "    0.1018470972776413,\n",
       "    0.08629541844129562,\n",
       "    0.09497470408678055,\n",
       "    0.07568106800317764,\n",
       "    0.06972856819629669,\n",
       "    0.08134143799543381,\n",
       "    0.08638651669025421,\n",
       "    0.08919110149145126,\n",
       "    0.1287546455860138,\n",
       "    0.0756705179810524,\n",
       "    0.08428878337144852,\n",
       "    0.09855078160762787,\n",
       "    0.10455189645290375,\n",
       "    0.09168145805597305,\n",
       "    0.11739648878574371,\n",
       "    0.07985079288482666,\n",
       "    0.09164654463529587,\n",
       "    0.07194919884204865,\n",
       "    0.08137187361717224,\n",
       "    0.06813083589076996,\n",
       "    0.07551734149456024,\n",
       "    0.07275412976741791,\n",
       "    0.09302043914794922,\n",
       "    0.08217480778694153,\n",
       "    0.07266397029161453,\n",
       "    0.07690811157226562,\n",
       "    0.07798000425100327,\n",
       "    0.08379893004894257,\n",
       "    0.08100868761539459,\n",
       "    0.07715517282485962,\n",
       "    0.08531346917152405,\n",
       "    0.07312466204166412,\n",
       "    0.07670442014932632,\n",
       "    0.08116123825311661,\n",
       "    0.08543659001588821,\n",
       "    0.08051954209804535],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.00025,\n",
       "    0.00025,\n",
       "    0.00025,\n",
       "    0.00025,\n",
       "    0.00025,\n",
       "    0.00025,\n",
       "    0.00025]},\n",
       "  'metrics': {'mse': 0.04064112901687622,\n",
       "   'rmse': 0.20159645090347256,\n",
       "   'mae': 0.08051953464746475,\n",
       "   'r2': 0.2924610376358032,\n",
       "   'mape': 4.737889766693115},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.037655064129623876,\n",
       "    0.032649315444046055,\n",
       "    0.03064448919532628,\n",
       "    0.029584229377837015,\n",
       "    0.029124318718396384,\n",
       "    0.028156832438604586,\n",
       "    0.027212041976123022,\n",
       "    0.027960894468786388,\n",
       "    0.026249491320601826,\n",
       "    0.02698752367547874,\n",
       "    0.026616405438760232,\n",
       "    0.027411008350037295,\n",
       "    0.026816989327299184,\n",
       "    0.025542126936388427,\n",
       "    0.026651194832962136,\n",
       "    0.025506542224822373,\n",
       "    0.025367348743923778,\n",
       "    0.024910257711749654,\n",
       "    0.025570247675581224,\n",
       "    0.025088220417242627,\n",
       "    0.02479252672015593,\n",
       "    0.024740485635040135,\n",
       "    0.02409836665952,\n",
       "    0.024082517765205484,\n",
       "    0.024270669043321032,\n",
       "    0.02301239475993247,\n",
       "    0.02354771178215742,\n",
       "    0.02308075830083469,\n",
       "    0.022950791336339097,\n",
       "    0.022370709202669817,\n",
       "    0.02227962492354985,\n",
       "    0.02183889629768914,\n",
       "    0.021969921501546072,\n",
       "    0.020436550715360147,\n",
       "    0.019764563004518378,\n",
       "    0.02083074248492204,\n",
       "    0.01949614504801816,\n",
       "    0.019118679141433073,\n",
       "    0.019110271321802305,\n",
       "    0.01837742688327,\n",
       "    0.01816883121199649,\n",
       "    0.018345950845757436,\n",
       "    0.016960560344159603,\n",
       "    0.017223400401015734,\n",
       "    0.017423166957651746,\n",
       "    0.017603704577376103,\n",
       "    0.017639926795301766,\n",
       "    0.016806793447327,\n",
       "    0.015467236974629862,\n",
       "    0.015622506046603465],\n",
       "   'val_loss': [0.005095501895993948,\n",
       "    0.006722958292812109,\n",
       "    0.005234993062913418,\n",
       "    0.00460275262594223,\n",
       "    0.004709327593445778,\n",
       "    0.00456258887425065,\n",
       "    0.0044926973059773445,\n",
       "    0.004981980659067631,\n",
       "    0.004605826456099749,\n",
       "    0.004324547480791807,\n",
       "    0.004531851503998041,\n",
       "    0.005085146054625511,\n",
       "    0.005320905242115259,\n",
       "    0.003989104647189379,\n",
       "    0.0043943352065980434,\n",
       "    0.004334195517003536,\n",
       "    0.006145771127194166,\n",
       "    0.004262000322341919,\n",
       "    0.0040619042702019215,\n",
       "    0.004294085316359997,\n",
       "    0.00476604700088501,\n",
       "    0.004089662339538336,\n",
       "    0.003938724286854267,\n",
       "    0.00427652383223176,\n",
       "    0.0041917855851352215,\n",
       "    0.0038722939789295197,\n",
       "    0.0040402947925031185,\n",
       "    0.004427381791174412,\n",
       "    0.003896786831319332,\n",
       "    0.004082908853888512,\n",
       "    0.00409676181152463,\n",
       "    0.004180331248790026,\n",
       "    0.003939021844416857,\n",
       "    0.004374989774078131,\n",
       "    0.004319936502724886,\n",
       "    0.0044796038419008255,\n",
       "    0.0038509482983499765,\n",
       "    0.0046129534021019936,\n",
       "    0.004053798504173756,\n",
       "    0.004104208201169968,\n",
       "    0.004301867447793484,\n",
       "    0.00384550541639328,\n",
       "    0.0040711211040616035,\n",
       "    0.003958188463002443,\n",
       "    0.004130315501242876,\n",
       "    0.004261174239218235,\n",
       "    0.0038850505370646715,\n",
       "    0.004101465921849012,\n",
       "    0.004064378794282675,\n",
       "    0.004301160573959351],\n",
       "   'train_mae': [0.2041602884900981,\n",
       "    0.1816690327792332,\n",
       "    0.17860711134713272,\n",
       "    0.17074274451568208,\n",
       "    0.16807880072758116,\n",
       "    0.1647438805164962,\n",
       "    0.16139515155348286,\n",
       "    0.16292501163893733,\n",
       "    0.1568341201235508,\n",
       "    0.1585422183932929,\n",
       "    0.15740968723749293,\n",
       "    0.15656262404959778,\n",
       "    0.1612080237988768,\n",
       "    0.1526587608045545,\n",
       "    0.1571440552843028,\n",
       "    0.15295036676628837,\n",
       "    0.15220651338840352,\n",
       "    0.15090162127182402,\n",
       "    0.15311638137389874,\n",
       "    0.15026565230098263,\n",
       "    0.14973864986978727,\n",
       "    0.14865006968892855,\n",
       "    0.1453764014716806,\n",
       "    0.14721410978457022,\n",
       "    0.145966873086732,\n",
       "    0.14398745421705575,\n",
       "    0.14426994632030354,\n",
       "    0.1416252581723805,\n",
       "    0.14139940615358024,\n",
       "    0.1401222032205812,\n",
       "    0.13858779828096257,\n",
       "    0.13833766019549862,\n",
       "    0.1356789341260647,\n",
       "    0.13310748595615912,\n",
       "    0.13002748350644933,\n",
       "    0.1331125880623686,\n",
       "    0.1298594664910744,\n",
       "    0.1288166750093986,\n",
       "    0.1277757384653749,\n",
       "    0.12479891319727075,\n",
       "    0.1248343242653485,\n",
       "    0.12659759793815942,\n",
       "    0.12097999016786444,\n",
       "    0.12133062759350086,\n",
       "    0.12121553230902245,\n",
       "    0.12232354257641168,\n",
       "    0.12020885327766681,\n",
       "    0.12180388641768489,\n",
       "    0.11431614335240989,\n",
       "    0.1141181056355608],\n",
       "   'val_mae': [0.0696435272693634,\n",
       "    0.08871205151081085,\n",
       "    0.05999954417347908,\n",
       "    0.04480218514800072,\n",
       "    0.0621461421251297,\n",
       "    0.04732159152626991,\n",
       "    0.0592753030359745,\n",
       "    0.056615620851516724,\n",
       "    0.06390530616044998,\n",
       "    0.04862581565976143,\n",
       "    0.061761144548654556,\n",
       "    0.07315098494291306,\n",
       "    0.07617845386266708,\n",
       "    0.0440564788877964,\n",
       "    0.057341381907463074,\n",
       "    0.056796107441186905,\n",
       "    0.08855532854795456,\n",
       "    0.04697804898023605,\n",
       "    0.04776076599955559,\n",
       "    0.042465053498744965,\n",
       "    0.06717721372842789,\n",
       "    0.050554294139146805,\n",
       "    0.041271716356277466,\n",
       "    0.043185047805309296,\n",
       "    0.049102533608675,\n",
       "    0.036216091364622116,\n",
       "    0.04459406062960625,\n",
       "    0.05553761497139931,\n",
       "    0.04059279337525368,\n",
       "    0.04558518901467323,\n",
       "    0.04265427216887474,\n",
       "    0.049753669649362564,\n",
       "    0.04352084547281265,\n",
       "    0.058455049991607666,\n",
       "    0.057127196341753006,\n",
       "    0.06250489503145218,\n",
       "    0.03962336853146553,\n",
       "    0.05165211856365204,\n",
       "    0.050255585461854935,\n",
       "    0.052307259291410446,\n",
       "    0.05449577793478966,\n",
       "    0.04529976844787598,\n",
       "    0.051451053470373154,\n",
       "    0.05038346350193024,\n",
       "    0.05253495275974274,\n",
       "    0.04847968742251396,\n",
       "    0.046479731798172,\n",
       "    0.051146816462278366,\n",
       "    0.050506267696619034,\n",
       "    0.05789831653237343],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001]},\n",
       "  'metrics': {'mse': 0.008602321147918701,\n",
       "   'rmse': 0.09274869890148703,\n",
       "   'mae': 0.057898301631212234,\n",
       "   'r2': 0.7785164713859558,\n",
       "   'mape': 7.6511406898498535},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.036774479473630585,\n",
       "    0.03164051752537489,\n",
       "    0.029573908013602098,\n",
       "    0.028697788218657174,\n",
       "    0.027597479491184154,\n",
       "    0.02777229870359103,\n",
       "    0.027648473034302394,\n",
       "    0.026416366298993427,\n",
       "    0.026040569537629683,\n",
       "    0.026227100441853207,\n",
       "    0.02643069646631678,\n",
       "    0.025893276712546747,\n",
       "    0.025846163493891558,\n",
       "    0.025250105746090414,\n",
       "    0.024946024517218272,\n",
       "    0.02546491681908568,\n",
       "    0.024331431556493043,\n",
       "    0.025037970021367074,\n",
       "    0.025659675213197865,\n",
       "    0.024885105341672896,\n",
       "    0.02474496072779099,\n",
       "    0.024444146764775118,\n",
       "    0.02424832616622249,\n",
       "    0.023654193555315336,\n",
       "    0.023062435537576677,\n",
       "    0.02318545536448558,\n",
       "    0.022717890795320272,\n",
       "    0.023450675730903942,\n",
       "    0.022570980712771416,\n",
       "    0.02329204302902023,\n",
       "    0.02227740700667103,\n",
       "    0.021911879318455854,\n",
       "    0.02198303593322635,\n",
       "    0.021330542303621768,\n",
       "    0.020650896771500508,\n",
       "    0.020200124941766262,\n",
       "    0.021287376806139947,\n",
       "    0.020636881577471893,\n",
       "    0.018854212450484433,\n",
       "    0.018138506558413307,\n",
       "    0.01787564034263293,\n",
       "    0.01714206483835975,\n",
       "    0.016900936188176275,\n",
       "    0.016893656551837923,\n",
       "    0.01687667431930701,\n",
       "    0.015722491933653753,\n",
       "    0.016451506906499467,\n",
       "    0.01518784196426471,\n",
       "    0.015054224167640011,\n",
       "    0.014896728672708075],\n",
       "   'val_loss': [0.010567956604063511,\n",
       "    0.00858230795711279,\n",
       "    0.010274250991642475,\n",
       "    0.008649341762065887,\n",
       "    0.008742688223719597,\n",
       "    0.009298738092184067,\n",
       "    0.008351149968802929,\n",
       "    0.007765456568449736,\n",
       "    0.007619089912623167,\n",
       "    0.008079783990979195,\n",
       "    0.007895121350884438,\n",
       "    0.007886877283453941,\n",
       "    0.007850557565689087,\n",
       "    0.00846069771796465,\n",
       "    0.00833730585873127,\n",
       "    0.008040492422878742,\n",
       "    0.0071265543811023235,\n",
       "    0.00717800622805953,\n",
       "    0.008053015917539597,\n",
       "    0.007130337879061699,\n",
       "    0.007615495007485151,\n",
       "    0.009636208415031433,\n",
       "    0.006951660383492708,\n",
       "    0.007300013210624456,\n",
       "    0.010569113306701183,\n",
       "    0.006267233286052942,\n",
       "    0.006232198793441057,\n",
       "    0.007846945896744728,\n",
       "    0.012349896132946014,\n",
       "    0.006925544235855341,\n",
       "    0.006491928361356258,\n",
       "    0.008834575302898884,\n",
       "    0.006698952987790108,\n",
       "    0.00911709200590849,\n",
       "    0.007238815538585186,\n",
       "    0.011222334578633308,\n",
       "    0.007328474894165993,\n",
       "    0.007035470567643642,\n",
       "    0.008318123407661915,\n",
       "    0.007793883793056011,\n",
       "    0.005891631823033094,\n",
       "    0.0069694216363132,\n",
       "    0.007966541685163975,\n",
       "    0.007982469163835049,\n",
       "    0.006293332204222679,\n",
       "    0.006065854337066412,\n",
       "    0.005939108319580555,\n",
       "    0.008203057572245598,\n",
       "    0.007605447433888912,\n",
       "    0.007377123460173607],\n",
       "   'train_mae': [0.19977018237113953,\n",
       "    0.17890638659397762,\n",
       "    0.16812630991141,\n",
       "    0.1675825039545695,\n",
       "    0.16098636140426,\n",
       "    0.15907541265090305,\n",
       "    0.16318068131804467,\n",
       "    0.15629227533936502,\n",
       "    0.155015912403663,\n",
       "    0.1543982525666555,\n",
       "    0.15447314927975336,\n",
       "    0.15389039516448974,\n",
       "    0.15324528589844705,\n",
       "    0.15092203716437022,\n",
       "    0.1515633448958397,\n",
       "    0.1504136085510254,\n",
       "    0.14983591164151827,\n",
       "    0.14633327374855679,\n",
       "    0.1524874990185102,\n",
       "    0.14715175479650497,\n",
       "    0.1494881456096967,\n",
       "    0.1463563879330953,\n",
       "    0.14747312292456627,\n",
       "    0.14161230400204658,\n",
       "    0.14285433168212572,\n",
       "    0.14250100950400035,\n",
       "    0.1398621934155623,\n",
       "    0.14102293426791826,\n",
       "    0.13699672222137452,\n",
       "    0.14186765551567077,\n",
       "    0.13839302683869997,\n",
       "    0.1345620428522428,\n",
       "    0.13760623062650362,\n",
       "    0.1338242381811142,\n",
       "    0.13168086831768352,\n",
       "    0.1306827167669932,\n",
       "    0.132792882869641,\n",
       "    0.13182035088539124,\n",
       "    0.12384515826900801,\n",
       "    0.1230795090397199,\n",
       "    0.12121048321326573,\n",
       "    0.11842467710375786,\n",
       "    0.11804852907856306,\n",
       "    0.1191516508658727,\n",
       "    0.11809950868288675,\n",
       "    0.11367142274975776,\n",
       "    0.11617535278201103,\n",
       "    0.11094131047526995,\n",
       "    0.11233172838886579,\n",
       "    0.10994572465618452],\n",
       "   'val_mae': [0.10268953442573547,\n",
       "    0.07192350924015045,\n",
       "    0.10828512161970139,\n",
       "    0.07673831284046173,\n",
       "    0.08702011406421661,\n",
       "    0.09677659720182419,\n",
       "    0.05903865769505501,\n",
       "    0.07490866631269455,\n",
       "    0.05485909804701805,\n",
       "    0.0765024870634079,\n",
       "    0.05303822457790375,\n",
       "    0.07299808412790298,\n",
       "    0.07243148982524872,\n",
       "    0.049379341304302216,\n",
       "    0.06934936344623566,\n",
       "    0.06767523288726807,\n",
       "    0.06395195424556732,\n",
       "    0.06707116216421127,\n",
       "    0.0693509504199028,\n",
       "    0.06312501430511475,\n",
       "    0.05669244006276131,\n",
       "    0.06650172173976898,\n",
       "    0.05614650249481201,\n",
       "    0.07086310535669327,\n",
       "    0.06588464230298996,\n",
       "    0.06027878820896149,\n",
       "    0.05429895222187042,\n",
       "    0.048602983355522156,\n",
       "    0.08148665726184845,\n",
       "    0.06052594259381294,\n",
       "    0.0566214881837368,\n",
       "    0.07210715115070343,\n",
       "    0.058621443808078766,\n",
       "    0.05951235070824623,\n",
       "    0.06088316813111305,\n",
       "    0.06357266753911972,\n",
       "    0.0749017670750618,\n",
       "    0.055342983454465866,\n",
       "    0.048846617341041565,\n",
       "    0.06575936079025269,\n",
       "    0.05393201485276222,\n",
       "    0.0551646463572979,\n",
       "    0.05653168261051178,\n",
       "    0.04773830622434616,\n",
       "    0.062336649745702744,\n",
       "    0.047433048486709595,\n",
       "    0.052486296743154526,\n",
       "    0.06113608554005623,\n",
       "    0.04890824481844902,\n",
       "    0.05990278720855713],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005]},\n",
       "  'metrics': {'mse': 0.014754248782992363,\n",
       "   'rmse': 0.12146706871820181,\n",
       "   'mae': 0.059902798384428024,\n",
       "   'r2': 0.6268211603164673,\n",
       "   'mape': 7.398682117462158},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.03170556487936166,\n",
       "    0.030399572044130293,\n",
       "    0.028696969301710205,\n",
       "    0.02772540719278397,\n",
       "    0.026864401004727808,\n",
       "    0.026394641687793118,\n",
       "    0.02671956577368321,\n",
       "    0.025735180224141768,\n",
       "    0.02515876927082577,\n",
       "    0.025604925629112027,\n",
       "    0.025256412103772163,\n",
       "    0.025312788033437346,\n",
       "    0.024309210283982176,\n",
       "    0.02508759023922105,\n",
       "    0.024492319252702496,\n",
       "    0.023752783515280294,\n",
       "    0.02431328336317693,\n",
       "    0.02403278916233009,\n",
       "    0.02327116209292604,\n",
       "    0.0235017005714678,\n",
       "    0.023689257551825816,\n",
       "    0.023564696492206667,\n",
       "    0.023062980673726526,\n",
       "    0.023325215694644758,\n",
       "    0.022587021932967247,\n",
       "    0.022450041746900927,\n",
       "    0.022341125674786105,\n",
       "    0.022467078070246404,\n",
       "    0.022133723321941592,\n",
       "    0.023259771897667838,\n",
       "    0.0223619872103295,\n",
       "    0.020067221455035672,\n",
       "    0.020368119791871118,\n",
       "    0.019539906252776424,\n",
       "    0.019807988387202064,\n",
       "    0.01849078594316398,\n",
       "    0.017529093149688937,\n",
       "    0.01691704655006047,\n",
       "    0.01678169170214284,\n",
       "    0.016181288136830254,\n",
       "    0.015926394461383744,\n",
       "    0.015283490531146526,\n",
       "    0.01564431355725373,\n",
       "    0.0145371196671359],\n",
       "   'val_loss': [0.05478015914559364,\n",
       "    0.055384572595357895,\n",
       "    0.050058651715517044,\n",
       "    0.05023850500583649,\n",
       "    0.05143212154507637,\n",
       "    0.05109049007296562,\n",
       "    0.04645991325378418,\n",
       "    0.05098412558436394,\n",
       "    0.0451415553689003,\n",
       "    0.04923522472381592,\n",
       "    0.044738464057445526,\n",
       "    0.04863343760371208,\n",
       "    0.048539821058511734,\n",
       "    0.05405263230204582,\n",
       "    0.04638097062706947,\n",
       "    0.0523996502161026,\n",
       "    0.05731799453496933,\n",
       "    0.04744794964790344,\n",
       "    0.045221902430057526,\n",
       "    0.0481017641723156,\n",
       "    0.04328799247741699,\n",
       "    0.053201302886009216,\n",
       "    0.047383733093738556,\n",
       "    0.04307424649596214,\n",
       "    0.048511702567338943,\n",
       "    0.05283011868596077,\n",
       "    0.05864769220352173,\n",
       "    0.055339787155389786,\n",
       "    0.054522424936294556,\n",
       "    0.05156756564974785,\n",
       "    0.055067844688892365,\n",
       "    0.05858460068702698,\n",
       "    0.05907740816473961,\n",
       "    0.062047962099313736,\n",
       "    0.05387197807431221,\n",
       "    0.05450614541769028,\n",
       "    0.05707355588674545,\n",
       "    0.055844005197286606,\n",
       "    0.05476386472582817,\n",
       "    0.05894653499126434,\n",
       "    0.06514359265565872,\n",
       "    0.06843341886997223,\n",
       "    0.0629432424902916,\n",
       "    0.06948225200176239],\n",
       "   'train_mae': [0.17751045813483576,\n",
       "    0.17353041518119075,\n",
       "    0.1623442588794616,\n",
       "    0.16262213188794353,\n",
       "    0.157604644615804,\n",
       "    0.1558304472315696,\n",
       "    0.15482154680836585,\n",
       "    0.15292021295716685,\n",
       "    0.15062123368824681,\n",
       "    0.15036921731887326,\n",
       "    0.15183300116369802,\n",
       "    0.1487581429462279,\n",
       "    0.1468099060077821,\n",
       "    0.14677782885489926,\n",
       "    0.14653137061865099,\n",
       "    0.14204578053566716,\n",
       "    0.1448951949996333,\n",
       "    0.14315392341344588,\n",
       "    0.14227235413366748,\n",
       "    0.14007125049829483,\n",
       "    0.14489166630852607,\n",
       "    0.13953722220274709,\n",
       "    0.1395624652504921,\n",
       "    0.1403553906467653,\n",
       "    0.13874011342563936,\n",
       "    0.1385759884311307,\n",
       "    0.13836247666228202,\n",
       "    0.13586239252359636,\n",
       "    0.13739636564447033,\n",
       "    0.1413860647909103,\n",
       "    0.1380187285042578,\n",
       "    0.12836270082381465,\n",
       "    0.12956545165469568,\n",
       "    0.12843723907586066,\n",
       "    0.1257041824921485,\n",
       "    0.12436873585947099,\n",
       "    0.12020419610123481,\n",
       "    0.11599768722249616,\n",
       "    0.11842278511293473,\n",
       "    0.1152117264366919,\n",
       "    0.11430168488333302,\n",
       "    0.11190233427670694,\n",
       "    0.11166541038020965,\n",
       "    0.10895029359286831],\n",
       "   'val_mae': [0.2678653597831726,\n",
       "    0.2545113265514374,\n",
       "    0.2513205409049988,\n",
       "    0.2396898716688156,\n",
       "    0.23925291001796722,\n",
       "    0.2300134003162384,\n",
       "    0.2318374067544937,\n",
       "    0.22341911494731903,\n",
       "    0.2221132516860962,\n",
       "    0.22904320061206818,\n",
       "    0.2198582887649536,\n",
       "    0.22608090937137604,\n",
       "    0.21966592967510223,\n",
       "    0.22244907915592194,\n",
       "    0.21631045639514923,\n",
       "    0.2096380889415741,\n",
       "    0.22909143567085266,\n",
       "    0.2191033512353897,\n",
       "    0.22747597098350525,\n",
       "    0.21664808690547943,\n",
       "    0.2086511254310608,\n",
       "    0.21392521262168884,\n",
       "    0.21762678027153015,\n",
       "    0.2121572494506836,\n",
       "    0.2158638834953308,\n",
       "    0.21335656940937042,\n",
       "    0.21386824548244476,\n",
       "    0.22563961148262024,\n",
       "    0.2152664065361023,\n",
       "    0.212795227766037,\n",
       "    0.20726783573627472,\n",
       "    0.21987397968769073,\n",
       "    0.2095349282026291,\n",
       "    0.2254662811756134,\n",
       "    0.21821191906929016,\n",
       "    0.2137819528579712,\n",
       "    0.21416018903255463,\n",
       "    0.217905655503273,\n",
       "    0.22769972681999207,\n",
       "    0.21541129052639008,\n",
       "    0.21706588566303253,\n",
       "    0.2282530963420868,\n",
       "    0.2128095030784607,\n",
       "    0.23732855916023254],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005]},\n",
       "  'metrics': {'mse': 0.13906854391098022,\n",
       "   'rmse': 0.3729189508606129,\n",
       "   'mae': 0.23732857406139374,\n",
       "   'r2': -0.06895661354064941,\n",
       "   'mape': 109.05182647705078},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.03581985365599394,\n",
       "    0.03178089958964847,\n",
       "    0.0289438437030185,\n",
       "    0.027902719477424398,\n",
       "    0.027228204358834773,\n",
       "    0.02698092977516353,\n",
       "    0.027140356076415628,\n",
       "    0.026930652413284406,\n",
       "    0.026241999643389136,\n",
       "    0.026461945759365335,\n",
       "    0.025867509189993143,\n",
       "    0.025182817480526865,\n",
       "    0.025159989221720025,\n",
       "    0.02596219244878739,\n",
       "    0.024798380938591436,\n",
       "    0.025347885559313,\n",
       "    0.024794358760118484,\n",
       "    0.02378488951944746,\n",
       "    0.024272311275126413,\n",
       "    0.02377306527341716,\n",
       "    0.02462202514288947,\n",
       "    0.02366350233205594,\n",
       "    0.02355854120105505,\n",
       "    0.023897979146568105,\n",
       "    0.023949560447363183,\n",
       "    0.02264803044090513,\n",
       "    0.02235654616379179,\n",
       "    0.022225449210964143,\n",
       "    0.022169157295138575,\n",
       "    0.020873565139481798,\n",
       "    0.02124770436785184,\n",
       "    0.021282693021930754,\n",
       "    0.02172892278758809,\n",
       "    0.02005242300219834],\n",
       "   'val_loss': [0.03850644454360008,\n",
       "    0.04162047803401947,\n",
       "    0.03818574547767639,\n",
       "    0.03923474997282028,\n",
       "    0.03573459014296532,\n",
       "    0.0362585224211216,\n",
       "    0.03520342335104942,\n",
       "    0.03651752695441246,\n",
       "    0.03671782836318016,\n",
       "    0.03567059710621834,\n",
       "    0.03821220621466637,\n",
       "    0.03449505195021629,\n",
       "    0.03578610345721245,\n",
       "    0.03409787639975548,\n",
       "    0.03506879881024361,\n",
       "    0.03582002595067024,\n",
       "    0.03448043018579483,\n",
       "    0.037084002047777176,\n",
       "    0.035231880843639374,\n",
       "    0.035213932394981384,\n",
       "    0.03544348478317261,\n",
       "    0.04096950218081474,\n",
       "    0.04019184410572052,\n",
       "    0.04231806844472885,\n",
       "    0.035613853484392166,\n",
       "    0.03959383815526962,\n",
       "    0.03851957619190216,\n",
       "    0.03991490975022316,\n",
       "    0.04610701650381088,\n",
       "    0.04585990682244301,\n",
       "    0.04514516144990921,\n",
       "    0.04379422962665558,\n",
       "    0.039772022515535355,\n",
       "    0.047586120665073395],\n",
       "   'train_mae': [0.19687242712825537,\n",
       "    0.17716717068105936,\n",
       "    0.16987884743139148,\n",
       "    0.16200371365994215,\n",
       "    0.15909270686097443,\n",
       "    0.1590712801553309,\n",
       "    0.1580672520212829,\n",
       "    0.1585990118328482,\n",
       "    0.15454341401346028,\n",
       "    0.15620816498994827,\n",
       "    0.1538336598314345,\n",
       "    0.15314364270307124,\n",
       "    0.14980491856113076,\n",
       "    0.15191800380125642,\n",
       "    0.1514205054845661,\n",
       "    0.1507402656134218,\n",
       "    0.14941864041611552,\n",
       "    0.14476290601305664,\n",
       "    0.1461209838744253,\n",
       "    0.14409279986284673,\n",
       "    0.14527030126191676,\n",
       "    0.1431174275930971,\n",
       "    0.1448821066878736,\n",
       "    0.1438277771230787,\n",
       "    0.1441867204848677,\n",
       "    0.13998470176011324,\n",
       "    0.13765733176842332,\n",
       "    0.13904777821153402,\n",
       "    0.1367801851592958,\n",
       "    0.13324434962123632,\n",
       "    0.13316409778781235,\n",
       "    0.133997340220958,\n",
       "    0.1338156193960458,\n",
       "    0.13138476805761456],\n",
       "   'val_mae': [0.24025775492191315,\n",
       "    0.24966442584991455,\n",
       "    0.23179832100868225,\n",
       "    0.23457017540931702,\n",
       "    0.19331417977809906,\n",
       "    0.21924999356269836,\n",
       "    0.21589554846286774,\n",
       "    0.2007116675376892,\n",
       "    0.2167568951845169,\n",
       "    0.21413852274417877,\n",
       "    0.22792379558086395,\n",
       "    0.1965893805027008,\n",
       "    0.21683169901371002,\n",
       "    0.2040274739265442,\n",
       "    0.19831621646881104,\n",
       "    0.2130267471075058,\n",
       "    0.2096320539712906,\n",
       "    0.2182985097169876,\n",
       "    0.21322661638259888,\n",
       "    0.19884416460990906,\n",
       "    0.20893047749996185,\n",
       "    0.2418832629919052,\n",
       "    0.23080623149871826,\n",
       "    0.24481944739818573,\n",
       "    0.2022397518157959,\n",
       "    0.23231352865695953,\n",
       "    0.22320739924907684,\n",
       "    0.22602152824401855,\n",
       "    0.24858848750591278,\n",
       "    0.24759282171726227,\n",
       "    0.24455556273460388,\n",
       "    0.24266165494918823,\n",
       "    0.21143954992294312,\n",
       "    0.24945147335529327],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005]},\n",
       "  'metrics': {'mse': 0.09517224878072739,\n",
       "   'rmse': 0.30849999802386935,\n",
       "   'mae': 0.24945147335529327,\n",
       "   'r2': 0.058207809925079346,\n",
       "   'mape': 56.5208625793457},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}},\n",
       " {'history': {'train_loss': [0.0352557043855389,\n",
       "    0.03061219293511275,\n",
       "    0.02961973829025572,\n",
       "    0.027896171915486004,\n",
       "    0.028223333823861496,\n",
       "    0.027241047612871185,\n",
       "    0.026322944682430138,\n",
       "    0.02637651145006671,\n",
       "    0.02644456544834556,\n",
       "    0.027339668775146656,\n",
       "    0.025850920796845898,\n",
       "    0.025821455359233147,\n",
       "    0.025517053718706877,\n",
       "    0.025185872546651146,\n",
       "    0.025133296424015003,\n",
       "    0.0243991944148685,\n",
       "    0.025262467563152313,\n",
       "    0.025175237689505924,\n",
       "    0.02429990428076549,\n",
       "    0.02452975819169572,\n",
       "    0.02374654987149618,\n",
       "    0.023501977211598194,\n",
       "    0.025052751273368343,\n",
       "    0.0242367402622194,\n",
       "    0.02446339452977885,\n",
       "    0.023279854914907253,\n",
       "    0.02314310653530287,\n",
       "    0.024960470329405682,\n",
       "    0.022723908592579944,\n",
       "    0.022958572389501514,\n",
       "    0.02263296523949865,\n",
       "    0.022054989596433712,\n",
       "    0.021503178346337692,\n",
       "    0.020621357372764385,\n",
       "    0.020692035301842472,\n",
       "    0.020444235228227848,\n",
       "    0.019728009476128853,\n",
       "    0.01898101121751648,\n",
       "    0.018535610147272095,\n",
       "    0.01860328425060619,\n",
       "    0.018543141132051296,\n",
       "    0.01812364035427119,\n",
       "    0.019422331119351315,\n",
       "    0.01753177759096478,\n",
       "    0.016082172394927704,\n",
       "    0.015664418369080082,\n",
       "    0.01479892408701055,\n",
       "    0.014814589595930143,\n",
       "    0.01500321226194501,\n",
       "    0.01518006238973502],\n",
       "   'val_loss': [0.07517950981855392,\n",
       "    0.07393023371696472,\n",
       "    0.07124756276607513,\n",
       "    0.07351992279291153,\n",
       "    0.0711299404501915,\n",
       "    0.07098332792520523,\n",
       "    0.07024744153022766,\n",
       "    0.06962283700704575,\n",
       "    0.06786364316940308,\n",
       "    0.06724125146865845,\n",
       "    0.06968725472688675,\n",
       "    0.07141229510307312,\n",
       "    0.06722354143857956,\n",
       "    0.06592248380184174,\n",
       "    0.06747384369373322,\n",
       "    0.06559018790721893,\n",
       "    0.06800531595945358,\n",
       "    0.06705465912818909,\n",
       "    0.06553575396537781,\n",
       "    0.06656933575868607,\n",
       "    0.06561556458473206,\n",
       "    0.06649736315011978,\n",
       "    0.06805630028247833,\n",
       "    0.0673026368021965,\n",
       "    0.07076305896043777,\n",
       "    0.06667973101139069,\n",
       "    0.06522496789693832,\n",
       "    0.06946404278278351,\n",
       "    0.06753567606210709,\n",
       "    0.06080625206232071,\n",
       "    0.057782869786024094,\n",
       "    0.0672234296798706,\n",
       "    0.05502212047576904,\n",
       "    0.060705095529556274,\n",
       "    0.05556023120880127,\n",
       "    0.06917399168014526,\n",
       "    0.06427124887704849,\n",
       "    0.058341894298791885,\n",
       "    0.061129771173000336,\n",
       "    0.06243271753191948,\n",
       "    0.06443159282207489,\n",
       "    0.0652393326163292,\n",
       "    0.0587146021425724,\n",
       "    0.06243908405303955,\n",
       "    0.05567776411771774,\n",
       "    0.0691656693816185,\n",
       "    0.06412708759307861,\n",
       "    0.062271323055028915,\n",
       "    0.06727989763021469,\n",
       "    0.07370300590991974],\n",
       "   'train_mae': [0.19463262887615146,\n",
       "    0.1717850398836714,\n",
       "    0.16924190205154996,\n",
       "    0.16192094372077423,\n",
       "    0.16196877125537756,\n",
       "    0.15859030683835348,\n",
       "    0.1565067366217122,\n",
       "    0.15537609453454163,\n",
       "    0.15311939698277097,\n",
       "    0.15506858536691376,\n",
       "    0.15421380125211948,\n",
       "    0.15216881239956076,\n",
       "    0.14881973578171295,\n",
       "    0.1516024698362206,\n",
       "    0.14848557408108856,\n",
       "    0.14474801764343725,\n",
       "    0.1486506423715389,\n",
       "    0.14956870878284628,\n",
       "    0.14457979279034067,\n",
       "    0.14669555154713718,\n",
       "    0.1432598741217093,\n",
       "    0.14005855199965564,\n",
       "    0.14826071736487476,\n",
       "    0.14614384531071692,\n",
       "    0.1447846209912589,\n",
       "    0.1447324515743689,\n",
       "    0.13883443786339325,\n",
       "    0.15036211275693143,\n",
       "    0.14021250108877817,\n",
       "    0.13836594625855936,\n",
       "    0.13869836294289792,\n",
       "    0.13680894898645807,\n",
       "    0.13697580112652344,\n",
       "    0.130910865510955,\n",
       "    0.1303887807510116,\n",
       "    0.13038441042105356,\n",
       "    0.1296063740596627,\n",
       "    0.1264693389336268,\n",
       "    0.12491554044412845,\n",
       "    0.12423921421621785,\n",
       "    0.12450634665561444,\n",
       "    0.12170884487303821,\n",
       "    0.12514747408303348,\n",
       "    0.12148553674871271,\n",
       "    0.1155366651488073,\n",
       "    0.11409112020875468,\n",
       "    0.10909757853457422,\n",
       "    0.11030235389868419,\n",
       "    0.11016569873600295,\n",
       "    0.11105423082004894],\n",
       "   'val_mae': [0.2833964228630066,\n",
       "    0.30220159888267517,\n",
       "    0.2861112356185913,\n",
       "    0.2768956422805786,\n",
       "    0.2833670675754547,\n",
       "    0.29151222109794617,\n",
       "    0.2821907699108124,\n",
       "    0.2641906440258026,\n",
       "    0.27193430066108704,\n",
       "    0.25551143288612366,\n",
       "    0.29280921816825867,\n",
       "    0.3022516071796417,\n",
       "    0.2679414749145508,\n",
       "    0.2682698667049408,\n",
       "    0.2732431888580322,\n",
       "    0.26576942205429077,\n",
       "    0.27383819222450256,\n",
       "    0.27906256914138794,\n",
       "    0.2690327763557434,\n",
       "    0.2795095145702362,\n",
       "    0.26597389578819275,\n",
       "    0.26047953963279724,\n",
       "    0.2883201837539673,\n",
       "    0.2871444523334503,\n",
       "    0.27874380350112915,\n",
       "    0.27623286843299866,\n",
       "    0.27915966510772705,\n",
       "    0.2987775504589081,\n",
       "    0.29400935769081116,\n",
       "    0.24943898618221283,\n",
       "    0.25767233967781067,\n",
       "    0.2668219804763794,\n",
       "    0.24787400662899017,\n",
       "    0.2590795159339905,\n",
       "    0.2589012682437897,\n",
       "    0.2622840106487274,\n",
       "    0.27755245566368103,\n",
       "    0.27111104130744934,\n",
       "    0.2651554048061371,\n",
       "    0.2469869703054428,\n",
       "    0.27764278650283813,\n",
       "    0.27636921405792236,\n",
       "    0.25288593769073486,\n",
       "    0.24559840559959412,\n",
       "    0.23783990740776062,\n",
       "    0.2818753719329834,\n",
       "    0.2687808871269226,\n",
       "    0.2584298849105835,\n",
       "    0.2660440504550934,\n",
       "    0.2870626747608185],\n",
       "   'lr': [0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.001,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005,\n",
       "    0.0005]},\n",
       "  'metrics': {'mse': 0.14769043028354645,\n",
       "   'rmse': 0.3843051265382059,\n",
       "   'mae': 0.2870626747608185,\n",
       "   'r2': 0.07417482137680054,\n",
       "   'mape': 62.45063400268555},\n",
       "  'model': ImprovedLSTM(\n",
       "    (input_projection): Linear(in_features=77, out_features=64, bias=True)\n",
       "    (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (output_projection): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'scalers': {'feature_scaler': StandardScaler(),\n",
       "   'target_scaler': StandardScaler(with_std=False)}}]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965dd293",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.utils:Saved arrays to data/phas5_data_*.npy\n",
      "INFO:src.utils:X shape: (1056, 7, 11, 9), Y shape: (1056, 1, 11)\n",
      "INFO:src.pipeline:Loading data...\n",
      "INFO:src.utils:Loaded arrays from data/phas5_data_*.npy\n",
      "INFO:src.utils:X shape: (1056, 7, 11, 9), Y shape: (1056, 1, 11)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 2: Building model arrays (X, Y) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:src.pipeline:Failed to construct reference dataframe for NAM comparison: could not convert string to float: '2014-01-03 14:00:00+00:00'\n",
      "INFO:src.utils:Loaded 35 folds from exp-003/exp-003rolling_origin_splits.json\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 1 ===\n",
      "INFO:src.pipeline:Train samples: 21, Val samples: 18\n",
      "INFO:src.pipeline:Model parameters: 94,475\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.056521, Val Loss: 0.097582, Train MAE: 0.241174, Val MAE: 0.341793, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.051135, Val Loss: 0.089080, Train MAE: 0.223228, Val MAE: 0.332661, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.044316, Val Loss: 0.078961, Train MAE: 0.192104, Val MAE: 0.334169, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.027990, Val Loss: 0.184373, Train MAE: 0.153841, Val MAE: 0.511356, LR: 0.001000\n",
      "INFO:src.engine:Epoch [50/50] - Train Loss: 0.024508, Val Loss: 0.144790, Train MAE: 0.150110, Val MAE: 0.462067, LR: 0.000500\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 1.\n",
      "INFO:src.pipeline:Fold 1 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.289604\n",
      "INFO:src.pipeline:  rmse: 0.538149\n",
      "INFO:src.pipeline:  mae: 0.462067\n",
      "INFO:src.pipeline:  r2: -0.749676\n",
      "INFO:src.pipeline:  mape: 68.026848\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 2 ===\n",
      "INFO:src.pipeline:Train samples: 40, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 94,475\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.061697, Val Loss: 0.050070, Train MAE: 0.291221, Val MAE: 0.278748, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.058376, Val Loss: 0.056609, Train MAE: 0.282970, Val MAE: 0.302836, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 26\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 2.\n",
      "INFO:src.pipeline:Fold 2 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.116785\n",
      "INFO:src.pipeline:  rmse: 0.341739\n",
      "INFO:src.pipeline:  mae: 0.310008\n",
      "INFO:src.pipeline:  r2: -0.049910\n",
      "INFO:src.pipeline:  mape: 68.845589\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 3 ===\n",
      "INFO:src.pipeline:Train samples: 71, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 94,475\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.054084, Val Loss: 0.033211, Train MAE: 0.266147, Val MAE: 0.209085, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.052676, Val Loss: 0.027628, Train MAE: 0.256740, Val MAE: 0.184221, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.037011, Val Loss: 0.027467, Train MAE: 0.196689, Val MAE: 0.171648, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.024642, Val Loss: 0.028390, Train MAE: 0.156203, Val MAE: 0.176091, LR: 0.001000\n",
      "INFO:src.engine:Epoch [50/50] - Train Loss: 0.025213, Val Loss: 0.028041, Train MAE: 0.150430, Val MAE: 0.157203, LR: 0.000500\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 3.\n",
      "INFO:src.pipeline:Fold 3 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.056081\n",
      "INFO:src.pipeline:  rmse: 0.236815\n",
      "INFO:src.pipeline:  mae: 0.157203\n",
      "INFO:src.pipeline:  r2: 0.324149\n",
      "INFO:src.pipeline:  mape: 38.256535\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 4 ===\n",
      "INFO:src.pipeline:Train samples: 101, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 94,475\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.039917, Val Loss: 0.014063, Train MAE: 0.221201, Val MAE: 0.139516, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.033806, Val Loss: 0.012698, Train MAE: 0.190277, Val MAE: 0.106916, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.029452, Val Loss: 0.011651, Train MAE: 0.170673, Val MAE: 0.107846, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.031116, Val Loss: 0.011944, Train MAE: 0.179662, Val MAE: 0.102601, LR: 0.001000\n",
      "Exception ignored in: <function _releaseLock at 0x7fa552e9e7a0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/logging/__init__.py\", line 228, in _releaseLock\n",
      "    def _releaseLock():\n",
      "KeyboardInterrupt: \n",
      "INFO:src.engine:Epoch [50/50] - Train Loss: 0.021400, Val Loss: 0.011889, Train MAE: 0.140039, Val MAE: 0.094648, LR: 0.000500\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 4.\n",
      "INFO:src.pipeline:Fold 4 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.023779\n",
      "INFO:src.pipeline:  rmse: 0.154203\n",
      "INFO:src.pipeline:  mae: 0.094648\n",
      "INFO:src.pipeline:  r2: 0.489852\n",
      "INFO:src.pipeline:  mape: 14.489473\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 5 ===\n",
      "INFO:src.pipeline:Train samples: 132, Val samples: 28\n",
      "INFO:src.pipeline:Model parameters: 94,475\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.041353, Val Loss: 0.008767, Train MAE: 0.221385, Val MAE: 0.123045, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.036919, Val Loss: 0.006561, Train MAE: 0.182816, Val MAE: 0.075655, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.034941, Val Loss: 0.007404, Train MAE: 0.185780, Val MAE: 0.080267, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 35\n",
      "WARNING:src.pipeline:Reference dataframe missing; skipping NAM comparison for fold 5.\n",
      "INFO:src.pipeline:Fold 5 - Validation Metrics:\n",
      "INFO:src.pipeline:  mse: 0.018599\n",
      "INFO:src.pipeline:  rmse: 0.136378\n",
      "INFO:src.pipeline:  mae: 0.096828\n",
      "INFO:src.pipeline:  r2: 0.303155\n",
      "INFO:src.pipeline:  mape: 11.521437\n",
      "INFO:src.pipeline:\n",
      "=== Running Fold 6 ===\n",
      "INFO:src.pipeline:Train samples: 162, Val samples: 29\n",
      "INFO:src.pipeline:Model parameters: 94,475\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n"
     ]
    }
   ],
   "source": [
    "from src.preprocessing import KBinConfig, process_splits_to_kbins, build_model_arrays, to_fixedgrid_multiindex\n",
    "import pandas as pd\n",
    "from src.utils import DataManager\n",
    "from src.pipeline import SolarForecastingPipeline\n",
    "\n",
    "\n",
    "df_phase5 = df_interpolated[['CSI_ghi', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos',\n",
    "     'nam_cc',\n",
    "        '80_cloud_cover',\n",
    "       '56_cloud_cover', '20_cloud_cover',\n",
    "       '88_cloud_cover']]\n",
    "\n",
    "\n",
    "# --- Block 1: Define variables ---\n",
    "USE_KBINS = False\n",
    "k_bins = 60\n",
    "TARGET_COL = \"CSI_ghi\"  # <-- FIX 1: Define Target FIRST\n",
    "history_days = 7\n",
    "horizon_days = 1\n",
    "\n",
    "# This list now correctly excludes the target\n",
    "feature_cols = [c for c in df_phase5.columns.tolist() if c != TARGET_COL]\n",
    "\n",
    "# --- Block 2: Build model arrays (Your code was correct here) ---\n",
    "if USE_KBINS:\n",
    "        print(\"\\n--- Step 2: Converting full dataset to K-Bins format ---\")\n",
    "        # ... (rest of your K-Bins code) ...\n",
    "else:\n",
    "    print(\"\\n--- Step 2: Building model arrays (X, Y) ---\")\n",
    "    fixed_df = to_fixedgrid_multiindex(df_phase5, timestamp_col=\"measurement_time\", expected_T=None)  # or set T\n",
    "    \n",
    "    # You correctly passed the filtered 'feature_cols' here, fixing the data leak!\n",
    "    ph5_X, ph5_Y, ph5_labels_list = build_model_arrays(\n",
    "        fixed_df,\n",
    "        feature_cols=feature_cols,  # <-- This is correct!\n",
    "        target_col=TARGET_COL,\n",
    "        history_days=history_days,\n",
    "        horizon_days=horizon_days,\n",
    "    )\n",
    "\n",
    "# --- Block 3: Save arrays ---\n",
    "data_manager = DataManager()\n",
    "data_manager.save_arrays(\n",
    "    ph5_X, ph5_Y,\n",
    "    pd.DataFrame(index=pd.to_datetime(ph5_labels_list, utc=True)),\n",
    "    filename_prefix='phas5_data',  # <-- You are saving as 'phas2_data'\n",
    "    feature_cols=feature_cols,\n",
    "    target_col=TARGET_COL,\n",
    "    metadata={\n",
    "        \"input_csv\": \"data/processed/dayTime_NAM_spatial_5locations_dayahead_features_processed.csv\",\n",
    "        \"timestamp_col\": \"measurement_time\",\n",
    "        \"feature_set\": feature_cols,\n",
    "        \"history_days\": 7,\n",
    "        \"horizon_days\": 1,\n",
    "        \"k_bins\": None,\n",
    "    }\n",
    ")\n",
    "\n",
    "# --- Block 4: Configure and Run ---\n",
    "LSTM_CONFIG = {\n",
    "    \"experiment_name\": \"LSTM_UniDirectional_phase5_exp02\",\n",
    "    \"model_type\": \"LSTM\",\n",
    "    \"model_config\": {\n",
    "        \"hidden_size\": 64,\n",
    "        \"num_layers\": 2,\n",
    "        \"dropout\": 0.25,\n",
    "        \"bidirectional\": False,\n",
    "    },\n",
    "    \"data_prefix\": \"phas5_data\",  # <-- FIX 2: Match the filename_prefix\n",
    "    \"splits_file\": \"exp-003/exp-003rolling_origin_splits.json\",\n",
    "    \"feature_cols\": feature_cols,\n",
    "    \"feature_selection\": feature_cols, # <-- This is correct!\n",
    "    \"target_col\": TARGET_COL,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_epochs\": 50,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"loss_function\": \"Huber\",  # <-- This setting needs the fix below\n",
    "    \"early_stopping_patience\": 20,\n",
    "    \"max_folds\": 35,\n",
    "}\n",
    "\n",
    "# This will now work after you apply the fixes below\n",
    "pipeline = SolarForecastingPipeline(LSTM_CONFIG)\n",
    "phas5_LSTM_UniDir_fold_results, summary = pipeline.run()\n",
    "\n",
    "# To get the model from the LAST fold:\n",
    "phas5_LSTM_UniDir_fold_results_model = phas5_LSTM_UniDir_fold_results[-1]['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103155c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactive plot saved successfully to: solar_irradiance_fold35.html\n",
      "Open this file in any web browser to view the interactive visualization.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2c9c54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
