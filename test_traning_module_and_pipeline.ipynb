{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbdfa13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 11,560 records\n",
      "Date range: 2014-01-03 14:00:00+00:00 to 2016-12-30 23:00:00+00:00\n",
      "Timezone: UTC\n",
      "No NaN (missing) values found in any of the specified features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4098084/256398810.py:11: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df_interpolated = df.interpolate(method='linear')\n"
     ]
    }
   ],
   "source": [
    "# Loading Data\n",
    "from src.data_preparation import load_data\n",
    "\n",
    "PROCESSED_DATA_PATH = \"data/processed/df_1h_lag_BLV_spatial_images.csv\"\n",
    "df = load_data(PROCESSED_DATA_PATH, date_col=\"measurement_time\")\n",
    "\n",
    "df.drop(columns=[\"Unnamed: 0\", \"timestamp\"], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "df_interpolated = df.interpolate(method='linear')\n",
    "\n",
    "df_interpolated.fillna(0, inplace=True)\n",
    "\n",
    "features_to_check = ['ghi', 'dni', 'solar_zenith', 'GHI_cs', 'DNI_cs', 'CSI_ghi', \n",
    "                                   'CSI_dni',\n",
    "                                   'nam_ghi', 'nam_dni', 'nam_cc', 'B_CSI_ghi_8h', 'V_CSI_ghi_8h', 'L_CSI_ghi_8h', 'B_CSI_ghi_9h',\n",
    "       'V_CSI_ghi_9h', 'L_CSI_ghi_9h', 'B_CSI_ghi_10h', 'V_CSI_ghi_10h',\n",
    "       'L_CSI_ghi_10h', 'B_CSI_ghi_11h', 'V_CSI_ghi_11h', 'L_CSI_ghi_11h',\n",
    "       'B_CSI_ghi_12h', 'V_CSI_ghi_12h', 'L_CSI_ghi_12h', 'B_CSI_ghi_13h',\n",
    "       'V_CSI_ghi_13h', 'L_CSI_ghi_13h', 'B_CSI_ghi_14h', 'V_CSI_ghi_14h',\n",
    "       'L_CSI_ghi_14h', 'B_CSI_ghi_15h', 'V_CSI_ghi_15h', 'L_CSI_ghi_15h',\n",
    "       'B_CSI_ghi_16h', 'V_CSI_ghi_16h', 'L_CSI_ghi_16h', 'B_CSI_ghi_17h',\n",
    "       'V_CSI_ghi_17h', 'L_CSI_ghi_17h', 'B_CSI_ghi_18h', 'V_CSI_ghi_18h',\n",
    "       'L_CSI_ghi_18h', 'B_CSI_ghi_19h', 'V_CSI_ghi_19h', 'L_CSI_ghi_19h',\n",
    "       '80_dwsw', '80_cloud_cover', '56_dwsw', '56_cloud_cover',\n",
    "       '20_dwsw', '20_cloud_cover', '88_dwsw', '88_cloud_cover', 'AVG(R)',\n",
    "       'STD(R)', 'ENT(R)', 'AVG(G)', 'STD(G)', 'ENT(G)', 'AVG(B)', 'STD(B)',\n",
    "       'ENT(B)', 'AVG(RB)', 'STD(RB)', 'ENT(RB)', 'AVG(NRB)', 'STD(NRB)',\n",
    "       'ENT(NRB)']\n",
    "\n",
    "# 2. Calculate the number of NaN values for each feature\n",
    "# We use .isna() instead of == 0\n",
    "nans_per_feature = df_interpolated[features_to_check].isna().sum()\n",
    "\n",
    "# 3. Filter to get only features that actually have NaN values\n",
    "features_with_nans = nans_per_feature[nans_per_feature > 0]\n",
    "\n",
    "# 4. Report the findings for which features have NaNs\n",
    "if features_with_nans.empty:\n",
    "    print(\"No NaN (missing) values found in any of the specified features.\")\n",
    "else:\n",
    "    print(\"--- Features With NaN Values ---\")\n",
    "    print(\"The following features have NaN values, with the total count for each:\")\n",
    "    # Sort for clearer output\n",
    "    print(features_with_nans.sort_values(ascending=False))\n",
    "    print(\"\\n\" + \"=\"*40 + \"\\n\")\n",
    "\n",
    "    # 5. Analyze the distribution of hours for rows containing NaNs\n",
    "    print(\"--- Distribution of NaN-Value Records by Hour ---\")\n",
    "    \n",
    "    # Create a boolean mask for rows that contain *at least one* NaN\n",
    "    # in the specified columns\n",
    "    rows_with_any_nan = df_interpolated[features_to_check].isna().any(axis=1)\n",
    "    \n",
    "    if rows_with_any_nan.sum() > 0:\n",
    "        # Get the index for these rows\n",
    "        nan_rows_index = df_interpolated.index[rows_with_any_nan]\n",
    "        \n",
    "        # Extract the hour from the DatetimeIndex and get the value counts\n",
    "        hour_distribution = nan_rows_index.hour.value_counts().sort_index()\n",
    "        \n",
    "        print(\"Distribution of records (rows) containing at least one NaN, by hour:\")\n",
    "        print(hour_distribution)\n",
    "        \n",
    "        # Optional: Print total number of affected rows\n",
    "        print(f\"\\nTotal number of rows with at least one NaN: {rows_with_any_nan.sum()}\")\n",
    "    else:\n",
    "        # This case shouldn't be hit if features_with_nans was not empty,\n",
    "        # but it's good practice to include.\n",
    "        print(\"No rows found with NaN values (this is unexpected, check logic).\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9002f78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fcb461d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_phase1 = df_interpolated[['solar_zenith', 'CSI_ghi','time_gap_hours',\n",
    "        'time_gap_norm', 'day_boundary_flag', 'hour_progression',\n",
    "        'absolute_hour',\n",
    "        'season_flag', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos']]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85b709c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 2: Building model arrays (X, Y) ---\n"
     ]
    }
   ],
   "source": [
    "from src.preprocessing import build_model_arrays, to_fixedgrid_multiindex\n",
    "import pandas as pd\n",
    "from src.utils import DataManager\n",
    "from src.pipeline import SolarForecastingPipeline\n",
    "\n",
    "# --- Block 1: Define variables ---\n",
    "\n",
    "TARGET_COL = \"CSI_ghi\"  # <-- FIX 1: Define Target FIRST\n",
    "history_days = 7\n",
    "horizon_days = 1\n",
    "\n",
    "# This list now correctly excludes the target\n",
    "feature_cols = [c for c in df_phase1.columns.tolist() if c != TARGET_COL]\n",
    "\n",
    "# --- Block 2: Build model arrays (Your code was correct here) ---\n",
    "\n",
    "\n",
    "print(\"\\n--- Step 2: Building model arrays (X, Y) ---\")\n",
    "fixed_df = to_fixedgrid_multiindex(df_phase1, timestamp_col=\"measurement_time\", expected_T=None)  # or set T\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "330ba2a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "0",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "9bf7f7f8-15b0-47f4-80b6-0980bef30c74",
       "rows": [
        [
         "solar_zenith",
         "0"
        ],
        [
         "CSI_ghi",
         "0"
        ],
        [
         "time_gap_hours",
         "0"
        ],
        [
         "time_gap_norm",
         "0"
        ],
        [
         "day_boundary_flag",
         "0"
        ],
        [
         "hour_progression",
         "0"
        ],
        [
         "absolute_hour",
         "0"
        ],
        [
         "season_flag",
         "0"
        ],
        [
         "hour_sin",
         "0"
        ],
        [
         "hour_cos",
         "0"
        ],
        [
         "month_sin",
         "0"
        ],
        [
         "month_cos",
         "0"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 12
       }
      },
      "text/plain": [
       "solar_zenith         0\n",
       "CSI_ghi              0\n",
       "time_gap_hours       0\n",
       "time_gap_norm        0\n",
       "day_boundary_flag    0\n",
       "hour_progression     0\n",
       "absolute_hour        0\n",
       "season_flag          0\n",
       "hour_sin             0\n",
       "hour_cos             0\n",
       "month_sin            0\n",
       "month_cos            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixed_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e82f2faa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('CSI_ghi',\n",
       " ['solar_zenith',\n",
       "  'time_gap_hours',\n",
       "  'time_gap_norm',\n",
       "  'day_boundary_flag',\n",
       "  'hour_progression',\n",
       "  'absolute_hour',\n",
       "  'season_flag',\n",
       "  'hour_sin',\n",
       "  'hour_cos',\n",
       "  'month_sin',\n",
       "  'month_cos'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TARGET_COL , feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "130ded27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['solar_zenith', 'CSI_ghi', 'time_gap_hours', 'time_gap_norm',\n",
       "       'day_boundary_flag', 'hour_progression', 'absolute_hour', 'season_flag',\n",
       "       'hour_sin', 'hour_cos', 'month_sin', 'month_cos'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixed_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca916ed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "0",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "9c841cb2-3521-433f-965e-919c8e0c45aa",
       "rows": [
        [
         "solar_zenith",
         "0"
        ],
        [
         "CSI_ghi",
         "0"
        ],
        [
         "time_gap_hours",
         "0"
        ],
        [
         "time_gap_norm",
         "0"
        ],
        [
         "day_boundary_flag",
         "0"
        ],
        [
         "hour_progression",
         "0"
        ],
        [
         "absolute_hour",
         "0"
        ],
        [
         "season_flag",
         "0"
        ],
        [
         "hour_sin",
         "0"
        ],
        [
         "hour_cos",
         "0"
        ],
        [
         "month_sin",
         "0"
        ],
        [
         "month_cos",
         "0"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 12
       }
      },
      "text/plain": [
       "solar_zenith         0\n",
       "CSI_ghi              0\n",
       "time_gap_hours       0\n",
       "time_gap_norm        0\n",
       "day_boundary_flag    0\n",
       "hour_progression     0\n",
       "absolute_hour        0\n",
       "season_flag          0\n",
       "hour_sin             0\n",
       "hour_cos             0\n",
       "month_sin            0\n",
       "month_cos            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixed_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f3f476a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# _tensor_from_norm NaN Issue\n",
    "norm_df = fixed_df.copy()\n",
    "norm_df = norm_df.sort_index()\n",
    "\n",
    "dates = list(norm_df.index.get_level_values(\"date\").unique())\n",
    "K = int(norm_df.index.get_level_values(\"bin_id\").max()) + 1\n",
    "# the number of features, the third dim.\n",
    "F = len(feature_cols)\n",
    "\n",
    "# (num_days, K_bins, F). Filled with (Not a Number)\n",
    "X = np.full((len(dates), K, F), np.nan, dtype=float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f6b6689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# builds the tensor one feature at a time.\n",
    "for j, col in enumerate(feature_cols):\n",
    "    if col not in norm_df.columns:\n",
    "        continue\n",
    "\n",
    "    # (date, bin_id) -> value; drop target_time by taking first. 2D matrix (mat) where rows are dates and columns are bin IDs.\n",
    "    mat = (\n",
    "            norm_df[col]                                                                # 1. Select Series\n",
    "            .groupby(level=[\"date\", \"bin_id\"])                                          # 2. Group by (date, bin)\n",
    "            .first()                         # one value per (date, bin_id)             # 3. Collapse time level\n",
    "            .unstack(\"bin_id\")               # rows: date, cols: bin_id                 # 4. Pivot bins to columns\n",
    "            .reindex(index=dates, columns=range(K))                                     # 5. Enforce shape\n",
    "        )\n",
    "\n",
    "X[:, :, j] = mat.values # takes the 2D matrix of data for the current feature and slots it perfectly into its designated \"slice\" of the final 3D tensor X.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cf2772d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.True_, np.int64(116942))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(X).any(), np.isnan(X).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dcb93530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Total NaN Analysis ---\n",
      "Total elements in X: 128502\n",
      "Total NaN values:    116942\n",
      "Overall NaN percentage: 91.00%\n",
      "\n",
      "--- 2. NaN Distribution per Bin ---\n",
      "Total data points per bin: 11682\n",
      "NaN counts and percentage per bin:\n",
      "  Bin 0: 10620 NaNs ( 90.91%)\n",
      "  Bin 1: 10631 NaNs ( 91.00%)\n",
      "  Bin 2: 10631 NaNs ( 91.00%)\n",
      "  Bin 3: 10631 NaNs ( 91.00%)\n",
      "  Bin 4: 10631 NaNs ( 91.00%)\n",
      "  Bin 5: 10631 NaNs ( 91.00%)\n",
      "  Bin 6: 10631 NaNs ( 91.00%)\n",
      "  Bin 7: 10631 NaNs ( 91.00%)\n",
      "  Bin 8: 10631 NaNs ( 91.00%)\n",
      "  Bin 9: 10631 NaNs ( 91.00%)\n",
      "  Bin 10: 10643 NaNs ( 91.11%)\n",
      "\n",
      "\n",
      "--- 3. NaN Distribution per Feature ---\n",
      "Total data points per feature: 11682\n",
      "NaN counts and percentage per feature:\n",
      "  solar_zenith   : 11682 NaNs (100.00%)\n",
      "  time_gap_hours : 11682 NaNs (100.00%)\n",
      "  time_gap_norm  : 11682 NaNs (100.00%)\n",
      "  day_boundary_flag: 11682 NaNs (100.00%)\n",
      "  hour_progression: 11682 NaNs (100.00%)\n",
      "  absolute_hour  : 11682 NaNs (100.00%)\n",
      "  season_flag    : 11682 NaNs (100.00%)\n",
      "  hour_sin       : 11682 NaNs (100.00%)\n",
      "  hour_cos       : 11682 NaNs (100.00%)\n",
      "  month_sin      : 11682 NaNs (100.00%)\n",
      "  month_cos      :  122 NaNs (  1.04%)\n",
      "\n",
      "\n",
      "--- 4. Detailed 2D Breakdown (NaN Counts per Bin and Feature) ---\n",
      "        solar_zenith  time_gap_hours  time_gap_norm  day_boundary_flag  \\\n",
      "Bin ID                                                                   \n",
      "0               1062            1062           1062               1062   \n",
      "1               1062            1062           1062               1062   \n",
      "2               1062            1062           1062               1062   \n",
      "3               1062            1062           1062               1062   \n",
      "4               1062            1062           1062               1062   \n",
      "5               1062            1062           1062               1062   \n",
      "6               1062            1062           1062               1062   \n",
      "7               1062            1062           1062               1062   \n",
      "8               1062            1062           1062               1062   \n",
      "9               1062            1062           1062               1062   \n",
      "10              1062            1062           1062               1062   \n",
      "\n",
      "        hour_progression  absolute_hour  season_flag  hour_sin  hour_cos  \\\n",
      "Bin ID                                                                     \n",
      "0                   1062           1062         1062      1062      1062   \n",
      "1                   1062           1062         1062      1062      1062   \n",
      "2                   1062           1062         1062      1062      1062   \n",
      "3                   1062           1062         1062      1062      1062   \n",
      "4                   1062           1062         1062      1062      1062   \n",
      "5                   1062           1062         1062      1062      1062   \n",
      "6                   1062           1062         1062      1062      1062   \n",
      "7                   1062           1062         1062      1062      1062   \n",
      "8                   1062           1062         1062      1062      1062   \n",
      "9                   1062           1062         1062      1062      1062   \n",
      "10                  1062           1062         1062      1062      1062   \n",
      "\n",
      "        month_sin  month_cos  \n",
      "Bin ID                        \n",
      "0            1062          0  \n",
      "1            1062         11  \n",
      "2            1062         11  \n",
      "3            1062         11  \n",
      "4            1062         11  \n",
      "5            1062         11  \n",
      "6            1062         11  \n",
      "7            1062         11  \n",
      "8            1062         11  \n",
      "9            1062         11  \n",
      "10           1062         23  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"--- 1. Total NaN Analysis ---\")\n",
    "total_nans = np.isnan(X).sum()\n",
    "total_elements = X.size\n",
    "percent_nan = (total_nans / total_elements) * 100\n",
    "print(f\"Total elements in X: {total_elements}\")\n",
    "print(f\"Total NaN values:    {total_nans}\")\n",
    "print(f\"Overall NaN percentage: {percent_nan:.2f}%\\n\")\n",
    "\n",
    "\n",
    "print(\"--- 2. NaN Distribution per Bin ---\")\n",
    "# Sum NaNs along the 'days' (0) and 'features' (2) axes\n",
    "nans_per_bin = np.isnan(X).sum(axis=(0, 2))\n",
    "# Total possible data points for one bin = num_days * num_features\n",
    "total_per_bin = X.shape[0] * X.shape[2]\n",
    "percent_per_bin = (nans_per_bin / total_per_bin) * 100\n",
    "\n",
    "print(f\"Total data points per bin: {total_per_bin}\")\n",
    "print(\"NaN counts and percentage per bin:\")\n",
    "for i in range(K):\n",
    "    print(f\"  Bin {i}: {nans_per_bin[i]:>4} NaNs ({percent_per_bin[i]:6.2f}%)\")\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "print(\"--- 3. NaN Distribution per Feature ---\")\n",
    "# Sum NaNs along the 'days' (0) and 'bins' (1) axes\n",
    "nans_per_feature = np.isnan(X).sum(axis=(0, 1))\n",
    "# Total possible data points for one feature = num_days * num_bins\n",
    "total_per_feature = X.shape[0] * X.shape[1]\n",
    "percent_per_feature = (nans_per_feature / total_per_feature) * 100\n",
    "\n",
    "print(f\"Total data points per feature: {total_per_feature}\")\n",
    "print(\"NaN counts and percentage per feature:\")\n",
    "for j, col in enumerate(feature_cols):\n",
    "    print(f\"  {col:<15}: {nans_per_feature[j]:>4} NaNs ({percent_per_feature[j]:6.2f}%)\")\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "print(\"--- 4. Detailed 2D Breakdown (NaN Counts per Bin and Feature) ---\")\n",
    "# Sum NaNs only along the 'days' (0) axis\n",
    "# This gives a 2D matrix of shape (K_bins, F_features)\n",
    "nan_matrix = np.isnan(X).sum(axis=0)\n",
    "\n",
    "# Use pandas for a nice printout\n",
    "nan_summary_df = pd.DataFrame(nan_matrix, columns=feature_cols, dtype=int)\n",
    "nan_summary_df.index.name = 'Bin ID'\n",
    "print(nan_summary_df)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021c7f45",
   "metadata": {},
   "source": [
    "In summary:\n",
    "- 1039 days (1062 - 23) have complete data (all 11 bins).\n",
    "- 12 days (23 - 11) are missing only the last bin (Bin 10).\n",
    "- 11 days are missing all bins except the first one (Bins 1-10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40f6ed52",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_imputed_zero = np.nan_to_num(X, nan=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a29eec52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.False_, np.int64(0))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(X_imputed_zero).any(), np.isnan(X_imputed_zero).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10b00af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocessing import _tensor_from_norm\n",
    "norm_df = fixed_df.copy()\n",
    "# Once for all the input features (feature_cols) to create a 3D tensor X_all with the shape (num_days, K, F).\n",
    "X_all, dates = _tensor_from_norm(norm_df, feature_cols=feature_cols)\n",
    "# Once for just the single target variable (target_col) to create a tensor y_all.\n",
    "y_all, _ = _tensor_from_norm(norm_df, feature_cols=[TARGET_COL])\n",
    "# squeezes out the last dimension, making y_all a more convenient 2D matrix of shape (num_days, K).\n",
    "y_all = y_all[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb912bb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.False_, np.False_)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(X_all).any(), np.isnan(y_all).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95b6d4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You correctly passed the filtered 'feature_cols' here, fixing the data leak!\n",
    "ph1_X, ph1_Y, ph1_labels_list = build_model_arrays(\n",
    "        fixed_df,\n",
    "        feature_cols=feature_cols,  \n",
    "        target_col=TARGET_COL,\n",
    "        history_days=history_days,\n",
    "        horizon_days=horizon_days,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b77f9c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.False_"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(ph1_X).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41b7ce45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.utils:Saved arrays to data/phas1_data_*.npy\n",
      "INFO:src.utils:X shape: (1055, 7, 11, 11), Y shape: (1055, 1, 11)\n"
     ]
    }
   ],
   "source": [
    "# --- Block 3: Save arrays ---\n",
    "data_manager = DataManager()\n",
    "data_manager.save_arrays(\n",
    "    ph1_X, ph1_Y,\n",
    "    pd.DataFrame(index=pd.to_datetime(ph1_labels_list, utc=True)),\n",
    "    filename_prefix='phas1_data',  # <-- You are saving as 'phas2_data'\n",
    "    feature_cols=feature_cols,\n",
    "    target_col=TARGET_COL,\n",
    "    metadata={\n",
    "        \"input_csv\": \"data/processed/df_1h_lag_BLV_spatial_images.csv\",\n",
    "        \"timestamp_col\": \"measurement_time\",\n",
    "        \"feature_set\": feature_cols,\n",
    "        \"history_days\": 7,\n",
    "        \"horizon_days\": 1,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd923c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Block 4: Configure and Run ---\n",
    "LSTM_CONFIG = {\n",
    "    \"experiment_name\": \"UniLSTM_p1_exp01_essentialF\",\n",
    "    \"model_type\": \"LSTM\",\n",
    "    \"model_config\": {\n",
    "        \"hidden_size\": 64,\n",
    "        \"num_layers\": 2,\n",
    "        \"dropout\": 0.35,\n",
    "        \"bidirectional\": True,\n",
    "    },\n",
    "    \"data_prefix\": \"phas1_data\",  # <-- FIX 2: Match the filename_prefix\n",
    "    \"splits_file\": \"exp-004/exp-004rolling_origin_splits.json\",\n",
    "    \"feature_cols\": feature_cols,\n",
    "    \"feature_selection\": feature_cols, # <-- This is correct!\n",
    "    \"target_col\": TARGET_COL,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_epochs\": 50,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"loss_function\": \"Huber\",  # <-- This setting needs the fix below\n",
    "    \"early_stopping_patience\": 20,\n",
    "    \"max_folds\": 35,\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da69a68f",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ef8c99",
   "metadata": {},
   "source": [
    "#### Function Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ce46c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = SolarForecastingPipeline(LSTM_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad784d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading data...\n",
      "INFO:src.utils:Loaded arrays from data/phas1_data_*.npy\n",
      "INFO:src.utils:X shape: (1055, 7, 11, 11), Y shape: (1055, 1, 11)\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Save configuration LSTM_CONFIG, and initilize the experiment directories.\n",
    "pipeline.tracker.save_config(pipeline.config)\n",
    "\n",
    "logger.info(\"Loading data...\")\n",
    "\n",
    "# Load Packed X, Y, Labels, and metadata\n",
    "\"\"\"\n",
    "{\n",
    "  \"X_shape\": [\n",
    "    1055,\n",
    "    7,\n",
    "    11,\n",
    "    3\n",
    "  ],\n",
    "  \"Y_shape\": [\n",
    "    1055,\n",
    "    1,\n",
    "    11\n",
    "  ],\n",
    "  \"X_dtype\": \"float64\",\n",
    "  \"Y_dtype\": \"float64\",\n",
    "  \"saved_at\": \"2025-11-07T15:53:45.042544\",\n",
    "  \"has_labels\": true,\n",
    "  \"feature_cols\": [\n",
    "    \"solar_zenith\",\n",
    "    \"absolute_hour\",\n",
    "    \"season_flag\"\n",
    "  ],\n",
    "  \"target_col\": \"CSI_ghi\",\n",
    "  \"input_csv\": \"data/processed/df_1h_lag_BLV_spatial_images.csv\",\n",
    "  \"timestamp_col\": \"measurement_time\",\n",
    "  \"feature_set\": [\n",
    "    \"solar_zenith\",\n",
    "    \"absolute_hour\",\n",
    "    \"season_flag\"\n",
    "  ],\n",
    "  \"history_days\": 7,\n",
    "  \"horizon_days\": 1\n",
    "}\n",
    "\"\"\"\n",
    "X, Y, labels, metadata = pipeline.data_manager.load_arrays(\n",
    "            filename_prefix=pipeline.config.get('data_prefix')\n",
    "        )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "897b5382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.False_"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.isnan(X).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b27bb4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Building simple reference from existing columns (no pvlib, no regridding)...\n",
      "/home/muhammadhassan/App_v02/src/evaluation_utils.py:132: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  ref[\"nam_csi\"].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
      "INFO:__main__:Reference dataframe prepared (11560 rows).\n",
      "INFO:src.utils:Loaded 35 folds from exp-004/exp-004rolling_origin_splits.json\n"
     ]
    }
   ],
   "source": [
    "# Selected Features based in the data.\n",
    "saved_features = metadata.get(\"feature_cols\")\n",
    "# Selected Features passed from User.\n",
    "expected_features = pipeline.config.get(\"feature_cols\")\n",
    "\n",
    "if expected_features is not None:\n",
    "    if saved_features is not None and list(expected_features) != list(saved_features):\n",
    "        raise ValueError(\n",
    "                    \"Feature mismatch between configuration and saved arrays. \"\n",
    "                    f\"Config expects {expected_features}, arrays contain {saved_features}.\"\n",
    "            )\n",
    "else:\n",
    "    pipeline.config[\"feature_cols\"] = saved_features\n",
    "\n",
    "labels_index = labels.index if labels is not None else None\n",
    "if labels_index is None:\n",
    "    logger.warning(\n",
    "                \"Labels dataframe not available; NAM comparison metrics will be skipped.\"\n",
    "            )\n",
    "    pipeline.reference_df = None\n",
    "else:\n",
    "    # Loading Raw data that uesed to cook the X, Y, Labels Tensors.\n",
    "    from src.evaluation_utils import _load_processed_dataframe\n",
    "    csv_path = metadata.get(\"input_csv\", PROCESSED_DATA_PATH) \n",
    "    base_df = _load_processed_dataframe(csv_path)\n",
    "    try:\n",
    "        from src.evaluation_utils import build_reference_from_existing\n",
    "        logger.info(\"Building simple reference from existing columns (no pvlib, no regridding)...\")\n",
    "        # Use the SAME merged dataframe you used to build arrays (before windowing)\n",
    "        # Suppose it's called `merged_df` or `base_df` in your pipeline\n",
    "        pipeline.reference_df = build_reference_from_existing(\n",
    "                    base_df,                         # <-- your processed/merged modeling df\n",
    "                    time_col=\"measurement_time\",\n",
    "                    nam_time_col=\"nam_target_time\",\n",
    "                    meas_ghi_col=\"ghi\",\n",
    "                    nam_ghi_col=\"nam_ghi\",\n",
    "                    cs_ghi_col=\"GHI_cs\",\n",
    "                    actual_csi_col=\"CSI_ghi\",\n",
    "                )\n",
    "        logger.info(\"Reference dataframe prepared (%d rows).\", len(pipeline.reference_df))\n",
    "    except Exception as exc:\n",
    "            logger.warning(\"Failed to construct simple reference: %s\", exc)\n",
    "            pipeline.reference_df = None\n",
    "\n",
    "\n",
    "# Load splits\n",
    "splits_data = pipeline.data_manager.load_rolling_splits(\n",
    "            pipeline.config.get('splits_file', 'rolling_origin_splits.json')\n",
    "        )\n",
    "\n",
    "# Run each fold\n",
    "fold_results = []\n",
    "max_folds = pipeline.config.get('max_folds', len(splits_data['folds']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b9e777b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.False_"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "fold_1_data = splits_data['folds'][0]\n",
    "fold_id = fold_1_data[\"fold_id\"]\n",
    "\n",
    "if labels is not None:\n",
    "    train_idx, val_idx = pipeline.data_manager.get_fold_indices(\n",
    "            X, labels, fold_1_data\n",
    "    )\n",
    "\n",
    "np.isnan(X).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "23e96501",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.pipeline:\n",
      "=== Running Fold 1 ===\n",
      "INFO:src.pipeline:Train samples: 21, Val samples: 18\n",
      "INFO:src.pipeline:Model parameters: 182,667\n",
      "INFO:src.engine:Using HuberLoss (Smooth L1 Loss)\n",
      "INFO:src.engine:Epoch [10/50] - Train Loss: 0.270908, Val Loss: 0.256904, Train MAE: 0.626994, Val MAE: 0.580839, LR: 0.001000\n",
      "INFO:src.engine:Epoch [20/50] - Train Loss: 0.110409, Val Loss: 0.138143, Train MAE: 0.371620, Val MAE: 0.432294, LR: 0.001000\n",
      "INFO:src.engine:Epoch [30/50] - Train Loss: 0.078178, Val Loss: 0.128315, Train MAE: 0.324863, Val MAE: 0.419106, LR: 0.001000\n",
      "INFO:src.engine:Epoch [40/50] - Train Loss: 0.066869, Val Loss: 0.126452, Train MAE: 0.270943, Val MAE: 0.415075, LR: 0.000500\n",
      "INFO:src.engine:Early stopping at epoch 45\n",
      "WARNING:src.evaluation_utils:Comparison dataframe is empty; returning NaN metrics.\n",
      "INFO:src.pipeline:Fold 1 - Validation Metrics:\n",
      "INFO:src.pipeline:  MAE: 0.421724\n",
      "INFO:src.pipeline:  RMSE: 0.510666\n",
      "INFO:src.pipeline:  MAPE: 3708741.250000\n",
      "INFO:src.pipeline:  per_horizon_MAE: [0.421724]\n"
     ]
    }
   ],
   "source": [
    "# Run fold\n",
    "fold_result = pipeline.run_fold(\n",
    "                X,\n",
    "                Y,\n",
    "                train_idx,\n",
    "                val_idx,\n",
    "                fold_id,\n",
    "                labels_index=labels_index,\n",
    "                reference_df=pipeline.reference_df,\n",
    "            )\n",
    "fold_results.append(fold_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055f0951",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "032d2634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will now work after you apply the fixes below\n",
    "# pipeline = SolarForecastingPipeline(LSTM_CONFIG)\n",
    "# _, summary = pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51be6edb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
